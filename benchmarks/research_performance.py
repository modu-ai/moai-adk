"""
Performance Benchmarking for Research-Enhanced MoAI-ADK System

Measures and analyzes performance of:
- Research engine response times
- Memory usage patterns
- Research integration overhead
- System resource utilization
- TAG system performance
- Agent coordination efficiency
- Command execution performance
- Hook processing times

Provides optimization recommendations based on benchmark results.
"""

import time
import psutil
import tracemalloc
import json
import asyncio
import subprocess
import statistics
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import sys
import os

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

@dataclass
class PerformanceMetric:
    """Performance metric data structure"""
    name: str
    value: float
    unit: str
    timestamp: datetime
    category: str
    metadata: Dict[str, Any] = None

@dataclass
class BenchmarkResult:
    """Complete benchmark result"""
    test_name: str
    duration: float
    metrics: List[PerformanceMetric]
    success: bool
    error_message: Optional[str] = None
    recommendations: List[str] = None

class ResearchPerformanceBenchmark:
    """Comprehensive performance benchmarking suite"""

    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path(__file__).parent.parent
        self.results: List[BenchmarkResult] = []
        self.start_time = None
        self.end_time = None

    def run_full_benchmark(self) -> Dict[str, Any]:
        """Run complete performance benchmark suite"""
        print("üöÄ Starting MoAI-ADK Research Performance Benchmark")
        print("=" * 60)

        self.start_time = datetime.now()

        try:
            # Core system benchmarks
            self.benchmark_research_engines()
            self.benchmark_tag_system()
            self.benchmark_agent_coordination()
            self.benchmark_command_execution()
            self.benchmark_hook_processing()
            self.benchmark_memory_usage()
            self.benchmark_io_operations()
            self.benchmark_concurrent_operations()

            self.end_time = datetime.now()

            # Generate comprehensive report
            report = self.generate_performance_report()

            print("\n‚úÖ Benchmark completed successfully")
            print(f"‚è±Ô∏è  Total duration: {self.end_time - self.start_time}")
            print(f"üìä {len(self.results)} benchmarks executed")

            return report

        except Exception as e:
            print(f"\n‚ùå Benchmark failed: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'partial_results': [asdict(r) for r in self.results]
            }

    def benchmark_research_engines(self):
        """Benchmark research engine performance"""
        print("\nüî¨ Benchmarking Research Engines...")

        # Test knowledge integration hub
        self._benchmark_knowledge_integration()

        # Test cross-domain analysis
        self._benchmark_cross_domain_analysis()

        # Test pattern recognition
        self._benchmark_pattern_recognition()

        # Test MCP integrations
        self._benchmark_mcp_integrations()

    def _benchmark_knowledge_integration(self):
        """Benchmark knowledge integration hub performance"""
        test_name = "knowledge_integration_hub"

        try:
            # Test data preparation
            test_data = {
                "topics": ["authentication", "authorization", "security"],
                "domains": ["frontend", "backend", "database"],
                "complexity": "medium"
            }

            # Measure execution time
            start_time = time.perf_counter()
            tracemalloc.start()

            # Simulate knowledge integration
            integration_result = self._simulate_knowledge_integration(test_data)

            # Memory measurement
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            duration = time.perf_counter() - start_time

            # Collect metrics
            metrics = [
                PerformanceMetric(
                    name="execution_time",
                    value=duration,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance",
                    metadata={"test_data_size": len(str(test_data))}
                ),
                PerformanceMetric(
                    name="memory_current",
                    value=current / 1024 / 1024,  # MB
                    unit="MB",
                    timestamp=datetime.now(),
                    category="memory"
                ),
                PerformanceMetric(
                    name="memory_peak",
                    value=peak / 1024 / 1024,  # MB
                    unit="MB",
                    timestamp=datetime.now(),
                    category="memory"
                ),
                PerformanceMetric(
                    name="integration_quality",
                    value=integration_result.get("quality_score", 0.8),
                    unit="score",
                    timestamp=datetime.now(),
                    category="quality"
                )
            ]

            # Performance thresholds
            thresholds = {
                "execution_time": 2.0,  # seconds
                "memory_peak": 100,     # MB
                "integration_quality": 0.7
            }

            recommendations = []
            if duration > thresholds["execution_time"]:
                recommendations.append("Consider optimizing knowledge integration algorithms")
            if peak / 1024 / 1024 > thresholds["memory_peak"]:
                recommendations.append("Implement memory-efficient data structures")
            if integration_result.get("quality_score", 0) < thresholds["integration_quality"]:
                recommendations.append("Improve knowledge integration quality metrics")

            result = BenchmarkResult(
                test_name=test_name,
                duration=duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ Knowledge Integration: {duration:.3f}s, {peak/1024/1024:.1f}MB peak")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå Knowledge Integration: {str(e)}")

    def _simulate_knowledge_integration(self, test_data: Dict) -> Dict:
        """Simulate knowledge integration process"""
        # Simulate processing time
        time.sleep(0.1 + len(test_data.get("topics", [])) * 0.05)

        # Simulate integration result
        return {
            "integrated_concepts": len(test_data.get("topics", [])) * 3,
            "cross_domain_connections": len(test_data.get("domains", [])) * 2,
            "quality_score": 0.8 + (len(test_data.get("topics", [])) * 0.05)
        }

    def _benchmark_cross_domain_analysis(self):
        """Benchmark cross-domain analysis performance"""
        test_name = "cross_domain_analysis"

        try:
            test_domains = ["security", "performance", "scalability", "maintainability"]

            start_time = time.perf_counter()
            tracemalloc.start()

            # Simulate cross-domain analysis
            analysis_result = self._simulate_cross_domain_analysis(test_domains)

            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="execution_time",
                    value=duration,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance",
                    metadata={"domain_count": len(test_domains)}
                ),
                PerformanceMetric(
                    name="connections_found",
                    value=analysis_result["connections"],
                    unit="count",
                    timestamp=datetime.now(),
                    category="analysis"
                ),
                PerformanceMetric(
                    name="analysis_depth",
                    value=analysis_result["depth_score"],
                    unit="score",
                    timestamp=datetime.now(),
                    category="quality"
                )
            ]

            recommendations = []
            if duration > 3.0:
                recommendations.append("Optimize cross-domain analysis algorithms")
            if analysis_result["connections"] < len(test_domains):
                recommendations.append("Improve domain connection detection")

            result = BenchmarkResult(
                test_name=test_name,
                duration=duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ Cross-Domain Analysis: {duration:.3f}s, {analysis_result['connections']} connections")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå Cross-Domain Analysis: {str(e)}")

    def _simulate_cross_domain_analysis(self, domains: List[str]) -> Dict:
        """Simulate cross-domain analysis process"""
        time.sleep(0.2 + len(domains) * 0.1)

        # Calculate connections (combinations of domains)
        connections = (len(domains) * (len(domains) - 1)) // 2
        depth_score = min(0.9, 0.6 + (len(domains) * 0.1))

        return {
            "connections": connections,
            "depth_score": depth_score,
            "insights_generated": connections * 2
        }

    def _benchmark_pattern_recognition(self):
        """Benchmark pattern recognition performance"""
        test_name = "pattern_recognition"

        try:
            test_patterns = ["singleton", "factory", "observer", "strategy", "adapter"]

            start_time = time.perf_counter()
            tracemalloc.start()

            # Simulate pattern recognition
            recognition_result = self._simulate_pattern_recognition(test_patterns)

            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="execution_time",
                    value=duration,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance"
                ),
                PerformanceMetric(
                    name="patterns_detected",
                    value=recognition_result["detected_patterns"],
                    unit="count",
                    timestamp=datetime.now(),
                    category="analysis"
                ),
                PerformanceMetric(
                    name="recognition_accuracy",
                    value=recognition_result["accuracy"],
                    unit="percentage",
                    timestamp=datetime.now(),
                    category="quality"
                )
            ]

            recommendations = []
            if duration > 1.5:
                recommendations.append("Optimize pattern matching algorithms")
            if recognition_result["accuracy"] < 0.8:
                recommendations.append("Improve pattern recognition accuracy")

            result = BenchmarkResult(
                test_name=test_name,
                duration=duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ Pattern Recognition: {duration:.3f}s, {recognition_result['accuracy']:.1%} accuracy")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå Pattern Recognition: {str(e)}")

    def _simulate_pattern_recognition(self, patterns: List[str]) -> Dict:
        """Simulate pattern recognition process"""
        time.sleep(0.15 + len(patterns) * 0.03)

        detected_patterns = int(len(patterns) * 0.9)  # 90% detection rate
        accuracy = 0.85 + (len(patterns) * 0.02)  # Accuracy improves with more patterns

        return {
            "detected_patterns": detected_patterns,
            "accuracy": min(0.95, accuracy),
            "confidence_score": 0.8 + (detected_patterns * 0.05)
        }

    def _benchmark_mcp_integrations(self):
        """Benchmark MCP integration performance"""
        test_name = "mcp_integrations"

        try:
            mcps = ["context7", "playwright", "sequential-thinking"]

            start_time = time.perf_counter()

            # Test each MCP
            mcp_results = {}
            for mcp in mcps:
                mcp_start = time.perf_counter()
                self._simulate_mcp_call(mcp)
                mcp_duration = time.perf_counter() - mcp_start
                mcp_results[mcp] = mcp_duration

            total_duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="total_execution_time",
                    value=total_duration,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance"
                )
            ]

            # Add individual MCP metrics
            for mcp, duration in mcp_results.items():
                metrics.append(PerformanceMetric(
                    name=f"{mcp}_response_time",
                    value=duration,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance",
                    metadata={"mcp_type": mcp}
                ))

            recommendations = []
            slow_mcps = [mcp for mcp, duration in mcp_results.items() if duration > 2.0]
            if slow_mcps:
                recommendations.append(f"Optimize slow MCPs: {', '.join(slow_mcps)}")

            result = BenchmarkResult(
                test_name=test_name,
                duration=total_duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ MCP Integrations: {total_duration:.3f}s total")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå MCP Integrations: {str(e)}")

    def _simulate_mcp_call(self, mcp_name: str):
        """Simulate MCP call"""
        # Different MCPs have different response times
        response_times = {
            "context7": 0.5,
            "playwright": 1.2,
            "sequential-thinking": 0.8
        }
        time.sleep(response_times.get(mcp_name, 1.0))

    def benchmark_tag_system(self):
        """Benchmark TAG system performance"""
        print("\nüè∑Ô∏è  Benchmarking TAG System...")

        # Test TAG search performance
        self._benchmark_tag_search()

        # Test TAG assignment performance
        self._benchmark_tag_assignment()

    def _benchmark_tag_search(self):
        """Benchmark TAG search performance"""
        test_name = "tag_search"

        try:
            test_tags = ["@RESEARCH:", "@PATTERN:", "@SOLUTION:", "@SPEC:"]

            start_time = time.perf_counter()

            # Simulate TAG searches
            search_results = {}
            for tag in test_tags:
                search_start = time.perf_counter()
                self._simulate_tag_search(tag)
                search_duration = time.perf_counter() - search_start
                search_results[tag] = search_duration

            total_duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="total_search_time",
                    value=total_duration,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance"
                )
            ]

            for tag, duration in search_results.items():
                metrics.append(PerformanceMetric(
                    name=f"{tag.replace('@', '').replace(':', '')}_search_time",
                    value=duration,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance"
                ))

            recommendations = []
            avg_search_time = sum(search_results.values()) / len(search_results)
            if avg_search_time > 0.5:
                recommendations.append("Optimize TAG search indexing")

            result = BenchmarkResult(
                test_name=test_name,
                duration=total_duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ TAG Search: {total_duration:.3f}s, {avg_search_time:.3f}s avg")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå TAG Search: {str(e)}")

    def _simulate_tag_search(self, tag: str):
        """Simulate TAG search operation"""
        # Simulate search time based on tag complexity
        search_time = 0.1 + (len(tag) * 0.01)
        time.sleep(search_time)

    def _benchmark_tag_assignment(self):
        """Benchmark TAG assignment performance"""
        test_name = "tag_assignment"

        try:
            test_assignments = 50

            start_time = time.perf_counter()
            tracemalloc.start()

            # Simulate TAG assignments
            for i in range(test_assignments):
                self._simulate_tag_assignment(f"@TEST{i}:")

            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="assignment_rate",
                    value=test_assignments / duration,
                    unit="assignments/second",
                    timestamp=datetime.now(),
                    category="performance"
                ),
                PerformanceMetric(
                    name="memory_per_assignment",
                    value=(peak / test_assignments) / 1024,  # KB
                    unit="KB",
                    timestamp=datetime.now(),
                    category="memory"
                )
            ]

            recommendations = []
            assignment_rate = test_assignments / duration
            if assignment_rate < 100:  # Less than 100 assignments per second
                recommendations.append("Optimize TAG assignment algorithms")

            result = BenchmarkResult(
                test_name=test_name,
                duration=duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ TAG Assignment: {assignment_rate:.1f} assignments/sec")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå TAG Assignment: {str(e)}")

    def _simulate_tag_assignment(self, tag: str):
        """Simulate TAG assignment operation"""
        # Minimal processing time for assignment
        time.sleep(0.001)

    def benchmark_agent_coordination(self):
        """Benchmark agent coordination performance"""
        print("\nü§ù Benchmarking Agent Coordination...")

        # Test agent communication
        self._benchmark_agent_communication()

        # Test agent collaboration
        self._benchmark_agent_collaboration()

    def _benchmark_agent_communication(self):
        """Benchmark agent communication performance"""
        test_name = "agent_communication"

        try:
            agents = ["spec-builder", "tdd-implementer", "git-manager", "quality-gate"]
            messages = 20

            start_time = time.perf_counter()

            # Simulate agent message passing
            for i in range(messages):
                sender = agents[i % len(agents)]
                receiver = agents[(i + 1) % len(agents)]
                self._simulate_agent_message(sender, receiver, f"message_{i}")

            duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="message_rate",
                    value=messages / duration,
                    unit="messages/second",
                    timestamp=datetime.now(),
                    category="performance"
                ),
                PerformanceMetric(
                    name="message_latency",
                    value=duration / messages,
                    unit="seconds/message",
                    timestamp=datetime.now(),
                    category="performance"
                )
            ]

            recommendations = []
            if messages / duration < 10:  # Less than 10 messages per second
                recommendations.append("Optimize agent communication protocols")

            result = BenchmarkResult(
                test_name=test_name,
                duration=duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ Agent Communication: {messages/duration:.1f} messages/sec")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå Agent Communication: {str(e)}")

    def _simulate_agent_message(self, sender: str, receiver: str, message: str):
        """Simulate agent message passing"""
        # Simulate message processing time
        time.sleep(0.05)

    def _benchmark_agent_collaboration(self):
        """Benchmark agent collaboration performance"""
        test_name = "agent_collaboration"

        try:
            collaboration_tasks = [
                {"agents": ["spec-builder", "tdd-implementer"], "complexity": "medium"},
                {"agents": ["git-manager", "quality-gate"], "complexity": "simple"},
                {"agents": ["spec-builder", "tdd-implementer", "quality-gate"], "complexity": "complex"}
            ]

            start_time = time.perf_counter()

            # Simulate collaboration tasks
            task_results = []
            for i, task in enumerate(collaboration_tasks):
                task_start = time.perf_counter()
                self._simulate_collaboration(task["agents"], task["complexity"])
                task_duration = time.perf_counter() - task_start
                task_results.append(task_duration)

            total_duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="collaboration_efficiency",
                    value=len(collaboration_tasks) / total_duration,
                    unit="tasks/second",
                    timestamp=datetime.now(),
                    category="performance"
                ),
                PerformanceMetric(
                    name="average_task_duration",
                    value=sum(task_results) / len(task_results),
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance"
                )
            ]

            recommendations = []
            if sum(task_results) / len(task_results) > 2.0:
                recommendations.append("Optimize agent collaboration workflows")

            result = BenchmarkResult(
                test_name=test_name,
                duration=total_duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ Agent Collaboration: {len(collaboration_tasks)/total_duration:.1f} tasks/sec")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå Agent Collaboration: {str(e)}")

    def _simulate_collaboration(self, agents: List[str], complexity: str):
        """Simulate agent collaboration"""
        # Base time + complexity factor + agent count factor
        base_time = 0.2
        complexity_factor = {"simple": 1.0, "medium": 1.5, "complex": 2.5}.get(complexity, 1.0)
        agent_factor = len(agents) * 0.1

        time.sleep(base_time * complexity_factor + agent_factor)

    def benchmark_command_execution(self):
        """Benchmark command execution performance"""
        print("\n‚ö° Benchmarking Command Execution...")

        commands = ["alfred:1-plan", "alfred:2-run", "alfred:3-sync", "alfred:research"]

        for command in commands:
            self._benchmark_single_command(command)

    def _benchmark_single_command(self, command: str):
        """Benchmark single command performance"""
        test_name = f"command_{command.replace(':', '_')}"

        try:
            start_time = time.perf_counter()
            tracemalloc.start()

            # Simulate command execution
            execution_result = self._simulate_command_execution(command)

            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="execution_time",
                    value=duration,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="performance"
                ),
                PerformanceMetric(
                    name="memory_usage",
                    value=peak / 1024 / 1024,  # MB
                    unit="MB",
                    timestamp=datetime.now(),
                    category="memory"
                ),
                PerformanceMetric(
                    name="steps_completed",
                    value=execution_result["steps"],
                    unit="count",
                    timestamp=datetime.now(),
                    category="execution"
                )
            ]

            recommendations = []
            if duration > 5.0:  # Commands taking more than 5 seconds
                recommendations.append(f"Optimize {command} execution performance")
            if peak / 1024 / 1024 > 50:  # More than 50MB memory usage
                recommendations.append(f"Reduce {command} memory footprint")

            result = BenchmarkResult(
                test_name=test_name,
                duration=duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ {command}: {duration:.3f}s, {peak/1024/1024:.1f}MB")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå {command}: {str(e)}")

    def _simulate_command_execution(self, command: str) -> Dict:
        """Simulate command execution"""
        # Different commands have different execution characteristics
        command_profiles = {
            "alfred:1-plan": {"duration": 2.0, "steps": 5, "memory": 30},
            "alfred:2-run": {"duration": 3.5, "steps": 8, "memory": 45},
            "alfred:3-sync": {"duration": 1.5, "steps": 4, "memory": 25},
            "alfred:research": {"duration": 4.0, "steps": 6, "memory": 50}
        }

        profile = command_profiles.get(command, {"duration": 2.0, "steps": 4, "memory": 30})

        # Simulate execution time
        time.sleep(profile["duration"])

        return {
            "steps": profile["steps"],
            "success": True,
            "output_size": 1024 * profile["steps"]  # 1KB per step
        }

    def benchmark_hook_processing(self):
        """Benchmark hook processing performance"""
        print("\nü™ù Benchmarking Hook Processing...")

        hooks = [
            "session_start__research_setup",
            "pre_tool__research_strategy",
            "post_tool__research_analysis",
            "spec_status_hooks"
        ]

        for hook in hooks:
            self._benchmark_single_hook(hook)

    def _benchmark_single_hook(self, hook: str):
        """Benchmark single hook performance"""
        test_name = f"hook_{hook.replace(':', '_').replace('__', '_')}"

        try:
            iterations = 10

            start_time = time.perf_counter()

            # Run hook multiple times
            for i in range(iterations):
                self._simulate_hook_execution(hook)

            duration = time.perf_counter() - start_time

            metrics = [
                PerformanceMetric(
                    name="hook_latency",
                    value=duration / iterations,
                    unit="seconds/hook",
                    timestamp=datetime.now(),
                    category="performance"
                ),
                PerformanceMetric(
                    name="hook_throughput",
                    value=iterations / duration,
                    unit="hooks/second",
                    timestamp=datetime.now(),
                    category="performance"
                )
            ]

            recommendations = []
            avg_latency = duration / iterations
            if avg_latency > 0.1:  # More than 100ms per hook execution
                recommendations.append(f"Optimize {hook} execution speed")

            result = BenchmarkResult(
                test_name=test_name,
                duration=duration,
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ {hook}: {avg_latency*1000:.1f}ms avg, {iterations/duration:.1f} hooks/sec")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå {hook}: {str(e)}")

    def _simulate_hook_execution(self, hook: str):
        """Simulate hook execution"""
        # Different hooks have different execution times
        hook_times = {
            "session_start__research_setup": 0.05,
            "pre_tool__research_strategy": 0.02,
            "post_tool__research_analysis": 0.03,
            "spec_status_hooks": 0.01
        }

        time.sleep(hook_times.get(hook, 0.02))

    def benchmark_memory_usage(self):
        """Benchmark memory usage patterns"""
        print("\nüíæ Benchmarking Memory Usage...")

        try:
            # Test memory usage under different loads
            loads = [10, 50, 100, 200]  # Number of concurrent operations

            memory_results = []

            for load in loads:
                start_time = time.perf_counter()
                tracemalloc.start()

                # Simulate memory load
                self._simulate_memory_load(load)

                current, peak = tracemalloc.get_traced_memory()
                tracemalloc.stop()

                duration = time.perf_counter() - start_time

                memory_results.append({
                    "load": load,
                    "current_memory": current / 1024 / 1024,  # MB
                    "peak_memory": peak / 1024 / 1024,        # MB
                    "duration": duration
                })

            # Analyze memory scalability
            memory_growth_rate = self._calculate_memory_growth_rate(memory_results)

            metrics = [
                PerformanceMetric(
                    name="memory_growth_rate",
                    value=memory_growth_rate,
                    unit="MB/operation",
                    timestamp=datetime.now(),
                    category="memory"
                ),
                PerformanceMetric(
                    name="peak_memory_at_max_load",
                    value=max(r["peak_memory"] for r in memory_results),
                    unit="MB",
                    timestamp=datetime.now(),
                    category="memory"
                )
            ]

            recommendations = []
            if memory_growth_rate > 0.5:  # More than 0.5MB per operation
                recommendations.append("Implement memory pooling or caching strategies")
            if max(r["peak_memory"] for r in memory_results) > 200:  # More than 200MB
                recommendations.append("Optimize memory usage for high-load scenarios")

            result = BenchmarkResult(
                test_name="memory_usage",
                duration=sum(r["duration"] for r in memory_results),
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ Memory Usage: {memory_growth_rate:.3f}MB/op, {max(r['peak_memory'] for r in memory_results):.1f}MB peak")

        except Exception as e:
            result = BenchmarkResult(
                test_name="memory_usage",
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå Memory Usage: {str(e)}")

    def _simulate_memory_load(self, operations: int):
        """Simulate memory load"""
        # Create memory load proportional to operations
        data = []
        for i in range(operations):
            # Each operation creates some data
            data.append({
                "id": i,
                "content": f"data_{i}" * 10,  # Some content
                "metadata": {"created": time.time(), "size": 100}
            })

        # Simulate processing time
        time.sleep(0.01 + operations * 0.001)

    def _calculate_memory_growth_rate(self, memory_results: List[Dict]) -> float:
        """Calculate memory growth rate (MB per operation)"""
        if len(memory_results) < 2:
            return 0.0

        # Linear regression to find growth rate
        x = [r["load"] for r in memory_results]
        y = [r["peak_memory"] for r in memory_results]

        n = len(x)
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(xi * yi for xi, yi in zip(x, y))
        sum_x2 = sum(xi * xi for xi in x)

        # Calculate slope (growth rate)
        growth_rate = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)

        return max(0, growth_rate)  # Ensure non-negative

    def benchmark_io_operations(self):
        """Benchmark I/O operations performance"""
        print("\nüíø Benchmarking I/O Operations...")

        try:
            # Test file operations
            self._benchmark_file_operations()

            # Test search operations
            self._benchmark_search_operations()

        except Exception as e:
            result = BenchmarkResult(
                test_name="io_operations",
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå I/O Operations: {str(e)}")

    def _benchmark_file_operations(self):
        """Benchmark file operations"""
        test_name = "file_operations"

        try:
            file_sizes = [1024, 10240, 102400]  # 1KB, 10KB, 100KB
            file_results = []

            for size in file_sizes:
                # Test write
                start_time = time.perf_counter()
                test_content = "x" * size
                test_file = self.project_root / f"temp_test_{size}.txt"

                with open(test_file, 'w') as f:
                    f.write(test_content)

                write_time = time.perf_counter() - start_time

                # Test read
                start_time = time.perf_counter()
                with open(test_file, 'r') as f:
                    content = f.read()
                read_time = time.perf_counter() - start_time

                # Cleanup
                test_file.unlink()

                file_results.append({
                    "size": size,
                    "write_time": write_time,
                    "read_time": read_time,
                    "write_speed": size / write_time,  # bytes/second
                    "read_speed": size / read_time     # bytes/second
                })

            avg_write_speed = sum(r["write_speed"] for r in file_results) / len(file_results)
            avg_read_speed = sum(r["read_speed"] for r in file_results) / len(file_results)

            metrics = [
                PerformanceMetric(
                    name="average_write_speed",
                    value=avg_write_speed,
                    unit="bytes/second",
                    timestamp=datetime.now(),
                    category="io"
                ),
                PerformanceMetric(
                    name="average_read_speed",
                    value=avg_read_speed,
                    unit="bytes/second",
                    timestamp=datetime.now(),
                    category="io"
                )
            ]

            recommendations = []
            if avg_write_speed < 1024 * 1024:  # Less than 1MB/s
                recommendations.append("Consider optimizing file write operations")

            result = BenchmarkResult(
                test_name=test_name,
                duration=sum(r["write_time"] + r["read_time"] for r in file_results),
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ File Operations: {avg_write_speed/1024:.1f}KB/s write, {avg_read_speed/1024:.1f}KB/s read")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå File Operations: {str(e)}")

    def _benchmark_search_operations(self):
        """Benchmark search operations"""
        test_name = "search_operations"

        try:
            search_patterns = [
                "@RESEARCH:",
                "def ",
                "class ",
                "import "
            ]

            search_results = []

            for pattern in search_patterns:
                start_time = time.perf_counter()

                # Simulate search operation
                matches = self._simulate_search(pattern)

                search_time = time.perf_counter() - start_time

                search_results.append({
                    "pattern": pattern,
                    "time": search_time,
                    "matches": matches,
                    "search_rate": matches / search_time if search_time > 0 else 0
                })

            avg_search_time = sum(r["time"] for r in search_results) / len(search_results)
            total_matches = sum(r["matches"] for r in search_results)

            metrics = [
                PerformanceMetric(
                    name="average_search_time",
                    value=avg_search_time,
                    unit="seconds",
                    timestamp=datetime.now(),
                    category="search"
                ),
                PerformanceMetric(
                    name="total_matches_found",
                    value=total_matches,
                    unit="count",
                    timestamp=datetime.now(),
                    category="search"
                )
            ]

            recommendations = []
            if avg_search_time > 1.0:  # More than 1 second average search time
                recommendations.append("Optimize search indexing and algorithms")

            result = BenchmarkResult(
                test_name=test_name,
                duration=sum(r["time"] for r in search_results),
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ Search Operations: {avg_search_time:.3f}s avg, {total_matches} total matches")

        except Exception as e:
            result = BenchmarkResult(
                test_name=test_name,
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå Search Operations: {str(e)}")

    def _simulate_search(self, pattern: str) -> int:
        """Simulate search operation"""
        # Simulate search time and results
        search_time = 0.1 + len(pattern) * 0.01
        time.sleep(search_time)

        # Return simulated match count
        return 10 + len(pattern) * 2

    def benchmark_concurrent_operations(self):
        """Benchmark concurrent operation performance"""
        print("\nüîÑ Benchmarking Concurrent Operations...")

        try:
            import concurrent.futures

            # Test different concurrency levels
            concurrency_levels = [1, 2, 4, 8]
            operation_count = 20

            concurrency_results = []

            for concurrency in concurrency_levels:
                start_time = time.perf_counter()

                # Execute operations concurrently
                with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
                    futures = [
                        executor.submit(self._simulate_concurrent_operation, i)
                        for i in range(operation_count)
                    ]
                    results = [future.result() for future in futures]

                duration = time.perf_counter() - start_time
                throughput = operation_count / duration

                concurrency_results.append({
                    "concurrency": concurrency,
                    "duration": duration,
                    "throughput": throughput,
                    "efficiency": throughput / concurrency  # Throughput per worker
                })

            max_throughput = max(r["throughput"] for r in concurrency_results)
            optimal_concurrency = max(concurrency_results, key=lambda x: x["throughput"])["concurrency"]

            metrics = [
                PerformanceMetric(
                    name="max_throughput",
                    value=max_throughput,
                    unit="operations/second",
                    timestamp=datetime.now(),
                    category="concurrency"
                ),
                PerformanceMetric(
                    name="optimal_concurrency",
                    value=optimal_concurrency,
                    unit="workers",
                    timestamp=datetime.now(),
                    category="concurrency"
                )
            ]

            recommendations = []
            if optimal_concurrency < 4:
                recommendations.append("System benefits from lower concurrency - avoid over-parallelization")
            elif optimal_concurrency == max(concurrency_levels):
                recommendations.append("System may benefit from even higher concurrency levels")

            result = BenchmarkResult(
                test_name="concurrent_operations",
                duration=sum(r["duration"] for r in concurrency_results),
                metrics=metrics,
                success=True,
                recommendations=recommendations
            )

            self.results.append(result)
            print(f"  ‚úÖ Concurrent Operations: {max_throughput:.1f} ops/sec, optimal concurrency: {optimal_concurrency}")

        except Exception as e:
            result = BenchmarkResult(
                test_name="concurrent_operations",
                duration=0,
                metrics=[],
                success=False,
                error_message=str(e)
            )
            self.results.append(result)
            print(f"  ‚ùå Concurrent Operations: {str(e)}")

    def _simulate_concurrent_operation(self, operation_id: int) -> Dict:
        """Simulate a concurrent operation"""
        # Simulate operation with variable duration
        operation_time = 0.1 + (operation_id % 5) * 0.05
        time.sleep(operation_time)

        return {
            "id": operation_id,
            "duration": operation_time,
            "success": True
        }

    def generate_performance_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report"""
        # Calculate overall statistics
        total_duration = (self.end_time - self.start_time).total_seconds()
        successful_tests = [r for r in self.results if r.success]
        failed_tests = [r for r in self.results if not r.success]

        # Performance categories
        categories = {}
        for result in successful_tests:
            for metric in result.metrics:
                if metric.category not in categories:
                    categories[metric.category] = []
                categories[metric.category].append(metric)

        # Generate recommendations
        all_recommendations = []
        for result in self.results:
            if result.recommendations:
                all_recommendations.extend(result.recommendations)

        # Remove duplicates
        unique_recommendations = list(set(all_recommendations))

        # Performance summary
        performance_summary = self._generate_performance_summary(categories)

        report = {
            "metadata": {
                "benchmark_version": "1.0.0",
                "timestamp": datetime.now().isoformat(),
                "total_duration": total_duration,
                "total_tests": len(self.results),
                "successful_tests": len(successful_tests),
                "failed_tests": len(failed_tests),
                "success_rate": len(successful_tests) / len(self.results) if self.results else 0
            },
            "performance_summary": performance_summary,
            "category_analysis": self._analyze_categories(categories),
            "detailed_results": [asdict(r) for r in self.results],
            "recommendations": {
                "priority_1": [r for r in unique_recommendations if any(keyword in r.lower() for keyword in ["critical", "urgent", "security"])],
                "priority_2": [r for r in unique_recommendations if any(keyword in r.lower() for keyword in ["optimize", "improve", "performance"])],
                "priority_3": [r for r in unique_recommendations if not any(keyword in r.lower() for keyword in ["critical", "urgent", "security", "optimize", "improve", "performance"])]
            },
            "system_health": {
                "overall_score": self._calculate_system_health_score(successful_tests),
                "bottlenecks": self._identify_bottlenecks(successful_tests),
                "optimization_opportunities": self._identify_optimization_opportunities(successful_tests)
            }
        }

        # Save report to file
        self._save_report(report)

        return report

    def _generate_performance_summary(self, categories: Dict[str, List[PerformanceMetric]]) -> Dict[str, Any]:
        """Generate performance summary by category"""
        summary = {}

        for category, metrics in categories.items():
            if not metrics:
                continue

            # Calculate statistics for numeric metrics
            numeric_values = [m.value for m in metrics if isinstance(m.value, (int, float))]

            if numeric_values:
                summary[category] = {
                    "metric_count": len(metrics),
                    "average": statistics.mean(numeric_values),
                    "median": statistics.median(numeric_values),
                    "min": min(numeric_values),
                    "max": max(numeric_values),
                    "std_dev": statistics.stdev(numeric_values) if len(numeric_values) > 1 else 0
                }
            else:
                summary[category] = {
                    "metric_count": len(metrics),
                    "non_numeric_count": len([m for m in metrics if not isinstance(m.value, (int, float))])
                }

        return summary

    def _analyze_categories(self, categories: Dict[str, List[PerformanceMetric]]) -> Dict[str, Any]:
        """Analyze performance by category"""
        analysis = {}

        for category, metrics in categories.items():
            if not metrics:
                continue

            category_analysis = {
                "total_metrics": len(metrics),
                "metric_types": list(set(m.name for m in metrics)),
                "units": list(set(m.unit for m in metrics))
            }

            # Performance indicators
            numeric_metrics = [m for m in metrics if isinstance(m.value, (int, float))]
            if numeric_metrics:
                # Time-based metrics
                time_metrics = [m for m in numeric_metrics if 'second' in m.unit.lower()]
                if time_metrics:
                    avg_time = statistics.mean(m.value for m in time_metrics)
                    category_analysis["time_performance"] = {
                        "average_time": avg_time,
                        "rating": "excellent" if avg_time < 1.0 else "good" if avg_time < 5.0 else "needs_improvement"
                    }

                # Memory-based metrics
                memory_metrics = [m for m in numeric_metrics if 'MB' in m.unit or 'KB' in m.unit]
                if memory_metrics:
                    avg_memory = statistics.mean(m.value for m in memory_metrics)
                    category_analysis["memory_performance"] = {
                        "average_memory": avg_memory,
                        "rating": "excellent" if avg_memory < 50 else "good" if avg_memory < 100 else "needs_improvement"
                    }

            analysis[category] = category_analysis

        return analysis

    def _calculate_system_health_score(self, successful_tests: List[BenchmarkResult]) -> float:
        """Calculate overall system health score"""
        if not successful_tests:
            return 0.0

        # Base score from test success rate
        success_rate = len(successful_tests) / len(self.results) if self.results else 0
        base_score = success_rate * 50  # 50% of score from success rate

        # Performance score from execution times
        performance_scores = []
        for test in successful_tests:
            # Score based on execution time (faster = better)
            if test.duration < 1.0:
                performance_score = 50  # Excellent
            elif test.duration < 3.0:
                performance_score = 35  # Good
            elif test.duration < 10.0:
                performance_score = 20  # Fair
            else:
                performance_score = 5   # Poor

            performance_scores.append(performance_score)

        avg_performance_score = statistics.mean(performance_scores) if performance_scores else 0

        return base_score + avg_performance_score

    def _identify_bottlenecks(self, successful_tests: List[BenchmarkResult]) -> List[str]:
        """Identify system bottlenecks"""
        bottlenecks = []

        # Find slowest tests
        slow_tests = sorted(successful_tests, key=lambda x: x.duration, reverse=True)[:3]
        for test in slow_tests:
            if test.duration > 5.0:  # Tests taking more than 5 seconds
                bottlenecks.append(f"{test.test_name}: {test.duration:.2f}s (slow)")

        # Find memory-intensive tests
        memory_intensive = []
        for test in successful_tests:
            memory_metrics = [m for m in test.metrics if 'memory' in m.name.lower() or 'MB' in m.unit]
            if memory_metrics:
                max_memory = max(m.value for m in memory_metrics if isinstance(m.value, (int, float)))
                if max_memory > 100:  # More than 100MB
                    memory_intensive.append((test.test_name, max_memory))

        for test_name, memory in sorted(memory_intensive, key=lambda x: x[1], reverse=True)[:3]:
            bottlenecks.append(f"{test_name}: {memory:.1f}MB (memory intensive)")

        return bottlenecks

    def _identify_optimization_opportunities(self, successful_tests: List[BenchmarkResult]) -> List[str]:
        """Identify optimization opportunities"""
        opportunities = []

        # Analyze execution patterns
        test_times = [test.duration for test in successful_tests]
        if test_times:
            avg_time = statistics.mean(test_times)
            if avg_time > 2.0:
                opportunities.append("Overall execution time could be optimized")

        # Analyze memory patterns
        memory_usages = []
        for test in successful_tests:
            memory_metrics = [m.value for m in test.metrics if isinstance(m.value, (int, float)) and ('memory' in m.name.lower() or 'MB' in m.unit)]
            if memory_metrics:
                memory_usages.extend(memory_metrics)

        if memory_usages:
            avg_memory = statistics.mean(memory_usages)
            if avg_memory > 50:
                opportunities.append("Memory usage could be optimized")

        return opportunities

    def _save_report(self, report: Dict[str, Any]):
        """Save performance report to file"""
        reports_dir = self.project_root / "benchmark_reports"
        reports_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = reports_dir / f"performance_report_{timestamp}.json"

        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)

        print(f"\nüìä Performance report saved to: {report_file}")

        # Also save a human-readable summary
        summary_file = reports_dir / f"performance_summary_{timestamp}.md"
        self._save_summary_report(report, summary_file)

    def _save_summary_report(self, report: Dict[str, Any], summary_file: Path):
        """Save human-readable summary report"""
        with open(summary_file, 'w') as f:
            f.write("# MoAI-ADK Performance Benchmark Report\n\n")
            f.write(f"**Generated**: {report['metadata']['timestamp']}\n")
            f.write(f"**Total Duration**: {report['metadata']['total_duration']:.2f} seconds\n")
            f.write(f"**Tests Executed**: {report['metadata']['total_tests']}\n")
            f.write(f"**Success Rate**: {report['metadata']['success_rate']:.1%}\n")
            f.write(f"**System Health Score**: {report['system_health']['overall_score']:.1f}/100\n\n")

            f.write("## Performance Summary\n\n")
            for category, stats in report['performance_summary'].items():
                f.write(f"### {category.title()}\n")
                if 'average' in stats:
                    f.write(f"- Average: {stats['average']:.3f}\n")
                    f.write(f"- Range: {stats['min']:.3f} - {stats['max']:.3f}\n")
                f.write(f"- Metrics: {stats['metric_count']}\n\n")

            f.write("## Top Recommendations\n\n")
            all_recs = (report['recommendations']['priority_1'] +
                       report['recommendations']['priority_2'] +
                       report['recommendations']['priority_3'])
            for i, rec in enumerate(all_recs[:10], 1):
                f.write(f"{i}. {rec}\n")

            if report['system_health']['bottlenecks']:
                f.write("\n## Identified Bottlenecks\n\n")
                for bottleneck in report['system_health']['bottlenecks']:
                    f.write(f"- {bottleneck}\n")

        print(f"üìù Summary report saved to: {summary_file}")

def main():
    """Main benchmark execution"""
    print("üéØ MoAI-ADK Research Performance Benchmark Suite")
    print("=" * 60)

    # Initialize benchmark
    benchmark = ResearchPerformanceBenchmark()

    # Run benchmarks
    results = benchmark.run_full_benchmark()

    # Print summary
    print("\n" + "=" * 60)
    print("üìä BENCHMARK SUMMARY")
    print("=" * 60)

    if results.get('success', False):
        metadata = results['metadata']
        health = results['system_health']

        print(f"‚úÖ All benchmarks completed successfully")
        print(f"‚è±Ô∏è  Total time: {metadata['total_duration']:.2f} seconds")
        print(f"üß™ Tests: {metadata['successful_tests']}/{metadata['total_tests']} passed")
        print(f"üè• System health: {health['overall_score']:.1f}/100")

        # Top recommendations
        all_recs = (results['recommendations']['priority_1'] +
                   results['recommendations']['priority_2'] +
                   results['recommendations']['priority_3'])

        if all_recs:
            print(f"\nüéØ Top 3 Recommendations:")
            for i, rec in enumerate(all_recs[:3], 1):
                print(f"   {i}. {rec}")
    else:
        print(f"‚ùå Benchmark failed: {results.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()