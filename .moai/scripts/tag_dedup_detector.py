#!/usr/bin/env python3
"""
MoAI-ADK TAG De-duplication Detector
GPT-5 Pro analysis ê¸°ë°˜ TAG ì¤‘ë³µ íƒì§€ ë° ìˆ˜ì • ì‹œìŠ¤í…œ
"""

import json
import re
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Set, Optional
from collections import defaultdict, Counter
from dataclasses import dataclass
from datetime import datetime

@dataclass
class TagInfo:
    """TAG ì •ë³´ êµ¬ì¡°ì²´"""
    tag: str
    file_path: str
    line_number: int
    line_content: str
    is_topline: bool
    tag_type: str  # SPEC, TEST, CODE, DOC
    domain: str
    id_number: str
    authority_score: int

@dataclass
class DuplicateGroup:
    """ì¤‘ë³µ ê·¸ë£¹ ì •ë³´"""
    tag_pattern: str
    occurrences: List[TagInfo]
    primary_candidate: Optional[TagInfo] = None
    duplicates: List[TagInfo] = None
    
    def __post_init__(self):
        if self.duplicates is None:
            self.duplicates = []

class TagDedupDetector:
    """TAG ì¤‘ë³µ íƒì§€ê¸°"""
    
    def __init__(self, config_path: str = None):
        self.project_root = Path.cwd()
        self.config = self._load_config(config_path)
        self.tag_pattern = re.compile(r'@([A-Z]+):([A-Z_]+)-([0-9]{3,})')
        self.all_tags: List[TagInfo] = []
        self.duplicate_groups: List[DuplicateGroup] = []
        
    def _load_config(self, config_path: str = None) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if config_path is None:
            config_path = self.project_root / ".moai" / "tag-policy-updated.json"
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            sys.exit(1)
    
    def _calculate_authority_score(self, file_path: str, line_number: int) -> int:
        """íŒŒì¼ ê¶Œí•œ ì ìˆ˜ ê³„ì‚°"""
        path = Path(file_path)
        authority_hierarchy = self.config.get("topline_rules", {}).get("authority_hierarchy", {})
        
        # ìµœê³  ê¶Œí•œ
        for pattern in authority_hierarchy.get("highest", []):
            if path.match(pattern):
                return 100
                
        # ë†’ì€ ê¶Œí•œ
        for pattern in authority_hierarchy.get("high", []):
            if path.match(pattern):
                return 80
                
        # ì¤‘ê°„ ê¶Œí•œ
        for pattern in authority_hierarchy.get("medium", []):
            if path.match(pattern):
                return 60
                
        # ë‚®ì€ ê¶Œí•œ
        for pattern in authority_hierarchy.get("low", []):
            if path.match(pattern):
                return 40
                
        # ê¸°ë³¸ ê¶Œí•œ
        return 20
    
    def _is_topline_tag(self, line_content: str, line_number: int) -> bool:
        """Topline TAG ì—¬ë¶€ í™•ì¸"""
        if line_number > 20:  # toplineì€ ì²˜ìŒ 20ì¤„ ì´ë‚´
            return False
            
        # ì£¼ì„ ë¸”ë¡ ìŠ¤í‚µ í™•ì¸
        skip_patterns = self.config.get("topline_rules", {}).get("skip_header_block", [])
        for pattern in skip_patterns:
            if pattern.lower() in line_content.lower():
                return False
                
        return True
    
    def _extract_tags_from_file(self, file_path: str) -> List[TagInfo]:
        """íŒŒì¼ì—ì„œ TAG ì¶”ì¶œ"""
        tags = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            for line_num, line in enumerate(lines, 1):
                matches = self.tag_pattern.finditer(line)
                for match in matches:
                    tag_type, domain, id_num = match.groups()
                    full_tag = match.group(0)
                    
                    tag_info = TagInfo(
                        tag=full_tag,
                        file_path=file_path,
                        line_number=line_num,
                        line_content=line.strip(),
                        is_topline=self._is_topline_tag(line, line_num),
                        tag_type=tag_type,
                        domain=domain,
                        id_number=id_num,
                        authority_score=self._calculate_authority_score(file_path, line_num)
                    )
                    tags.append(tag_info)
                    
        except Exception as e:
            print(f"âš ï¸  íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
            
        return tags
    
    def scan_all_files(self) -> None:
        """ì „ì²´ íŒŒì¼ ìŠ¤ìº”"""
        print("ğŸ” ì „ì²´ íŒŒì¼ ìŠ¤ìº” ì‹œì‘...")
        
        eligible_patterns = self.config.get("eligible", [])
        excluded_patterns = self.config.get("excluded", [])
        
        for pattern in eligible_patterns:
            for file_path in self.project_root.glob(pattern):
                # ì œì™¸ íŒ¨í„´ í™•ì¸
                if any(file_path.match(exclude) for exclude in excluded_patterns):
                    continue
                    
                if file_path.is_file():
                    file_tags = self._extract_tags_from_file(str(file_path))
                    self.all_tags.extend(file_tags)
        
        print(f"âœ… {len(self.all_tags)}ê°œì˜ TAG ë°œê²¬")
    
    def find_duplicates(self) -> None:
        """ì¤‘ë³µ TAG íƒì§€"""
        print("ğŸ” ì¤‘ë³µ TAG íƒì§€ ì‹œì‘...")
        
        # TAG íŒ¨í„´ë³„ë¡œ ê·¸ë£¹í™”
        tag_groups = defaultdict(list)
        for tag in self.all_tags:
            # TAG íŒ¨í„´ ì •ê·œí™” (íƒ€ì…:ë„ë©”ì¸-ID)
            pattern = f"{tag.tag_type}:{tag.domain}-{tag.id_number}"
            tag_groups[pattern].append(tag)
        
        # ì¤‘ë³µ ê·¸ë£¹ ì‹ë³„
        for pattern, occurrences in tag_groups.items():
            if len(occurrences) > 1:
                # Primary í›„ë³´ ì„ íƒ (ìµœê³  ê¶Œí•œ ì ìˆ˜)
                primary_candidate = max(occurrences, key=lambda x: x.authority_score)
                
                # ë‚˜ë¨¸ì§€ëŠ” ì¤‘ë³µìœ¼ë¡œ ë¶„ë¥˜
                duplicates = [tag for tag in occurrences if tag != primary_candidate]
                
                duplicate_group = DuplicateGroup(
                    tag_pattern=pattern,
                    occurrences=occurrences,
                    primary_candidate=primary_candidate,
                    duplicates=duplicates
                )
                self.duplicate_groups.append(duplicate_group)
        
        print(f"âœ… {len(self.duplicate_groups)}ê°œì˜ ì¤‘ë³µ ê·¸ë£¹ ë°œê²¬")
    
    def analyze_duplicates(self) -> Dict:
        """ì¤‘ë³µ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (Reference duplicates êµ¬ë¶„)"""
        analysis = {
            "summary": {
                "total_tags": len(self.all_tags),
                "unique_patterns": len(set(f"{t.tag_type}:{t.domain}-{t.id_number}" for t in self.all_tags)),
                "duplicate_groups": len(self.duplicate_groups),
                "total_duplicate_occurrences": sum(len(g.duplicates) for g in self.duplicate_groups),
                "topline_duplicates": 0,
                "reference_duplicates": 0,
                "allowed_reference_duplicates": 0,
                "critical_duplicates": 0
            },
            "duplicate_groups": [],
            "recommended_actions": []
        }

        # ì •ì±… ì„¤ì • í™•ì¸
        ref_dup_allowed = self.config.get("deduplication_policy", {}).get("reference_duplicates", {}).get("allowed", False)

        for group in self.duplicate_groups:
            # ì¤‘ë³µ ìœ í˜• ë¶„ë¥˜
            has_topline = group.primary_candidate.is_topline or any(dup.is_topline for dup in group.duplicates)

            group_info = {
                "tag_pattern": group.tag_pattern,
                "total_occurrences": len(group.occurrences),
                "duplicate_type": "topline" if has_topline else "reference",
                "severity": "critical" if has_topline else ("info" if ref_dup_allowed else "warning"),
                "primary_candidate": {
                    "file": group.primary_candidate.file_path,
                    "line": group.primary_candidate.line_number,
                    "authority_score": group.primary_candidate.authority_score,
                    "is_topline": group.primary_candidate.is_topline
                },
                "duplicates": [
                    {
                        "file": dup.file_path,
                        "line": dup.line_number,
                        "authority_score": dup.authority_score,
                        "is_topline": dup.is_topline
                    }
                    for dup in group.duplicates
                ]
            }
            analysis["duplicate_groups"].append(group_info)

            # ì¤‘ë³µ ìœ í˜•ë³„ ì¹´ìš´íŠ¸
            if has_topline:
                analysis["summary"]["topline_duplicates"] += 1
                analysis["summary"]["critical_duplicates"] += 1
            else:
                if ref_dup_allowed:
                    analysis["summary"]["allowed_reference_duplicates"] += 1
                else:
                    analysis["summary"]["reference_duplicates"] += 1

        # ì¶”ì²œ ì•¡ì…˜ ìƒì„± (ì •ì±… ê¸°ë°˜)
        if analysis["summary"]["topline_duplicates"] > 0:
            analysis["recommended_actions"].append({
                "priority": "critical",
                "action": "renumber_or_remove_topline_duplicates",
                "description": f"{analysis['summary']['topline_duplicates']}ê°œì˜ topline ì¤‘ë³µì€ ì¦‰ì‹œ ì²˜ë¦¬ í•„ìš”"
            })

        # Reference duplicates ì²˜ë¦¬
        if ref_dup_allowed:
            if analysis["summary"]["allowed_reference_duplicates"] > 0:
                analysis["recommended_actions"].append({
                    "priority": "info",
                    "action": "acknowledge_reference_duplicates",
                    "description": f"{analysis['summary']['allowed_reference_duplicates']}ê°œì˜ ì°¸ì¡° ì¤‘ë³µì€ ì •ì±…ìƒ í—ˆìš©ë¨ (TRACEABILITYìš©)"
                })
        else:
            if analysis["summary"]["reference_duplicates"] > 0:
                analysis["recommended_actions"].append({
                    "priority": "warning",
                    "action": "review_reference_duplicates",
                    "description": f"{analysis['summary']['reference_duplicates']}ê°œì˜ ì°¸ì¡° ì¤‘ë³µ ê²€í†  í•„ìš”"
                })

        # ì •ì±… ì¤€ìˆ˜ ì—¬ë¶€ íŒë‹¨
        analysis["policy_compliance"] = {
            "compliant": analysis["summary"]["critical_duplicates"] == 0,
            "critical_issues": analysis["summary"]["topline_duplicates"],
            "allowed_reference_count": analysis["summary"]["allowed_reference_duplicates"]
        }

        return analysis
    
    def generate_correction_plan(self, analysis: Dict) -> Dict:
        """ìˆ˜ì • ê³„íš ìƒì„±"""
        correction_plan = {
            "timestamp": datetime.now().isoformat(),
            "dry_run": True,
            "corrections": [],
            "rollback_info": {
                "backup_strategy": "git_tag",
                "restore_command": "git checkout TAG-DEDUP-BACKUP"
            }
        }
        
        for group in self.duplicate_groups:
            # Primaryë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìˆ˜ì •/ì‚­ì œ
            primary = group.primary_candidate
            
            for duplicate in group.duplicates:
                if duplicate.is_topline:
                    # Topline ì¤‘ë³µì€ ì¬ë²ˆí˜¸í™” ë˜ëŠ” ì œê±°
                    new_tag = self._generate_new_tag(duplicate)
                    correction = {
                        "type": "renumber",
                        "file_path": duplicate.file_path,
                        "line_number": duplicate.line_number,
                        "old_tag": duplicate.tag,
                        "new_tag": new_tag,
                        "confidence": 0.9,
                        "impact": "medium"
                    }
                    correction_plan["corrections"].append(correction)
                else:
                    # ì°¸ì¡° ì¤‘ë³µì€ ì¶”ì ë§Œ
                    correction = {
                        "type": "track_only",
                        "file_path": duplicate.file_path,
                        "line_number": duplicate.line_number,
                        "tag": duplicate.tag,
                        "note": "Reference duplicate - no action needed"
                    }
                    correction_plan["corrections"].append(correction)
        
        return correction_plan
    
    def _generate_new_tag(self, tag: TagInfo) -> str:
        """ìƒˆë¡œìš´ TAG ìƒì„±"""
        # ë„ë©”ì¸ë³„ ì¹´ìš´í„° ê´€ë¦¬ (ê°„ë‹¨í•œ êµ¬í˜„)
        domain_counters = defaultdict(int)
        
        for existing_tag in self.all_tags:
            if existing_tag.domain == tag.domain and existing_tag.tag_type == tag.tag_type:
                domain_counters[existing_tag.domain] += 1
        
        new_id = domain_counters[tag.domain] + 1
        new_tag = f"@{tag.tag_type}:{tag.domain}-{new_id:03d}"
        
        return new_tag
    
    def save_analysis_report(self, analysis: Dict, output_path: str = None) -> None:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥"""
        if output_path is None:
            output_path = self.project_root / ".moai" / "reports" / f"tag-dedup-analysis-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"
        
        os.makedirs(output_path.parent, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")
    
    def run_full_scan(self) -> Dict:
        """ì „ì²´ ìŠ¤ìº” ì‹¤í–‰"""
        print("ğŸš€ TAG ì¤‘ë³µ íƒì§€ ì‹œì‘...")
        print(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {self.project_root}")
        
        # 1. ì „ì²´ íŒŒì¼ ìŠ¤ìº”
        self.scan_all_files()
        
        # 2. ì¤‘ë³µ íƒì§€
        self.find_duplicates()
        
        # 3. ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        analysis = self.analyze_duplicates()
        
        # 4. ë¦¬í¬íŠ¸ ì €ì¥
        self.save_analysis_report(analysis)
        
        # 5. ìš”ì•½ ì¶œë ¥
        self._print_summary(analysis)
        
        return analysis
    
    def _print_summary(self, analysis: Dict) -> None:
        """ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        summary = analysis["summary"]
        
        print("\n" + "="*60)
        print("ğŸ“Š TAG ì¤‘ë³µ ë¶„ì„ ìš”ì•½")
        print("="*60)
        print(f"ì´ TAG ìˆ˜: {summary['total_tags']}")
        print(f"ê³ ìœ  TAG íŒ¨í„´: {summary['unique_patterns']}")
        print(f"ì¤‘ë³µ ê·¸ë£¹: {summary['duplicate_groups']}")
        print(f"ì´ ì¤‘ë³µ ë°œìƒ: {summary['total_duplicate_occurrences']}")
        print(f"Topline ì¤‘ë³µ: {summary['topline_duplicates']} (ì¹˜ëª…ì )")
        print(f"ì°¸ì¡° ì¤‘ë³µ: {summary['reference_duplicates']} (ì •ë³´)")
        
        print("\nğŸ¯ ì¶”ì²œ ì•¡ì…˜:")
        for action in analysis["recommended_actions"]:
            priority_emoji = "ğŸ”´" if action["priority"] == "critical" else "ğŸ”µ"
            print(f"{priority_emoji} {action['description']}")
        
        print("\nğŸ“ˆ ìƒìœ„ ì¤‘ë³µ ê·¸ë£¹:")
        for group in sorted(analysis["duplicate_groups"], key=lambda x: x["total_occurrences"], reverse=True)[:5]:
            print(f"  â€¢ {group['tag_pattern']}: {group['total_occurrences']}íšŒ ë°œìƒ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="MoAI-ADK TAG ì¤‘ë³µ íƒì§€ê¸°")
    parser.add_argument("--config", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--dry-run", action="store_true", default=True, help="ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ")
    
    args = parser.parse_args()
    
    try:
        detector = TagDedupDetector(args.config)
        analysis = detector.run_full_scan()
        
        if analysis["summary"]["topline_duplicates"] > 0:
            print(f"\nâš ï¸  {analysis['summary']['topline_duplicates']}ê°œì˜ ì¹˜ëª…ì  topline ì¤‘ë³µ ë°œê²¬!")
            print("ìˆ˜ì •ì„ ìœ„í•´ì„œëŠ” --apply-corrections ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            return 1
        else:
            print("\nâœ… ì¹˜ëª…ì  ì¤‘ë³µ ì—†ìŒ")
            return 0
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
