#!/usr/bin/env python3
"""
Skill Factory Recovery Script
Safe memory-efficient processing for Enterprise v4.0 compliance
Replaces Bun-based processor that caused Segmentation fault
"""

import os
import sys
import json
import time
import logging
import argparse
import traceback
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import gc

class MemoryEfficientSkillProcessor:
    """Memory-efficient SKILL.md processor with batch management"""
    
    def __init__(self, batch_size: int = 10, memory_limit_gb: float = 4.0):
        self.batch_size = batch_size
        self.memory_limit_bytes = memory_limit_gb * 1024 * 1024 * 1024
        self.processed_count = 0
        self.failed_count = 0
        self.current_batch = 0
        
        # Setup logging
        self.setup_logging()
        
        # Validate environment
        self.validate_environment()
        
    def setup_logging(self):
        """Configure comprehensive logging"""
        log_dir = Path(".moai/logs/recovery")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"skill_factory_recovery_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("=== Skill Factory Recovery Started ===")
        
    def validate_environment(self):
        """Validate processing environment"""
        try:
            import psutil
            available_memory = psutil.virtual_memory().available
            
            if available_memory < self.memory_limit_bytes:
                self.logger.warning(f"Low memory detected: {available_memory // (1024**3):.1f}GB available, {self.memory_limit_bytes // (1024**3):.1f}GB requested")
                
            self.logger.info(f"Environment validated - Available memory: {available_memory // (1024**3):.1f}GB")
            
        except ImportError:
            self.logger.warning("psutil not available - memory monitoring disabled")
            
    def get_skill_files(self) -> List[Path]:
        """Get all SKILL.md files prioritized by importance"""
        skills_dir = Path(".claude/skills")
        
        if not skills_dir.exists():
            raise FileNotFoundError(f"Skills directory not found: {skills_dir}")
            
        skill_files = list(skills_dir.glob("*/SKILL.md"))
        
        # Sort by priority (Alfred Core first, then Domain, then others)
        def priority_sort(path):
            name = path.parent.name.lower()
            if name.startswith('moai-alfred-'):
                return (0, name)  # Highest priority
            elif name.startswith('moai-domain-'):
                return (1, name)  # Second priority
            elif name.startswith('moai-lang-'):
                return (2, name)  # Third priority
            elif name.startswith('moai-essentials-'):
                return (3, name)  # Fourth priority
            else:
                return (4, name)  # Lowest priority
                
        skill_files.sort(key=priority_sort)
        
        self.logger.info(f"Found {len(skill_files)} SKILL.md files to process")
        return skill_files
        
    def check_memory_usage(self):
        """Monitor memory usage and force GC if needed"""
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            if memory_mb > self.memory_limit_bytes / 1024 / 1024:
                self.logger.warning(f"Memory usage high: {memory_mb:.1f}MB - forcing garbage collection")
                gc.collect()
                
            return memory_mb
        except ImportError:
            return None
            
    def validate_skill_file(self, skill_path: Path) -> Dict[str, any]:
        """Validate SKILL.md file for Enterprise v4.0 compliance"""
        validation_result = {
            'path': str(skill_path),
            'valid': True,
            'issues': [],
            'stats': {},
            'enterprise_v4_compliant': False
        }
        
        try:
            with open(skill_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            lines = content.split('\n')
            line_count = len(lines)
            file_size = len(content.encode('utf-8'))
            
            validation_result['stats'] = {
                'lines': line_count,
                'size_bytes': file_size,
                'size_kb': file_size / 1024
            }
            
            # Enterprise v4.0 checks
            issues = []
            
            # Size limits
            if line_count > 500:
                issues.append(f"File too long: {line_count} lines (max: 500)")
                
            if file_size > 100 * 1024:  # 100KB
                issues.append(f"File too large: {file_size / 1024:.1f}KB (max: 100KB)")
                
            # Required sections for Enterprise v4.0
            required_sections = ['Quick Start', 'Implementation', 'Advanced', 'Security & Compliance']
            missing_sections = []
            
            for section in required_sections:
                if section not in content:
                    missing_sections.append(section)
                    
            if missing_sections:
                issues.append(f"Missing Enterprise v4.0 sections: {', '.join(missing_sections)}")
                
            # YAML frontmatter check
            if not content.startswith('---'):
                issues.append("Missing YAML frontmatter")
                
            # Link validation (basic)
            if '[Skill(' in content and '](Skill(' not in content:
                issues.append("Invalid Skill reference format detected")
                
            validation_result['issues'] = issues
            validation_result['valid'] = len(issues) == 0
            validation_result['enterprise_v4_compliant'] = len(missing_sections) == 0 and line_count <= 500
            
        except Exception as e:
            validation_result['valid'] = False
            validation_result['issues'].append(f"Error reading file: {str(e)}")
            self.logger.error(f"Error validating {skill_path}: {e}")
            
        return validation_result
        
    def process_skill_file(self, skill_path: Path) -> bool:
        """Process individual skill file for Enterprise v4.0 compliance"""
        try:
            # Validate current state
            validation = self.validate_skill_file(skill_path)
            
            if validation['enterprise_v4_compliant']:
                self.logger.info(f"âœ“ {skill_path.parent.name} - Already compliant")
                return True
                
            # If not compliant, process for compliance
            self.logger.info(f"â†’ {skill_path.parent.name} - Processing for Enterprise v4.0")
            
            # Read content
            with open(skill_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Check if content is too long and needs optimization
            if validation['stats']['lines'] > 500:
                self.logger.warning(f"  â†’ File too long ({validation['stats']['lines']} lines), requires manual review")
                # For now, just log but don't auto-modify
                return False
                
            # Here would be the actual Enterprise v4.0 transformation logic
            # For recovery, we'll just validate and report status
            
            self.logger.info(f"  âœ“ {skill_path.parent.name} - Validated")
            return True
            
        except Exception as e:
            self.logger.error(f"âœ— {skill_path.parent.name} - Error: {e}")
            return False
            
    def process_batch(self, batch_files: List[Path]) -> Dict[str, int]:
        """Process a batch of skill files"""
        batch_start = time.time()
        batch_results = {'success': 0, 'failed': 0, 'skipped': 0}
        
        self.logger.info(f"Processing Batch {self.current_batch + 1}: {len(batch_files)} files")
        
        for skill_path in batch_files:
            try:
                # Memory check before processing
                memory_mb = self.check_memory_usage()
                if memory_mb and memory_mb > 3500:  # 3.5GB warning threshold
                    self.logger.warning(f"High memory usage: {memory_mb:.1f}MB")
                    
                # Process file
                if self.process_skill_file(skill_path):
                    batch_results['success'] += 1
                else:
                    batch_results['failed'] += 1
                    
                self.processed_count += 1
                
                # Force garbage collection after each file
                gc.collect()
                
            except KeyboardInterrupt:
                self.logger.info("Batch processing interrupted by user")
                break
            except Exception as e:
                self.logger.error(f"Unexpected error processing {skill_path}: {e}")
                batch_results['failed'] += 1
                self.failed_count += 1
                
        batch_duration = time.time() - batch_start
        self.logger.info(f"Batch {self.current_batch + 1} completed in {batch_duration:.1f}s - "
                        f"Success: {batch_results['success']}, Failed: {batch_results['failed']}")
                        
        return batch_results
        
    def run_recovery(self, max_batches: Optional[int] = None):
        """Main recovery execution"""
        try:
            skill_files = self.get_skill_files()
            total_files = len(skill_files)
            
            if max_batches:
                total_files = min(total_files, max_batches * self.batch_size)
                skill_files = skill_files[:total_files]
                
            total_batches = (len(skill_files) + self.batch_size - 1) // self.batch_size
            
            self.logger.info(f"Starting recovery: {len(skill_files)} files in {total_batches} batches")
            
            for batch_start in range(0, len(skill_files), self.batch_size):
                if max_batches and self.current_batch >= max_batches:
                    break
                    
                batch_end = min(batch_start + self.batch_size, len(skill_files))
                batch_files = skill_files[batch_start:batch_end]
                
                batch_results = self.process_batch(batch_files)
                self.current_batch += 1
                
                # Brief pause between batches for system recovery
                time.sleep(1)
                
            # Final report
            self.logger.info("=== Recovery Complete ===")
            self.logger.info(f"Total processed: {self.processed_count}")
            self.logger.info(f"Total failed: {self.failed_count}")
            self.logger.info(f"Success rate: {((self.processed_count - self.failed_count) / self.processed_count * 100):.1f}%")
            
            # Generate completion report
            self.generate_recovery_report(skill_files)
            
        except Exception as e:
            self.logger.error(f"Recovery failed: {e}")
            self.logger.error(traceback.format_exc())
            raise
            
    def generate_recovery_report(self, processed_files: List[Path]):
        """Generate detailed recovery report"""
        report_dir = Path(".moai/reports/recovery")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"skill_factory_recovery_report_{timestamp}.md"
        
        report_content = f"""# Skill Factory Recovery Report

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Total Files**: {len(processed_files)}  
**Processed**: {self.processed_count}  
**Failed**: {self.failed_count}  
**Success Rate**: {((self.processed_count - self.failed_count) / self.processed_count * 100):.1f}%  

## Recovery Configuration

- **Batch Size**: {self.batch_size} files
- **Memory Limit**: {self.memory_limit_bytes / (1024**3):.1f}GB
- **Batches Processed**: {self.current_batch}

## Files Processed

"""
        
        for skill_path in processed_files:
            validation = self.validate_skill_file(skill_path)
            status = "âœ… Compliant" if validation['enterprise_v4_compliant'] else "âš ï¸ Needs Review"
            report_content += f"- {skill_path.parent.name}: {status} ({validation['stats']['lines']} lines)\n"
            
        report_content += f"""

## Next Steps

1. **Manual Review Required**: Files marked as "Needs Review" require manual optimization
2. **Size Reduction**: Files > 500 lines need content consolidation
3. **Enterprise v4.0 Compliance**: Ensure all sections are properly structured
4. **Validation**: Run `moai-skill-validator` on all processed files

## Recovery Log

Complete recovery log available at: `.moai/logs/recovery/skill_factory_recovery_{timestamp}.log`

---
ðŸ¤– Generated with Skill Factory Recovery Script  
Replaces Bun-based processor (Segmentation fault recovery)
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
            
        self.logger.info(f"Recovery report generated: {report_file}")
        

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Skill Factory Recovery - Enterprise v4.0 Compliance')
    parser.add_argument('--batch-size', type=int, default=10, 
                       help='Number of files per batch (default: 10)')
    parser.add_argument('--max-batches', type=int, 
                       help='Maximum number of batches to process')
    parser.add_argument('--memory-limit', type=float, default=4.0,
                       help='Memory limit in GB (default: 4.0)')
    parser.add_argument('--dry-run', action='store_true',
                       help='Validate only, no modifications')
    
    args = parser.parse_args()
    
    try:
        processor = MemoryEfficientSkillProcessor(
            batch_size=args.batch_size,
            memory_limit_gb=args.memory_limit
        )
        
        if args.dry_run:
            processor.logger.info("DRY RUN MODE - Validating only")
            
        processor.run_recovery(max_batches=args.max_batches)
        
    except KeyboardInterrupt:
        print("\nRecovery interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"Recovery failed: {e}")
        sys.exit(1)
        

if __name__ == "__main__":
    main()
