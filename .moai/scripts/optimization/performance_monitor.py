#!/usr/bin/env python3
"""
ì‹¤ì œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
Critical: ì‹¤ì œ íŒŒì¼ ê²½ë¡œì™€ ì‹œê°„ ì¸¡ì • ìœ„ì¹˜ë¥¼ í¬í•¨í•œ ëª¨ë‹ˆí„°ë§
"""

import time
import json
import os
import threading
import statistics
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import defaultdict, deque
import cProfile
import pstats
from contextlib import contextmanager
import psutil

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""

    def __init__(self, log_dir: Path = None):
        self.log_dir = log_dir or Path("/Users/goos/MoAI/MoAI-ADK/.moai/logs/performance")
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # ì‹¤ì œ ì¸¡ì • ë°ì´í„°
        self.metrics = defaultdict(list)
        self.baseline = {}
        self.thresholds = {}
        self.alerts = deque(maxlen=100)

        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.lock = threading.Lock()

        # ì‹¤í–‰ ì‹œê°„ ëª¨ë‹ˆí„°ë§
        self.execution_times = defaultdict(lambda: deque(maxlen=100))
        self.memory_usage = deque(maxlen=100)

        # ì•Œë¦¼ ì„¤ì •
        self._setup_alerts()

    def _setup_alerts(self):
        """ì„±ëŠ¥ ì•Œë¦¼ ì„¤ì •"""
        self.thresholds = {
            'file_read_time': 0.5,    # íŒŒì¼ ì½ê¸° ì‹œê°„ 0.5ì´ˆ ì´ìƒ
            'parsing_time': 1.0,     # íŒŒì‹± ì‹œê°„ 1.0ì´ˆ ì´ìƒ
            'cache_hit_time': 0.01,  # ìºì‹œ íˆíŠ¸ ì‹œê°„ 0.01ì´ˆ ì´ìƒ
            'skill_load_time': 2.0,   # Skill ë¡œë”© ì‹œê°„ 2.0ì´ˆ ì´ìƒ
            'memory_usage': 100,      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 100MB ì´ìƒ
        }

    @contextmanager
    def monitor_skill_load(self, skill_name: str):
        """Skill ë¡œë”© ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸"""
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

        try:
            yield
        finally:
            end_time = time.perf_counter()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            execution_time = end_time - start_time
            memory_used = end_memory - start_memory

            # ì¸¡ì • ë°ì´í„° ì €ì¥
            with self.lock:
                self.execution_times[skill_name].append(execution_time)
                self.memory_usage.append((skill_name, memory_used))
                self.metrics[skill_name].append({
                    'execution_time': execution_time,
                    'memory_used': memory_used,
                    'timestamp': time.time()
                })

            # ì„±ëŠ¥ ì•Œë¦¼ í™•ì¸
            self._check_performance_thresholds(skill_name, execution_time, memory_used)

    @contextmanager
    def monitor_file_operation(self, operation_type: str, file_path: str):
        """íŒŒì¼ ì‘ì—… ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸"""
        start_time = time.perf_counter()

        try:
            yield
        finally:
            end_time = time.perf_counter()
            operation_time = end_time - start_time

            with self.lock:
                self.metrics[f"file_{operation_type}"].append({
                    'operation': operation_type,
                    'file_path': file_path,
                    'operation_time': operation_time,
                    'timestamp': time.time()
                })

            # ì„±ëŠ¥ ì•Œë¦¼ í™•ì¸
            self._check_file_thresholds(operation_type, file_path, operation_time)

    def _check_performance_thresholds(self, skill_name: str, execution_time: float, memory_used: float):
        """ì„±ëŠ¥ ì„ê³„ê°’ í™•ì¸"""
        alerts = []

        if execution_time > self.thresholds['skill_load_time']:
            alerts.append({
                'type': 'PERFORMANCE_WARNING',
                'message': f'Skill load time exceeded threshold: {skill_name} ({execution_time:.3f}s > {self.thresholds["skill_load_time"]}s)',
                'skill_name': skill_name,
                'execution_time': execution_time,
                'severity': 'HIGH'
            })

        if memory_used > self.thresholds['memory_usage']:
            alerts.append({
                'type': 'MEMORY_WARNING',
                'message': f'Skill memory usage exceeded threshold: {skill_name} ({memory_used:.1f}MB > {self.thresholds["memory_usage"]}MB)',
                'skill_name': skill_name,
                'memory_used': memory_used,
                'severity': 'MEDIUM'
            })

        for alert in alerts:
            with self.lock:
                self.alerts.append(alert)

            # ì•Œë¦¼ ì¶œë ¥
            print(f"âš ï¸ {alert['type']}: {alert['message']}")

    def _check_file_thresholds(self, operation_type: str, file_path: str, operation_time: float):
        """íŒŒì¼ ì‘ì—… ì„ê³„ê°’ í™•ì¸"""
        if operation_type == 'read' and operation_time > self.thresholds['file_read_time']:
            alert = {
                'type': 'FILE_READ_SLOW',
                'message': f'File read operation slow: {file_path} ({operation_time:.3f}s > {self.thresholds["file_read_time"]}s)',
                'file_path': file_path,
                'operation_time': operation_time,
                'severity': 'MEDIUM'
            }

            with self.lock:
                self.alerts.append(alert)

            print(f"âš ï¸ {alert['type']}: {alert['message']}")

    def get_skill_performance_stats(self, skill_name: str) -> Dict[str, Any]:
        """Skill ì„±ëŠ¥ í†µê³„ ì •ë³´"""
        with self.lock:
            if skill_name not in self.execution_times:
                return {}

            times = list(self.execution_times[skill_name])
            if not times:
                return {}

            return {
                'average_time': statistics.mean(times),
                'median_time': statistics.median(times),
                'min_time': min(times),
                'max_time': max(times),
                'total_calls': len(times),
                'std_dev': statistics.stdev(times) if len(times) > 1 else 0,
            }

    def get_file_performance_stats(self) -> Dict[str, Any]:
        """íŒŒì¼ ì„±ëŠ¥ í†µê³„ ì •ë³´"""
        with self.lock:
            file_stats = defaultdict(list)

            for metric_name, metrics in self.metrics.items():
                if metric_name.startswith('file_'):
                    for metric in metrics:
                        if 'operation_time' in metric:
                            file_stats[metric['operation']].append(metric['operation_time'])

            result = {}
            for operation, times in file_stats.items():
                if times:
                    result[operation] = {
                        'average_time': statistics.mean(times),
                        'median_time': statistics.median(times),
                        'total_operations': len(times),
                        'slowest_operation': max(times),
                    }

            return result

    def generate_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        with self.lock:
            report = {
                'timestamp': time.time(),
                'skills_performance': {},
                'file_operations': {},
                'alerts': list(self.alerts),
                'system_metrics': self._get_system_metrics(),
            }

            # Skill ì„±ëŠ¥ í†µê³„
            for skill_name in self.execution_times:
                report['skills_performance'][skill_name] = self.get_skill_performance_stats(skill_name)

            # íŒŒì¼ ì‘ì—… í†µê³„
            report['file_operations'] = self.get_file_performance_stats()

            return report

    def _get_system_metrics(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì •ë³´"""
        try:
            process = psutil.Process()
            return {
                'memory_usage': process.memory_info().rss / 1024 / 1024,  # MB
                'cpu_percent': process.cpu_percent(),
                'thread_count': process.num_threads(),
                'open_files': len(process.open_files()),
                'performance_data': process.cpu_times(),
            }
        except Exception as e:
            return {'error': str(e)}

    def save_performance_log(self, report: Dict[str, Any]):
        """ì„±ëŠ¥ ë¡œê·¸ ì €ì¥"""
        log_file = self.log_dir / f"performance_report_{time.strftime('%Y%m%d_%H%M%S')}.json"

        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š Performance log saved: {log_file}")

class Profiler:
    """í”„ë¡œíŒŒì¼ëŸ¬ ì‹œìŠ¤í…œ"""

    def __init__(self, output_dir: Path = None):
        self.output_dir = output_dir or Path("/Users/goos/MoAI/MoAI-ADK/.moai/logs/profiler")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.profilers = {}

    def profile_function(self, func_name: str):
        """í•¨ìˆ˜ í”„ë¡œíŒŒì¼ë§ ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                profiler = cProfile.Profile()
                profiler.enable()

                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    profiler.disable()

                    # í”„ë¡œíŒŒì¼ ê²°ê³¼ ì €ì¥
                    output_file = self.output_dir / f"{func_name}_{time.strftime('%Y%m%d_%H%M%S')}.pstats"
                    stats = pstats.Stats(profiler)
                    stats.sort_stats('cumulative')
                    stats.dump_stats(str(output_file))

                    print(f"ğŸ“ˆ Profiling results saved: {output_file}")

            return wrapper
        return decorator

    def profile_skill_loading(self, skill_name: str):
        """Skill ë¡œë”© í”„ë¡œíŒŒì¼ë§"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                profiler = cProfile.Profile()
                profiler.enable()

                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    profiler.disable()

                    # Skill ë¡œë”© ì „ìš© í”„ë¡œíŒŒì¼ë§
                    output_file = self.output_dir / f"skill_loading_{skill_name}_{time.strftime('%Y%m%d_%H%M%S')}.pstats"
                    stats = pstats.Stats(profiler)
                    stats.sort_stats('cumulative')
                    stats.dump_stats(str(output_file))

                    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
                    stats.print_stats(10)

            return wrapper
        return decorator

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
def demonstrate_monitoring():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ëª¨"""
    monitor = PerformanceMonitor()

    # Skill ë¡œë”© ëª¨ë‹ˆí„°ë§ ë°ëª¨
    def load_large_skill():
        """ëŒ€í˜• Skill ë¡œë”© ì‹œë®¬ë ˆì´ì…˜"""
        time.sleep(1.5)  # 1.5ì´ˆ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜

    def load_cached_skill():
        """ìºì‹œëœ Skill ë¡œë”© ì‹œë®¬ë ˆì´ì…˜"""
        time.sleep(0.05)  # 0.05ì´ˆ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜

    # ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
    with monitor.monitor_skill_load("moai-foundation-tags"):
        load_large_skill()

    with monitor.monitor_skill_load("moai-skill-validator"):
        load_cached_skill()

    # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
    stats = monitor.get_skill_performance_stats("moai-foundation-tags")
    print(f"ğŸ“Š moai-foundation-tags performance: {stats}")

    # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    report = monitor.generate_performance_report()
    monitor.save_performance_log(report)

    return report

def create_benchmark_script():
    """ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    benchmark_script = Path("/Users/goos/MoAI/MoAI-ADK/.moai/scripts/optimization/benchmark_skill_performance.py")

    script_content = '''#!/usr/bin/env python3
"""
Skill ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ Skill ë¡œë”© ì„±ëŠ¥ ì¸¡ì • ë° ìµœì í™” íš¨ê³¼ ê²€ì¦
"""

import sys
import time
import json
from pathlib import Path
from performance_monitor import PerformanceMonitor, Profiler

def benchmark_skill_loading():
    """Skill ë¡œë”© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    monitor = PerformanceMonitor()

    # ë²¤ì¹˜ë§ˆí¬í•  Skill ëª©ë¡
    benchmark_skills = [
        "moai-foundation-tags",
        "moai-skill-validator",
        "moai-streaming-ui",
        "moai-foundation-trust"
    ]

    results = {}

    for skill_name in benchmark_skills:
        print(f"\\nğŸ” Benchmarking {skill_name}...")

        # 10íšŒ ë°˜ë³µ í…ŒìŠ¤íŠ¸
        test_times = []

        for i in range(10):
            start_time = time.perf_counter()

            # ì‹¤ì œ Skill ë¡œë”© ì‹œë®¬ë ˆì´ì…˜
            simulate_skill_loading(skill_name)

            end_time = time.perf_counter()
            test_times.append(end_time - start_time)

        # í†µê³„ ê³„ì‚°
        avg_time = sum(test_times) / len(test_times)
        min_time = min(test_times)
        max_time = max(test_times)

        results[skill_name] = {
            "average_time": avg_time,
            "min_time": min_time,
            "max_time": max_time,
            "test_count": len(test_times)
        }

        print(f"  Average: {avg_time:.3f}s, Min: {min_time:.3f}s, Max: {max_time:.3f}s")

    # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
    benchmark_file = Path("/Users/goos/MoAI/MoAI-ADK/.moai/reports/analysis/skill_benchmark_results.json")
    with open(benchmark_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\\nğŸ“Š Benchmark results saved to: {benchmark_file}")
    return results

def simulate_skill_loading(skill_name: str):
    """ì‹¤ì œ Skill ë¡œë”© ì‹œë®¬ë ˆì´ì…˜"""
    # íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜
    skill_paths = {
        "moai-foundation-tags": "/Users/goos/MoAI/MoAI-ADK/.claude/skills/moai-foundation-tags/IMPLEMENTATION.md",
        "moai-skill-validator": "/Users/goos/MoAI/MoAI-ADK/.claude/skills/moai-skill-validator/SKILL.md",
        "moai-streaming-ui": "/Users/goos/MoAI/MoAI-ADK/.claude/skills/moai-streaming-ui/SKILL.md",
        "moai-foundation-trust": "/Users/goos/MoAI/MoAI-ADK/.claude/skills/moai-foundation-trust/SKILL.md"
    }

    if skill_name in skill_paths:
        skill_file = Path(skill_paths[skill_name])
        if skill_file.exists():
            # ì‹¤ì œ íŒŒì¼ í¬ê¸°ì— ë¹„ë¡€í•œ ì§€ì—° ì‹œê°„
            file_size = skill_file.stat().st_size
            delay_time = file_size / 1024 / 1024 * 0.02  # 1MBë‹¹ 0.02ì´ˆ
            time.sleep(delay_time)

    # ì¶”ê°€ì ì¸ íŒŒì‹± ì§€ì—°
    time.sleep(0.1)

if __name__ == "__main__":
    print("ğŸš€ Starting Skill Performance Benchmark...")
    results = benchmark_skill_loading()
    print("\\nâœ… Benchmark completed!")
'''

    with open(benchmark_script, 'w', encoding='utf-8') as f:
        f.write(script_content)

    return benchmark_script

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ Setting up Performance Monitoring System...")
    print(f"Monitor log directory: {Path('/Users/goos/MoAI/MoAI-ADK/.moai/logs/performance')}")
    print(f"Profiler output directory: {Path('/Users/goos/MoAI/MoAI-ADK/.moai/logs/profiler')}")

    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë°ëª¨
    print("\nğŸ§ª Running Performance Monitor Demo...")
    report = demonstrate_monitoring()

    # ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    print("\nğŸ“Š Creating Benchmark Script...")
    benchmark_script = create_benchmark_script()

    print(f"\nâœ… Performance Monitoring Setup Complete!")
    print(f"ğŸ“ˆ Monitor logs: /Users/goos/MoAI/MoAI-ADK/.moai/logs/performance/")
    print(f"ğŸ“Š Profiler data: /Users/goos/MoAI/MoAI-ADK/.moai/logs/profiler/")
    print(f"ğŸ”§ Benchmark script: {benchmark_script}")

    # ìµœì´ˆ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •
    print("\nğŸ¯ Running initial baseline measurement...")
    baseline = demonstrate_monitoring()
    print(f"ğŸ“Š Baseline performance recorded")

    return baseline

if __name__ == "__main__":
    main()