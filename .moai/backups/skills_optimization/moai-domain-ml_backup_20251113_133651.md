---
name: moai-domain-ml
version: 4.0.0
status: production
description: |
  Enterprise ML with TensorFlow 2.20.0, PyTorch 2.9.0, Scikit-learn 1.7.2.
  Master AutoML, neural architecture search, MLOps automation, and production deployment.
  Build scalable ML pipelines with comprehensive monitoring and experiment tracking.
allowed-tools: ["Read", "Write", "Edit", "Bash", "Glob", "WebFetch", "WebSearch"]
tags: ["machine-learning", "tensorflow", "pytorch", "scikit-learn", "automl", "mlops", "deep-learning"]
---

# Enterprise Machine Learning

**Production ML Expert**

> **Core Technologies**: TensorFlow 2.20.0, PyTorch 2.9.0, Scikit-learn 1.7.2  
> **AutoML**: H2O AutoML 3.44.0, AutoGluon 1.0.0, TPOT 0.12.2  
> **MLOps**: MLflow 2.9.0, Kubeflow 1.8.0, DVC 3.48.0  
> **Deployment**: ONNX 1.16.0, TensorFlow Serving, TorchServe

---

## ðŸ“– Progressive Disclosure

### Level 1: Quick Reference (70 lines)

#### Core Capabilities

**Deep Learning Frameworks**:
- **TensorFlow 2.20.0**: Modern Keras API, AutoGraph, distributed training
- **PyTorch 2.9.0**: Dynamic graphs, TorchScript, production deployment
- **JAX 0.4.33**: Functional programming, TPU acceleration

**Classical ML & AutoML**:
- **Scikit-learn 1.7.2**: Comprehensive ML algorithms and preprocessing
- **XGBoost 2.0.3**, **LightGBM 4.4.0**: Gradient boosting for tabular data
- **AutoML Platforms**: H2O AutoML, AutoGluon, TPOT for automated model selection

**MLOps & Deployment**:
- **MLflow 2.9.0**: Experiment tracking, model registry, deployment
- **FastAPI**: Scalable model serving with REST APIs
- **Docker/Kubernetes**: Containerized deployment and orchestration

**Quick Start - TensorFlow 2.20.0**:
```python
import tensorflow as tf
from tensorflow import keras

# Modern Keras neural network
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Training with callbacks
callbacks = [
    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),
    keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)
]

# model.fit(X_train, y_train, validation_data=(X_val, y_val), 
#           epochs=100, batch_size=32, callbacks=callbacks)
```

**Quick Start - PyTorch 2.9.0**:
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, num_classes)
        )
    
    def forward(self, x):
        return self.layers(x)

# Device-agnostic training
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = NeuralNetwork(784, 128, 10).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
```

**When to Use This Skill**:
- âœ… Building deep learning models for images, text, or sequences
- âœ… Implementing AutoML for automated model selection
- âœ… Deploying ML models to production with monitoring
- âœ… Setting up MLOps pipelines and experiment tracking
- âœ… Neural architecture search and hyperparameter optimization

### Level 2: Core Patterns (180 lines)

#### Essential Implementation Patterns

**1. ML Data Processing Pipeline**:
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

class MLDataProcessor:
    def __init__(self):
        self.numeric_features = []
        self.categorical_features = []
        self.preprocessor = None
        self.feature_names = []
    
    def fit(self, X):
        """Build preprocessing pipeline"""
        # Identify feature types
        self.numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
        self.categorical_features = X.select_dtypes(include=['object']).columns.tolist()
        
        # Numeric preprocessing
        numeric_transformer = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        # Categorical preprocessing
        categorical_transformer = Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        # Column transformer
        self.preprocessor = ColumnTransformer([
            ('num', numeric_transformer, self.numeric_features),
            ('cat', categorical_transformer, self.categorical_features)
        ])
        
        return self.preprocessor.fit(X)
    
    def transform(self, X):
        """Transform data using fitted pipeline"""
        return self.preprocessor.transform(X)
    
    def fit_transform(self, X):
        """Fit and transform in one step"""
        return self.fit(X).transform(X)

# Usage
# processor = MLDataProcessor()
# X_train_processed = processor.fit_transform(X_train)
# X_test_processed = processor.transform(X_test)
```

**2. Experiment Tracking with MLflow**:
```python
import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient

class ExperimentTracker:
    def __init__(self, experiment_name: str):
        mlflow.set_experiment(experiment_name)
        self.experiment_name = experiment_name
        self.client = MlflowClient()
    
    def log_experiment(self, model, X_train, X_val, y_train, y_val, 
                      model_params: dict, metrics: dict = None, tags: dict = None):
        """Log complete ML experiment with MLflow"""
        
        with mlflow.start_run() as run:
            # Log parameters
            for param, value in model_params.items():
                mlflow.log_param(param, value)
            
            # Log tags
            if tags:
                mlflow.set_tags(tags)
            
            # Train model
            model.fit(X_train, y_train)
            
            # Evaluate
            train_score = model.score(X_train, y_train)
            val_score = model.score(X_val, y_val)
            
            # Log metrics
            mlflow.log_metric("train_accuracy", train_score)
            mlflow.log_metric("val_accuracy", val_score)
            
            if metrics:
                for metric, value in metrics.items():
                    mlflow.log_metric(metric, value)
            
            # Log model
            mlflow.sklearn.log_model(model, "model")
            
            # Log feature importance if available
            if hasattr(model, 'feature_importances_'):
                feature_importance = dict(zip(range(len(model.feature_importances_)), 
                                           model.feature_importances_))
                mlflow.log_dict(feature_importance, "feature_importance.json")
            
            return run.info.run_id
    
    def compare_experiments(self, metric: str = 'val_accuracy', top_n: int = 5):
        """Compare experiments and return top performers"""
        
        experiment = self.client.get_experiment_by_name(self.experiment_name)
        runs = self.client.search_runs(
            experiment_ids=[experiment.experiment_id],
            order_by=[f"metrics.{metric} DESC"]
        )
        
        results = []
        for run in runs[:top_n]:
            results.append({
                'run_id': run.info.run_id,
                'metrics': dict(run.data.metrics),
                'params': dict(run.data.params)
            })
        
        return results

# Usage
# tracker = ExperimentTracker("text_classification")
# run_id = tracker.log_experiment(
#     model=rf_model, X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val,
#     model_params={'n_estimators': 100, 'max_depth': 10},
#     tags={'model_type': 'random_forest'}
# )
```

**3. AutoML with Hyperparameter Optimization**:
```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np

class AutoMLPipeline:
    def __init__(self, task_type='classification', random_state=42):
        self.task_type = task_type
        self.random_state = random_state
        self.best_model = None
        self.best_score = None
        
        # Define model search space
        self.models = {
            'random_forest': {
                'model': RandomForestClassifier(random_state=random_state),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [5, 10, 15, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            },
            'gradient_boosting': {
                'model': GradientBoostingClassifier(random_state=random_state),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'max_depth': [3, 5, 7],
                    'subsample': [0.8, 0.9, 1.0]
                }
            },
            'logistic_regression': {
                'model': LogisticRegression(random_state=random_state, max_iter=1000),
                'params': {
                    'C': [0.1, 1.0, 10.0],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear', 'saga']
                }
            }
        }
    
    def search_best_model(self, X_train, X_val, y_train, y_val, 
                         search_method='random', n_iter=50, cv=5):
        """AutoML hyperparameter optimization"""
        
        best_score = -np.inf
        best_model = None
        
        for model_name, model_config in self.models.items():
            # Hyperparameter search
            search = RandomizedSearchCV(
                model_config['model'],
                model_config['params'],
                n_iter=n_iter,
                cv=cv,
                scoring='accuracy',
                random_state=self.random_state,
                n_jobs=-1
            )
            
            search.fit(X_train, y_train)
            
            # Evaluate on validation set
            val_score = search.score(X_val, y_val)
            
            if val_score > best_score:
                best_score = val_score
                best_model = search.best_estimator_
        
        self.best_model = best_model
        self.best_score = best_score
        
        return best_model
    
    def ensemble_models(self, X_train, y_train, top_k=3):
        """Create ensemble of top models"""
        
        # Note: This would need search results from previous step
        from sklearn.ensemble import VotingClassifier
        
        # Create ensemble (simplified example)
        ensemble = VotingClassifier([
            ('rf', RandomForestClassifier(n_estimators=100)),
            ('gb', GradientBoostingClassifier(n_estimators=100)),
            ('lr', LogisticRegression(max_iter=1000))
        ], voting='soft')
        
        ensemble.fit(X_train, y_train)
        return ensemble

# Usage
# automl = AutoMLPipeline(task_type='classification')
# best_model = automl.search_best_model(X_train, X_val, y_train, y_val)
# ensemble_model = automl.ensemble_models(X_train, y_train)
```

**4. Model Evaluation Framework**:
```python
from sklearn.model_selection import cross_val_score, learning_curve
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

class ModelEvaluator:
    def __init__(self):
        self.results = {}
    
    def comprehensive_evaluation(self, model, X_test, y_test, class_names=None):
        """Complete model evaluation"""
        
        # Make predictions
        y_pred = model.predict(X_test)
        y_pred_proba = None
        
        if hasattr(model, 'predict_proba'):
            y_pred_proba = model.predict_proba(X_test)
        
        # Calculate metrics
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision_macro': precision_score(y_test, y_pred, average='macro'),
            'recall_macro': recall_score(y_test, y_pred, average='macro'),
            'f1_macro': f1_score(y_test, y_pred, average='macro')
        }
        
        # ROC AUC for binary classification
        if len(np.unique(y_test)) == 2 and y_pred_proba is not None:
            metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba[:, 1])
        
        # Classification report
        report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        
        self.results = {
            'metrics': metrics,
            'classification_report': report,
            'confusion_matrix': cm.tolist(),
            'predictions': y_pred.tolist(),
            'probabilities': y_pred_proba.tolist() if y_pred_proba is not None else None
        }
        
        return self.results
    
    def cross_validate_model(self, model, X, y, cv=5):
        """Cross-validation with multiple metrics"""
        
        cv_results = {}
        scoring_metrics = ['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted']
        
        for metric in scoring_metrics:
            scores = cross_val_score(model, X, y, cv=cv, scoring=metric)
            cv_results[metric] = {
                'mean': scores.mean(),
                'std': scores.std(),
                'scores': scores.tolist()
            }
        
        return cv_results
    
    def learning_curve_analysis(self, model, X, y, cv=5):
        """Learning curve analysis"""
        
        train_sizes, train_scores, val_scores = learning_curve(
            model, X, y, cv=cv, n_jobs=-1
        )
        
        return {
            'train_sizes': train_sizes.tolist(),
            'train_scores_mean': train_scores.mean(axis=1).tolist(),
            'val_scores_mean': val_scores.mean(axis=1).tolist()
        }

# Usage
# evaluator = ModelEvaluator()
# results = evaluator.comprehensive_evaluation(model, X_test, y_test)
# cv_results = evaluator.cross_validate_model(model, X, y)
# learning_data = evaluator.learning_curve_analysis(model, X, y)
```

### Level 3: Advanced Implementation (120 lines)

#### Production ML Systems

**1. Model Serving with FastAPI**:
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import mlflow.pyfunc
import pandas as pd
import time
import asyncio
from typing import List, Optional

class PredictionRequest(BaseModel):
    features: List[dict]
    model_version: Optional[str] = "latest"
    return_probabilities: Optional[bool] = False

class ModelServer:
    def __init__(self, model_uri: str):
        self.app = FastAPI(title="ML Model Server")
        self.model_uri = model_uri
        self.model = None
        self._load_model()
        self._setup_routes()
    
    def _load_model(self):
        """Load model from MLflow"""
        try:
            self.model = mlflow.pyfunc.load_model(self.model_uri)
            print(f"Model loaded from {self.model_uri}")
        except Exception as e:
            print(f"Failed to load model: {e}")
            raise
    
    def _setup_routes(self):
        """Setup API endpoints"""
        
        @self.app.get("/health")
        async def health_check():
            return {"status": "healthy", "model_uri": self.model_uri}
        
        @self.app.post("/predict")
        async def predict(request: PredictionRequest):
            """Make predictions"""
            if not self.model:
                raise HTTPException(status_code=503, detail="Model not loaded")
            
            try:
                # Convert to DataFrame
                df = pd.DataFrame(request.features)
                
                # Make prediction
                if request.return_probabilities and hasattr(self.model, 'predict_proba'):
                    predictions = self.model.predict(df)
                    probabilities = self.model.predict_proba(df).tolist()
                    return {
                        "predictions": predictions.tolist(),
                        "probabilities": probabilities,
                        "model_version": request.model_version
                    }
                else:
                    predictions = self.model.predict(df)
                    return {
                        "predictions": predictions.tolist(),
                        "model_version": request.model_version
                    }
            
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

# Usage
# server = ModelServer("runs:/12345abcdef/model")
# app = server.app
```

**2. Model Monitoring and Drift Detection**:
```python
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.metrics import accuracy_score
from datetime import datetime

class ModelMonitor:
    def __init__(self, performance_threshold: float = 0.05):
        self.performance_threshold = performance_threshold
        self.reference_data = None
        self.performance_history = []
        self.drift_alerts = []
    
    def set_reference_data(self, X_ref: pd.DataFrame, y_ref: pd.Series = None):
        """Set reference data for drift detection"""
        self.reference_data = {
            'X': X_ref,
            'y': y_ref,
            'feature_stats': self._calculate_feature_statistics(X_ref)
        }
        
        if y_ref is not None:
            self.reference_data['target_distribution'] = y_ref.value_counts(normalize=True).to_dict()
    
    def _calculate_feature_statistics(self, X: pd.DataFrame) -> dict:
        """Calculate feature statistics"""
        stats_dict = {}
        
        for column in X.columns:
            if X[column].dtype in ['int64', 'float64']:
                stats_dict[column] = {
                    'mean': X[column].mean(),
                    'std': X[column].std(),
                    'min': X[column].min(),
                    'max': X[column].max()
                }
            else:
                stats_dict[column] = {
                    'value_counts': X[column].value_counts(normalize=True).to_dict()
                }
        
        return stats_dict
    
    def detect_data_drift(self, X_current: pd.DataFrame) -> dict:
        """Detect data drift using KS test"""
        
        if self.reference_data is None:
            raise ValueError("Reference data not set")
        
        X_ref = self.reference_data['X']
        drift_results = {}
        
        for column in X_current.columns:
            if column not in X_ref.columns:
                continue
            
            if X_current[column].dtype in ['int64', 'float64']:
                # KS test for numerical features
                statistic, p_value = stats.ks_2samp(
                    X_current[column].dropna(), 
                    X_ref[column].dropna()
                )
                
                drift_results[column] = {
                    'drift_detected': p_value < 0.05,
                    'ks_statistic': statistic,
                    'p_value': p_value,
                    'ref_mean': self.reference_data['feature_stats'][column]['mean'],
                    'current_mean': X_current[column].mean()
                }
        
        # Overall drift score
        drifted_features = [col for col, result in drift_results.items() 
                           if result.get('drift_detected', False)]
        overall_drift_score = len(drifted_features) / len(drift_results)
        
        return {
            'feature_drift': drift_results,
            'overall_drift_score': overall_drift_score,
            'drifted_features': drifted_features,
            'timestamp': datetime.now().isoformat()
        }
    
    def monitor_model_performance(self, y_true: np.ndarray, y_pred: np.ndarray, 
                                 model_name: str = "model"):
        """Monitor model performance degradation"""
        
        accuracy = accuracy_score(y_true, y_pred)
        
        performance_entry = {
            'timestamp': datetime.now().isoformat(),
            'model_name': model_name,
            'accuracy': accuracy
        }
        
        self.performance_history.append(performance_entry)
        
        # Check for performance degradation
        if len(self.performance_history) > 1:
            prev_accuracy = self.performance_history[-2]['accuracy']
            accuracy_change = abs(accuracy - prev_accuracy)
            
            if accuracy_change > self.performance_threshold:
                alert = {
                    'timestamp': datetime.now().isoformat(),
                    'alert_type': 'performance_degradation',
                    'model_name': model_name,
                    'previous_accuracy': prev_accuracy,
                    'current_accuracy': accuracy,
                    'change': accuracy_change
                }
                
                self.drift_alerts.append(alert)
        
        return performance_entry
    
    def generate_monitoring_report(self) -> str:
        """Generate monitoring report"""
        
        report = "# Model Monitoring Report\n\n"
        report += f"**Generated**: {datetime.now().isoformat()}\n\n"
        
        if self.performance_history:
            report += "## Recent Performance\n\n"
            for entry in self.performance_history[-5:]:
                report += f"- {entry['timestamp']}: {entry['accuracy']:.4f}\n"
            report += "\n"
        
        if self.drift_alerts:
            report += "## Recent Alerts\n\n"
            for alert in self.drift_alerts[-3:]:
                report += f"- {alert['timestamp']}: {alert['alert_type']}\n"
        
        return report

# Usage
# monitor = ModelMonitor(performance_threshold=0.05)
# monitor.set_reference_data(X_train, y_train)
# drift_results = monitor.detect_data_drift(X_current)
# performance = monitor.monitor_model_performance(y_true, y_pred)
```

**3. Advanced Deep Learning with TensorFlow 2.20.0**:
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

class AdvancedNeuralNetwork:
    def __init__(self, input_shape, num_classes):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None
    
    def build_resnet_model(self, num_filters=64, num_blocks=3):
        """Build ResNet-style model"""
        
        inputs = keras.Input(shape=self.input_shape)
        x = layers.Conv2D(num_filters, 3, padding='same')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.ReLU()(x)
        
        # Residual blocks
        for _ in range(num_blocks):
            x = self._residual_block(x, num_filters)
        
        # Classification head
        x = layers.GlobalAveragePooling2D()(x)
        outputs = layers.Dense(self.num_classes, activation='softmax')(x)
        
        self.model = keras.Model(inputs, outputs)
        return self.model
    
    def _residual_block(self, x, filters):
        """Residual block implementation"""
        
        # First convolution
        y = layers.Conv2D(filters, 3, padding='same')(x)
        y = layers.BatchNormalization()(y)
        y = layers.ReLU()(y)
        
        # Second convolution
        y = layers.Conv2D(filters, 3, padding='same')(y)
        y = layers.BatchNormalization()(y)
        
        # Skip connection
        if x.shape[-1] != filters:
            x = layers.Conv2D(filters, 1, padding='same')(x)
        
        y = layers.Add()([x, y])
        y = layers.ReLU()(y)
        
        return y
    
    def compile_with_modern_optimizers(self, learning_rate=0.001):
        """Compile with latest optimizers"""
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
    
    def add_advanced_callbacks(self, model_path='best_model'):
        """Add comprehensive callbacks"""
        
        return [
            keras.callbacks.EarlyStopping(
                patience=10, 
                restore_best_weights=True,
                monitor='val_accuracy'
            ),
            keras.callbacks.ReduceLROnPlateau(
                factor=0.5, 
                patience=5,
                monitor='val_loss'
            ),
            keras.callbacks.ModelCheckpoint(
                model_path, 
                save_best_only=True,
                monitor='val_accuracy'
            ),
            keras.callbacks.TensorBoard(
                log_dir='./logs',
                histogram_freq=1
            )
        ]
    
    def train_with_mixup(self, X_train, y_train, X_val, y_val, 
                       epochs=100, batch_size=32, alpha=0.2):
        """Training with Mixup augmentation"""
        
        def mixup_data(x, y, alpha=0.2):
            """Mixup data augmentation"""
            if alpha > 0:
                lam = np.random.beta(alpha, alpha)
            else:
                lam = 1
            
            batch_size = x.shape[0]
            index = np.random.permutation(batch_size)
            
            mixed_x = lam * x + (1 - lam) * x[index]
            mixed_y = lam * y + (1 - lam) * y[index]
            
            return mixed_x, mixed_y
        
        # Custom training loop with mixup
        history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
        
        for epoch in range(epochs):
            # Training with mixup
            train_loss, train_acc = 0, 0
            num_batches = 0
            
            for i in range(0, len(X_train), batch_size):
                x_batch = X_train[i:i+batch_size]
                y_batch = y_train[i:i+batch_size]
                
                # Apply mixup
                x_mixed, y_mixed = mixup_data(x_batch, y_batch, alpha)
                
                # Train step
                with tf.GradientTape() as tape:
                    predictions = self.model(x_mixed, training=True)
                    loss = tf.keras.losses.categorical_crossentropy(y_mixed, predictions)
                
                gradients = tape.gradient(loss, self.model.trainable_variables)
                self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
                
                train_loss += loss.numpy()
                train_acc += tf.keras.metrics.categorical_accuracy(
                    y_batch, self.model(x_batch)
                ).numpy()
                num_batches += 1
            
            # Validation
            val_loss, val_acc = self.model.evaluate(X_val, y_val, verbose=0)
            
            # Record metrics
            history['loss'].append(train_loss / num_batches)
            history['accuracy'].append(train_acc / num_batches)
            history['val_loss'].append(val_loss)
            history['val_accuracy'].append(val_acc)
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}: loss={train_loss/num_batches:.4f}, "
                      f"acc={train_acc/num_batches:.4f}, "
                      f"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}")
        
        return history

# Usage
# cnn = AdvancedNeuralNetwork(input_shape=(32, 32, 3), num_classes=10)
# model = cnn.build_resnet_model()
# cnn.compile_with_modern_optimizers()
# callbacks = cnn.add_advanced_callbacks()
# history = cnn.train_with_mixup(X_train, y_train, X_val, y_val)
```

### Level 4: Reference (80 lines)

#### ML Technology Stack Reference

**Deep Learning Frameworks**:

| Framework | Version | Key Features | Best Use Cases |
|-----------|---------|--------------|----------------|
| **TensorFlow** | 2.20.0 | Keras API, AutoGraph, TPU support | Production deployment, research |
| **PyTorch** | 2.9.0 | Dynamic graphs, TorchScript | Research prototyping, NLP |
| **JAX** | 0.4.33 | Functional programming, TPU | Research, large-scale training |

**Classical ML & AutoML**:

| Library | Version | Key Features | Use Cases |
|---------|---------|--------------|-----------|
| **Scikit-learn** | 1.7.2 | Comprehensive algorithms | Tabular data, classification |
| **XGBoost** | 2.0.3 | Gradient boosting | Competitions, tabular data |
| **LightGBM** | 4.4.0 | Fast training | Large datasets |
| **H2O AutoML** | 3.44.0 | Automated ML | Rapid prototyping |
| **AutoGluon** | 1.0.0 | Tabular AutoML | Kaggle competitions |

#### ML Pipeline Patterns

**Standard ML Workflow**:
```python
# 1. Data Processing
processor = MLDataProcessor()
X_train_processed = processor.fit_transform(X_train)
X_test_processed = processor.transform(X_test)

# 2. Model Selection with AutoML
automl = AutoMLPipeline(task_type='classification')
best_model = automl.search_best_model(X_train_processed, X_val_processed, y_train, y_val)

# 3. Experiment Tracking
tracker = ExperimentTracker("experiment_name")
run_id = tracker.log_experiment(best_model, X_train_processed, X_val_processed, y_train, y_val)

# 4. Model Evaluation
evaluator = ModelEvaluator()
results = evaluator.comprehensive_evaluation(best_model, X_test_processed, y_test)

# 5. Model Deployment
server = ModelServer(f"runs:/{run_id}/model")
```

**Deep Learning Workflow**:
```python
# 1. Model Architecture
model = build_advanced_model(input_shape, num_classes)

# 2. Training with Modern Callbacks
callbacks = [
    keras.callbacks.EarlyStopping(patience=10),
    keras.callbacks.ReduceLROnPlateau(factor=0.5),
    keras.callbacks.ModelCheckpoint('best_model.h5')
]

# 3. Training
history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                    epochs=100, batch_size=32, callbacks=callbacks)

# 4. Evaluation and Monitoring
results = evaluate_model(model, X_test, y_test)
monitor = ModelMonitor()
monitor.set_reference_data(X_train, y_train)
```

#### MLOps Best Practices

**Experiment Tracking**:
- âœ… Log all parameters, metrics, and artifacts
- âœ… Use consistent naming conventions for experiments
- âœ… Track data versions and preprocessing steps
- âœ… Document experiment hypotheses and results

**Model Deployment**:
- âœ… Containerize models with Docker
- âœ… Use FastAPI for scalable REST APIs
- âœ… Implement health checks and monitoring
- âœ… Set up model versioning and A/B testing

**Model Monitoring**:
- âœ… Track prediction accuracy and performance
- âœ… Monitor data drift and concept drift
- âœ… Set up automated retraining pipelines
- âœ… Implement alerting for model degradation

#### Performance Optimization

**Training Optimization**:
```python
# GPU memory management
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Mixed precision training
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Distributed training
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = create_model()
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
```

**Inference Optimization**:
```python
# Model quantization for mobile deployment
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_model = converter.convert()

# Batch prediction for efficiency
def batch_predict(model, data, batch_size=32):
    predictions = []
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        batch_pred = model.predict(batch)
        predictions.extend(batch_pred)
    return predictions
```

#### Common ML Patterns

**Hyperparameter Optimization**:
```python
# Random search vs Grid search
search_space = {
    'learning_rate': [0.001, 0.01, 0.1],
    'batch_size': [16, 32, 64],
    'num_layers': [2, 3, 4],
    'units': [64, 128, 256]
}
```

**Cross-Validation Strategies**:
```python
# Time series cross-validation
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

# Stratified K-fold for imbalanced data
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

---

## ðŸŽ¯ Best Practices Checklist

**Must-Have:**
- âœ… Use MLflow for experiment tracking and reproducibility
- âœ… Implement proper train/validation/test splits
- âœ… Add comprehensive model evaluation metrics
- âœ… Use cross-validation for robust model assessment
- âœ… Implement data preprocessing pipelines

**Recommended:**
- âœ… Use AutoML for automated hyperparameter optimization
- âœ… Implement model monitoring and drift detection
- âœ… Set up scalable model serving with FastAPI
- âœ… Use ensemble methods for improved performance
- âœ… Implement proper model versioning

**Performance:**
- âš¡ Use GPU acceleration for deep learning
- âš¡ Implement batch processing for large datasets
- âš¡ Use quantization for mobile deployment
- âš¡ Optimize data loading and preprocessing
- âš¡ Use distributed training for large models

---

## ðŸ”„ Integration with Other Skills

**Complementary Skills:**
- `Skill("moai-domain-data-science")` â€“ Data analysis and statistical methods
- `Skill("moai-domain-testing")` â€“ Model testing and validation frameworks
- `Skill("moai-domain-devops")` â€“ CI/CD pipelines and infrastructure
- `Skill("moai-essentials-refactor")` â€“ Code optimization and performance

**Related Skills:**
- `Skill("moai-domain-backend")` â€“ Production API development
- `Skill("moai-domain-database")` â€“ Data storage and management
- `Skill("moai-context7-integration")` â€“ Real-time documentation and best practices

---

## ðŸ“š Official References

**Deep Learning Documentation**:
- [TensorFlow 2.20.0 API](https://www.tensorflow.org/api_docs)
- [Keras Guide](https://keras.io/guides/)
- [PyTorch 2.9.0 Docs](https://pytorch.org/docs/)

**MLOps Frameworks**:
- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

**AutoML Platforms**:
- [H2O AutoML](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)
- [AutoGluon](https://auto.gluon.ai/stable/index.html)
- [Scikit-learn Model Selection](https://scikit-learn.org/stable/model_selection.html)

---

## ðŸ“ˆ Version History

**v4.0.0** (2025-11-13)
- âœ¨ Aggressive optimization (1,114â†’495 lines, 55% reduction)
- âœ¨ Progressive Disclosure 4-layer structure
- âœ¨ Streamlined ML pipeline patterns and AutoML
- âœ¨ Production-ready model serving with FastAPI
- âœ¨ Advanced monitoring and drift detection
- âœ¨ TensorFlow 2.20.0 and PyTorch 2.9.0 integration
- âœ¨ Enterprise MLOps best practices

**v3.0.0** (2025-11-12)
- âœ¨ AutoML pipeline implementation
- âœ¨ MLflow experiment tracking
- âœ¨ Model monitoring and drift detection

---

**Generated with**: MoAI-ADK Skill Factory v4.0  
**Last Updated**: 2025-11-13  
**Status**: Production Ready
