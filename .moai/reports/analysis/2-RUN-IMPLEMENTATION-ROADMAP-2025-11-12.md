# 2-run.md ê°œì„  ì‹¤í–‰ ë¡œë“œë§µ

**ë¬¸ì„œ**: êµ¬ì²´ì  ì‹¤í–‰ ê°€ì´ë“œ
**ê¸°ì¤€**: 2-RUN-MODERNIZATION-GUIDE-2025-11-12.md
**ëŒ€ìƒ íŒŒì¼**: `/Users/goos/MoAI/MoAI-ADK/.claude/commands/alfred/2-run.md`

---

## ì‹¤í–‰ ê³„íš ê°œìš”

### 3ë‹¨ê³„ ì ‘ê·¼ë²•

| ë‹¨ê³„ | ì‘ì—… | ì†Œìš”ì‹œê°„ | ìš°ì„ ìˆœìœ„ |
|------|------|--------|--------|
| **Step 1** | PHASE 1 ê°œì„  (ë³‘ë ¬ Task) | 45ë¶„ | ë†’ìŒ â­â­â­ |
| **Step 2** | PHASE 2 ë¦¬ì„¤ê³„ (3ê°œ ë³‘ë ¬ Task) | 90ë¶„ | ë†’ìŒ â­â­â­ |
| **Step 3** | PHASE 2.3 ê°•í™” (TDD í”„ë¡¬í”„íŠ¸) | 60ë¶„ | ë†’ìŒ â­â­â­ |
| **Step 4** | ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€ (debug-helper) | 30ë¶„ | ì¤‘ê°„ â­â­ |
| **Step 5** | ë¬¸ì„œí™” ë° í…œí”Œë¦¿ ë™ê¸°í™” | 30ë¶„ | ì¤‘ê°„ â­â­ |

**ì´ ì†Œìš”ì‹œê°„**: ~3.5ì‹œê°„

---

## Step 1: PHASE 1 ê°œì„  - ë³‘ë ¬ Task ë„ì…

### 1.1 í˜„ì¬ ì½”ë“œ ë¶„ì„

**íŒŒì¼**: `.claude/commands/alfred/2-run.md`
**ìœ„ì¹˜**: Line 94-119 (PHASE 1 ì „ì²´)

```markdown
## ğŸš€ PHASE 1: Analysis & Planning

**Goal**: Analyze SPEC requirements and create execution plan.

### Step 1.1: Load Skills & Prepare Context
1. TUI System Ready
2. Read SPEC document
3. Update SPEC status to in-progress:
   python3 .claude/hooks/alfred/spec_status_hooks.py ...
4. Optionally invoke Explore agent

### Step 1.2: Invoke Implementation-Planner Agent
   Use Task tool with subagent_type: "implementation-planner"

### Step 1.3: Request User Approval
   Present plan and ask for approval
```

### 1.2 ê°œì„  ë‚´ìš©

**í•µì‹¬ ë³€ê²½**:
1. python3 ìŠ¤í¬ë¦½íŠ¸ í˜¸ì¶œ ì œê±°
2. Step 1.1ê³¼ 1.2ë¥¼ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ í†µí•©
3. Explore + tag-agentë¥¼ ë™ì‹œ í˜¸ì¶œ

### 1.3 êµ¬ì²´ì  ì½”ë“œ ë³€ê²½

**ë³€ê²½ ëŒ€ìƒ ë¼ì¸**: 94-119

**Old Code (ì œê±°)**:

```markdown
### Step 1.1: Load Skills & Prepare Context

1. **TUI System Ready**:
   - Interactive menus are available for all user interactions

2. **Read SPEC document**:
   - Read: `.moai/specs/SPEC-$ARGUMENTS/spec.md`
   - Determine if codebase exploration is needed (existing patterns, similar implementations)

3. **Update SPEC status to in-progress**:
   ```bash
   python3 .claude/hooks/alfred/spec_status_hooks.py status_update SPEC-$ARGUMENTS --status in-progress --reason "Implementation started via /alfred:2-run"
   ```

4. **Optionally invoke Explore agent for codebase analysis**:
   - IF SPEC requires understanding existing code patterns:
     - Use Task tool with `subagent_type: "Explore"`
     - Prompt: "Analyze codebase for SPEC-$ARGUMENTS: Similar implementations, test patterns, architecture, libraries/versions"
     - Thoroughness: "medium"
   - ELSE: Skip and proceed directly to Step 1.3

**Result**: SPEC context gathered. Ready for planning.
```

**New Code (ëŒ€ì²´)**:

```markdown
### Step 1.1: Parallel SPEC Analysis & Status Initialization

**Goal**: Prepare SPEC context and initialize tracking in parallel.

Use Task tool - two independent parallel calls:

**Task 1.1.A - SPEC Requirements Analysis**:
```
Task(subagent_type="Explore",
     description="Extract SPEC requirements and technical context",
     prompt="Analyze SPEC document at .moai/specs/SPEC-$ARGUMENTS/spec.md:

1. Requirements & Acceptance Criteria
   - What are the main requirements?
   - What are the acceptance criteria?
   - Any non-functional requirements (performance, security)?

2. Domain & Complexity
   - What domains are involved? (backend/frontend/devops/database/etc)
   - Assess overall complexity (Low/Medium/High)
   - Estimated effort in hours

3. Technical Context
   - Key dependencies and libraries needed
   - Similar implementations in codebase?
   - Architectural patterns to follow?

4. Constraints & Risks
   - Any time/resource constraints?
   - Potential technical risks?

Output: Structured analysis as JSON:
{
  \"requirements\": [...],
  \"acceptance_criteria\": [...],
  \"domains\": [...],
  \"complexity\": \"Low|Medium|High\",
  \"estimated_hours\": N,
  \"technical_context\": {...},
  \"risks\": [...]
}")
```

**Task 1.1.B - SPEC Status & TAG Initialization**:
```
Task(subagent_type="tag-agent",
     description="Initialize SPEC tracking and update status",
     prompt="For SPEC-$ARGUMENTS:

1. Status Update
   - Update SPEC status from 'draft' to 'in-progress'
   - Record timestamp and reason: 'Implementation started via /alfred:2-run'

2. TAG Initialization
   - Create @SPEC-$ARGUMENTS_IMPL tracking TAG
   - Initialize TAG chains for implementation:
     * @TEST-{N} â†’ @SPEC-$ARGUMENTS (test chain)
     * @CODE-{N} â†’ @TEST-{N} â†’ @SPEC-$ARGUMENTS (code chain)
     * @REFACTOR-{N} â†’ @CODE-{N} (quality improvements)
   - Set initial counter to 001

3. Logging
   - Log status change to .moai/logs/spec-status.log
   - Include: timestamp, SPEC-ID, old_status â†’ new_status, reason

Output: Initialization confirmation with:
- Status update timestamp
- Initialized TAGs
- Log file location")
```

**Wait for both tasks to complete** (parallel execution).

**Store results**:
- $SPEC_ANALYSIS (from Task 1.1.A)
- $TAG_INIT_STATUS (from Task 1.1.B)

**Result**: SPEC context gathered, status updated, TAG tracking initialized.
```

### 1.4 ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

ìƒì„±ëœ ê°œì„ ëœ Step 1.1ì´ ë‹¤ìŒì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸:

- [ ] `python3` í˜¸ì¶œ ì œê±°ë¨ (ìŠ¤í¬ë¦½íŠ¸ ì—†ìŒ)
- [ ] 2ê°œ ë³‘ë ¬ Task í˜¸ì¶œ ëª…í™•í•¨
- [ ] `Task()` ë„êµ¬ ë¬¸ë²• ì •í™•í•¨
- [ ] `subagent_type` ëª…ì‹œ (Explore, tag-agent)
- [ ] promptê°€ êµ¬ì¡°í™”ëœ ë¶„ì„ ìš”ì²­
- [ ] ê²°ê³¼ ì €ì¥ ëª…ì‹œ ($SPEC_ANALYSIS, $TAG_INIT_STATUS)
- [ ] ë¼ì¸ ìˆ˜: ~25-30ì¤„ (ì´ì „ ~22ì¤„ì—ì„œ ì•½ê°„ ì¦ê°€)

---

## Step 2: PHASE 2 ë¦¬ì„¤ê³„ - 3ê°œ ë³‘ë ¬ Task

### 2.1 í˜„ì¬ ì½”ë“œ ë¶„ì„

**íŒŒì¼**: `.claude/commands/alfred/2-run.md`
**ìœ„ì¹˜**: Line 217-324 (PHASE 2 ì „ì²´)

```markdown
## ğŸ”§ PHASE 2: Execute Task (TDD Implementation)

Step 2.1: Initialize Progress Tracking
Step 2.2: Check Domain Readiness (Optional)
Step 2.3: Invoke TDD-Implementer Agent
Step 2.4: Invoke Quality-Gate Agent
```

### 2.2 ê°œì„  ëª©í‘œ

**3ê°€ì§€ ë³‘ë ¬í™”**:
1. Execution milestones ì¶”ì¶œ (impl-planner)
2. Domain readiness ê²€ì¦ (Explore)
3. Resource optimization ê³„íš (impl-planner)

**4ê°€ì§€ ê°œì„ **:
1. Step 2.2ë¥¼ ì˜ë¬´í™” (optional â†’ mandatory)
2. ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì‹œê°„ ë‹¨ì¶•
3. TodoWrite ìë™ ì´ˆê¸°í™”
4. tdd-implementerì— í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ

### 2.3 êµ¬ì²´ì  ì½”ë“œ ë³€ê²½

**ë³€ê²½ ëŒ€ìƒ ë¼ì¸**: 217-286 (Step 2.1-2.3)

**Old Code (ì œê±°)**:

```markdown
### Step 2.1: Initialize Progress Tracking

Use TodoWrite to track all tasks:
1. Parse tasks from execution plan
2. Initialize TodoWrite

### Step 2.2: Check Domain Readiness (Optional)

For multi-domain SPECs:
1. Read SPEC metadata
2. For each domain, invoke Explore agent
3. Store feedback

### Step 2.3: Invoke TDD-Implementer Agent

Use Task tool:
- subagent_type: "tdd-implementer"
- prompt: [current prompt]
```

**New Code (ëŒ€ì²´)**:

```markdown
### Step 2.1: Parallel Resource Preparation

**Goal**: Prepare execution resources in parallel for faster planning.

Use Task tool - three independent parallel calls:

**Task 2.1.A - Execution Milestones Extraction**:
```
Task(subagent_type="implementation-planner",
     description="Extract and structure execution milestones for progress tracking",
     prompt="From execution plan of SPEC-$ARGUMENTS:

1. Break down into concrete, measurable tasks
   - Each task should be completable in 1-2 hours
   - Maximum 15 tasks (if more, group related tasks)
   - Clear, specific task names

2. Task structure
   - Task ID: T001, T002, ... (sequential)
   - Name: Clear, actionable description
   - Estimated hours: 0.5, 1, 1.5, 2 hours
   - Dependencies: Which other tasks must complete first?
   - Type: test, implementation, refactor, documentation

3. Risk assessment
   - Which tasks are high-risk? (1-3 tasks)
   - Why are they risky?
   - Mitigation strategy for each

Output: Structured JSON array:
[
  {
    \"id\": \"T001\",
    \"name\": \"Write authentication unit tests\",
    \"type\": \"test\",
    \"est_hours\": 1.5,
    \"depends_on\": [],
    \"risk_level\": \"low\"
  },
  ...
]

This list will be used to initialize TodoWrite for progress tracking.")
```

**Task 2.1.B - Domain Readiness Assessment**:
```
Task(subagent_type="Explore",
     description="Assess domain-specific readiness for SPEC-$ARGUMENTS",
     prompt="For SPEC-$ARGUMENTS, analyze domain readiness:

Domains involved: [extract from SPEC metadata, e.g., backend, frontend, devops]

For each domain:
1. Existing implementations
   - Similar features in current codebase?
   - Which files/modules implement similar functionality?
   - Copy-paste opportunities?

2. Library & Framework Analysis
   - Current libraries used in this domain
   - Their versions
   - Compatibility with Python/Node.js version?
   - Any deprecated dependencies to replace?

3. Testing Patterns
   - Common test patterns for this domain
   - Mocking/fixture strategies used
   - Test structure conventions (pytest vs unittest)

4. Architecture Patterns
   - Naming conventions for this domain
   - Folder structure
   - Import patterns
   - Error handling conventions

5. Potential Challenges
   - Known complexity areas in this domain?
   - Integration challenges with other domains?
   - Performance considerations?

6. Recommendations
   - Suggested testing approach
   - Recommended libraries/frameworks
   - Code patterns to follow

Output: Domain-specific guidance as structured analysis")
```

**Task 2.1.C - Resource Optimization Planning**:
```
Task(subagent_type="implementation-planner",
     description="Optimize TDD approach based on SPEC complexity and resources",
     prompt="Based on SPEC-$ARGUMENTS complexity and resources:

1. Test Framework Selection
   - Recommended test framework (pytest, unittest, jest, vitest, etc)
   - Why this choice? (speed, coverage reporting, CI integration)
   - Configuration recommendations

2. Test-to-Code Ratio
   - Recommended test-to-code ratio for this complexity level
   - Examples: 1:1 (one test file per code file), 1:2, etc.
   - Coverage target (default: â‰¥85%)

3. Quick Wins vs Risky Areas
   - High-value, low-effort tasks (quick wins) - prioritize these
   - High-risk, high-complexity tasks - plan mitigation
   - Normal tasks - standard approach

4. Commit Strategy
   - Atomic commits per RED/GREEN/REFACTOR phase
   - Suggested commit message patterns
   - Branch strategy (feature branch, develop merge)

5. Quality Gates
   - Minimum coverage: â‰¥85%
   - Code style checks: [eslint, ruff, black, etc]
   - Type checking: [mypy, tsc, etc]
   - Security scanning: [bandit, safety, etc]

Output: Resource optimization plan as structured JSON:
{
  \"test_framework\": \"pytest\",
  \"framework_rationale\": \"...\",
  \"test_to_code_ratio\": \"1:1\",
  \"coverage_target\": 0.85,
  \"quick_wins\": [...],
  \"risky_areas\": [...],
  \"quality_gates\": {
    \"min_coverage\": 0.85,
    \"style_checker\": \"ruff\",
    \"type_checker\": \"mypy\"
  }
}")
```

**Wait for all 3 tasks** (parallel execution).

**Store results**:
- $TASK_LIST (from Task 2.1.A)
- $DOMAIN_GUIDANCE (from Task 2.1.B)
- $RESOURCE_PLAN (from Task 2.1.C)

**Result**: Execution resources prepared, domain guidance gathered, optimization plan ready.

---

### Step 2.2: Initialize Progress Tracking from Structured Tasks

**Goal**: Automatically initialize TodoWrite from $TASK_LIST.

Use result from Task 2.1.A:

1. **Extract task list** from $TASK_LIST (JSON array)
2. **Initialize TodoWrite** with each task:
   ```
   TodoWrite(action="initialize", items=[
     { task: "T001: Write authentication unit tests", status: "pending", hours: 1.5 },
     { task: "T002: Implement login endpoint", status: "pending", hours: 1 },
     ...
   ])
   ```
3. **Set burndown tracking**:
   - Total hours: Sum of est_hours from all tasks
   - Track progress: Update TodoWrite as each task completes

**Result**: Progress tracking initialized, burndown calculation ready.

---

### Step 2.3: Invoke TDD-Implementer Agent (Enhanced)

**Goal**: Execute TDD cycle with structured guidance from PHASE 2.1.

Use Task tool:
- subagent_type: "tdd-implementer"
- description: "Execute TDD implementation cycle with structured guidance"
- prompt: (see detailed prompt below)
```

### 2.4 TDD-Implementer Enhanced Prompt

**ë§¤ìš° ì¤‘ìš”**: ì´ê²ƒì´ ê°€ì¥ í° ê°œì„  ì§€ì 

**í˜„ì¬ prompt í¬ê¸°**: ~30ì¤„
**ê°œì„ ëœ prompt í¬ê¸°**: ~100ì¤„ (3ë°° ì´ìƒ)

```markdown
### Step 2.3: Invoke TDD-Implementer Agent (ENHANCED)

Use Task tool:
- subagent_type: "tdd-implementer"
- description: "Execute TDD RED-GREEN-REFACTOR cycle with structured guidance"
- prompt: """
You are the TDD implementer agent. Execute strict RED-GREEN-REFACTOR cycle for SPEC-$ARGUMENTS.

CRITICAL CONFIGURATION:
Language settings from .moai/config.json:
- agent_prompt_language: English (instructions language)
- conversation_language: User's preferred language
- Code: Always English
- Comments: Per project language rules

EXECUTION CONTEXT:
- SPEC ID: $ARGUMENTS
- Execution Plan: [from implementation-planner analysis in PHASE 1]
- Execution Tasks: $TASK_LIST (use this as your task decomposition)
- Domain Guidance: $DOMAIN_GUIDANCE (follow domain patterns)
- Resource Plan: $RESOURCE_PLAN (use test framework, commit strategy)

PHASE 1: RED (Write Failing Tests)
========================================

For each task in $TASK_LIST where type="test":

1. Create Test File
   - Location: tests/test_{task_name}.py
   - Add @TEST-{COUNTER} TAG in header comment
   - Link to @SPEC-$ARGUMENTS in docstring

2. Write Test Cases
   - Happy path (main scenario from acceptance criteria)
   - Edge cases (boundary conditions, special inputs)
   - Error scenarios (invalid inputs, exceptions)
   - Integration points (if applicable)

3. Run Tests
   - Execute: pytest tests/test_{task_name}.py -v
   - Verify all tests fail (they should, code doesn't exist yet)
   - Log failure reasons to confirm tests are correct

4. Update Progress
   - Update TodoWrite: T001 status â†’ "test-written"
   - Document test count for this task

5. Commit RED Phase
   - Message: "test(@SPEC-$ARGUMENTS): Add failing tests for {task_name}"
   - Include: @TEST-{COUNTER}, @SPEC-$ARGUMENTS tags
   - Example:
     ```
     test(@SPEC-AUTH-001): Add failing tests for login endpoint

     Tests:
     - test_successful_login_with_valid_credentials
     - test_failed_login_with_invalid_password
     - test_failed_login_with_nonexistent_user
     - test_rate_limiting_after_5_attempts

     Related: @TEST-001 â†’ @SPEC-AUTH-001
     ```

PHASE 2: GREEN (Minimal Implementation)
========================================

For each task in $TASK_LIST where type="implementation":

1. Create Implementation Files
   - Location: src/{module_name}/{component}.py
   - Add @CODE-{COUNTER} TAG in header comment
   - Link chain: @CODE-{COUNTER} â†’ @TEST-{COUNTER} â†’ @SPEC-$ARGUMENTS

2. Write Minimal Code
   - Implement ONLY what's needed to pass tests
   - No optimization, no extra features
   - Focus on correctness, not elegance
   - Follow coding standards from $DOMAIN_GUIDANCE

3. Run Tests
   - Execute: pytest tests/test_{task_name}.py -v
   - Verify all tests pass
   - Check coverage: pytest --cov=src/{module_name} --cov-report=term-missing
   - Record coverage percentage

4. Update Progress
   - Update TodoWrite: T{N} status â†’ "implemented"
   - Record test pass rate, coverage %

5. Commit GREEN Phase
   - Message: "feat(@SPEC-$ARGUMENTS): Implement {component}"
   - Include: coverage %, test count
   - Example:
     ```
     feat(@SPEC-AUTH-001): Implement authentication service

     - All 8 tests passing
     - Coverage: 92%
     - Implements: login, verify_password, generate_token
     - Related: @CODE-001 â†’ @TEST-001 â†’ @SPEC-AUTH-001
     ```

PHASE 3: REFACTOR (Code Quality)
========================================

After all GREEN commits:

1. Review Code
   - Read all implementation files
   - Identify: duplication, unclear names, complex logic
   - Check against patterns from $DOMAIN_GUIDANCE

2. Improve Code Quality
   - Remove duplication (DRY principle)
   - Improve variable/function names (clarity)
   - Simplify complex logic (readability)
   - Apply design patterns from domain guidance
   - Optimize hot paths (performance)

3. Run Full Test Suite
   - Execute: pytest tests/
   - Ensure all tests still pass
   - Verify coverage hasn't decreased

4. Update Progress
   - Update TodoWrite: All tasks â†’ "completed"
   - Document improvements made

5. Commit REFACTOR Phase
   - Message: "refactor(@SPEC-$ARGUMENTS): Improve code quality"
   - Include: improvements summary
   - Example:
     ```
     refactor(@SPEC-AUTH-001): Improve code quality and readability

     - Extracted: password_validation utility function
     - Simplified: token_generation logic (30 LOC â†’ 15 LOC)
     - Applied: Factory pattern for service creation
     - All 8 tests still passing, coverage: 93%
     - Related: @CODE-001, @SPEC-AUTH-001
     ```

QUALITY ASSURANCE (Automatic)
========================================

During implementation:

1. Test Coverage
   - Target: â‰¥85% (from $RESOURCE_PLAN)
   - After GREEN: measure coverage
   - If below target: Add supplementary tests in REFACTOR

2. Code Style
   - Use linter from $RESOURCE_PLAN (ruff, eslint, etc)
   - Apply formatter (black, prettier, etc)
   - Fix style issues before commits

3. Type Checking (if applicable)
   - Run mypy, tsc, or equivalent
   - Fix type errors before commits

4. Security
   - No hardcoded credentials/secrets
   - Validate user input
   - Sanitize database queries
   - Check for OWASP Top 10 issues

PROGRESS REPORTING
========================================

After each phase (RED, GREEN, REFACTOR):

Report:
- Phase name (RED/GREEN/REFACTOR)
- Tasks completed in this phase
- Key metrics (test count, coverage %, LOC)
- Any blockers or issues encountered
- Time estimate vs actual (if tracked)

Example:
```
=== RED Phase Report ===
Tasks: T001, T002, T003 (all test-written)
Tests written: 24 total, 8 per task
Status: All tests failing as expected âœ“

=== GREEN Phase Report ===
Tasks: T001, T002, T003 (all implemented)
Tests passing: 24/24 (100%)
Coverage: 89% (src/auth/)
Status: Ready for REFACTOR âœ“

=== REFACTOR Phase Report ===
Tasks: T001, T002, T003 (all completed)
Improvements: 3 utilities extracted, 1 pattern applied
Coverage: 91% (final)
Status: Ready for quality-gate verification âœ“
```

ERROR HANDLING
========================================

If you encounter errors:

1. Test Failures (during GREEN)
   - Don't ignore failing tests
   - Debug and fix implementation, not tests
   - Re-run until all pass
   - Document fix approach

2. Coverage Below 85%
   - Identify uncovered code paths
   - Write tests for missing coverage
   - Re-measure and verify â‰¥85%

3. Complex Logic
   - If a task is too complex to implement cleanly
   - Break into smaller subtasks
   - Ask: Should this be split into 2-3 tasks?
   - Raise as blocker for manual review

4. Architectural Decisions
   - If implementation reveals design issue
   - Document the issue clearly
   - Suggest 2-3 solution approaches
   - Raise for manual review by implementation-planner

Raise blockers as:
```
BLOCKER: {Blocker Type}
Issue: {Detailed description}
Impact: {What's blocked?}
Suggested Solutions: {2-3 options}
Escalation: Requires manual review
```

SKILLS REFERENCE
========================================

Use these Skills as needed:
- Skill("moai-alfred-language-detection") - For language-specific patterns
- Skill("moai-essentials-debug") - When debugging test/code failures
- Skill("moai-foundation-tags") - For TAG chain documentation
- Skill("moai-alfred-trust-validation") - To verify TRUST 5 principles

TAG SYSTEM REFERENCE
========================================

Use these TAG patterns throughout:

@SPEC-$ARGUMENTS
- Root specification, appears in all TAGs

@TEST-{COUNTER}
- Format: @TEST-001, @TEST-002, ...
- Location: test file headers and docstrings
- Links: @SPEC-$ARGUMENTS

@CODE-{COUNTER}
- Format: @CODE-001, @CODE-002, ...
- Location: implementation file headers
- Links: @TEST-{COUNTER} â†’ @SPEC-$ARGUMENTS

@REFACTOR-{COUNTER}
- Format: @REFACTOR-001, @REFACTOR-002, ...
- Optional: Used to tag refactoring improvements
- Links: @CODE-{COUNTER}

COUNTER INCREMENT RULE:
- Start at 001 for RED phase
- Increment for each new test file
- Carry forward: @TEST-001 â†’ @CODE-001 (same number)
- Refactor uses same numbers as CODE

FINAL OUTPUT
========================================

After completing all three phases (RED â†’ GREEN â†’ REFACTOR):

Provide summary:
- Total TAGs created (@TEST, @CODE, @REFACTOR counts)
- Final test coverage %
- Final code statistics (LOC, files, modules)
- Commits created during cycle
- Status: READY FOR QUALITY-GATE or BLOCKERS FOUND
"""

Store: $IMPLEMENTATION_RESULTS
```

### 2.5 ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

ê°œì„ ëœ PHASE 2ê°€ ë‹¤ìŒì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸:

- [ ] Step 2.1ì—ì„œ 3ê°œ Task ë³‘ë ¬ í˜¸ì¶œ ëª…í™•í•¨
- [ ] Task 2.1.A: execution milestones (impl-planner)
- [ ] Task 2.1.B: domain readiness (Explore)
- [ ] Task 2.1.C: resource optimization (impl-planner)
- [ ] Step 2.2ì—ì„œ TodoWrite ìë™ ì´ˆê¸°í™”
- [ ] Step 2.3 promptê°€ 100ì¤„ ì´ìƒì˜ ìƒì„¸ ê°€ì´ë“œ
- [ ] RED/GREEN/REFACTOR phases ëª…í™•í•˜ê²Œ êµ¬ë¶„
- [ ] TAG ì²´ì¸ (@TEST â†’ @CODE â†’ @REFACTOR) ëª…ì‹œ
- [ ] Progress reporting ì„¹ì…˜ í¬í•¨
- [ ] Error handling ì„¹ì…˜ í¬í•¨
- [ ] Skills ì°¸ê³  ì„¹ì…˜ í¬í•¨

---

## Step 3: PHASE 2.4 í›„ ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€

### 3.1 í˜„ì¬ ì½”ë“œ ë¶„ì„

**íŒŒì¼**: `.claude/commands/alfred/2-run.md`
**ìœ„ì¹˜**: Line 319-323 (Quality-Gate ì²˜ë¦¬)

```markdown
### Step 2.4: Invoke Quality-Gate Agent

Handle result:
- IF PASS â†’ Proceed to PHASE 3
- IF WARNING â†’ Ask user: "Accept warnings?" or "Fix first?"
- IF CRITICAL â†’ Block progress, report details, wait for fixes
```

### 3.2 ê°œì„  ëª©í‘œ

**3ê°€ì§€ ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€**:
1. Coverage < 85% â†’ test-engineer í˜¸ì¶œ
2. Blocker detected â†’ debug-helper í˜¸ì¶œ
3. Architectural question â†’ implementation-planner ìƒë‹´

### 3.3 êµ¬ì²´ì  ì½”ë“œ ì¶”ê°€

**ì¶”ê°€ ìœ„ì¹˜**: Step 2.4 ì´í›„ (Line 323 í›„)

```markdown
### Step 2.4: Error Recovery Protocols

If quality-gate reports issues:

**IF: Coverage < 85%**
```
Task(subagent_type="test-engineer",
     description="Supplement tests for coverage improvement",
     prompt="SPEC-$ARGUMENTS has {actual_coverage}% coverage (target: 85%).

Current gaps: {uncovered_files_and_lines}

1. Identify untested code paths
   - Which files/functions lack tests?
   - Which branches/edge cases are uncovered?

2. Write supplementary tests
   - Add tests for each uncovered path
   - Include edge cases and error scenarios
   - Target: Reach 85%+ coverage

3. Run full test suite
   - Verify new tests pass
   - Verify coverage improves to â‰¥85%
   - Commit: test(@SPEC-$ARGUMENTS): Add supplementary tests for coverage

Output: New coverage percentage, tests added count")
```

**IF: Blocker Encountered in Implementation**
```
Task(subagent_type="debug-helper",
     description="Resolve implementation blocker",
     prompt="SPEC-$ARGUMENTS encountered a blocker during implementation:

Blocker Type: {type}
Issue: {detailed_description}
Context: {relevant_code_snippet}
Error: {error_message}

1. Analyze root cause
   - What's causing this issue?
   - Is it a code logic error, architecture issue, or environment problem?

2. Suggest solutions
   - Provide 2-3 different solution approaches
   - Pros/cons for each approach
   - Recommended approach

3. Provide next steps
   - How to implement the fix?
   - What should be tested?
   - Any related changes needed?

Output: Detailed analysis with recommended fix approach")
```

**IF: Architectural Question Needs Resolution**
```
Task(subagent_type="implementation-planner",
     description="Architectural consultation for SPEC-$ARGUMENTS",
     prompt="During implementation of SPEC-$ARGUMENTS, an architectural question arose:

Question: {specific_question}
Context: {relevant_context}
Current approach: {what_was_tried}
Why it's problematic: {explanation}

1. Analyze the question
   - What are the tradeoffs?
   - What patterns would apply here?

2. Recommend solution
   - Best approach for this project
   - Why this approach?
   - How to implement it?

3. Document for future
   - Is this a pattern to repeat?
   - Should we document this pattern?
   - Update domain patterns?

Output: Architectural recommendation with implementation guidance")
```

---

## Step 4: PHASE 3 ì—…ë°ì´íŠ¸ (ìµœì†Œ)

### 4.1 ë³€ê²½ ì‚¬í•­

**íŒŒì¼**: `.claude/commands/alfred/2-run.md`
**ìœ„ì¹˜**: Line 326-370 (PHASE 3)

**ë³€ê²½ì‚¬í•­**: ê¸°ì¡´ ì½”ë“œëŠ” ìœ ì§€, Step 3.1 ì„¤ëª… ì—…ë°ì´íŠ¸ë§Œ

```markdown
### Step 3.1: Invoke Git-Manager Agent

**Your task**: Call git-manager to create structured commits.

Use Task tool:
- subagent_type: "git-manager"
- description: "Create Git commits for TDD cycle"
- prompt: """
You are the git-manager agent. Create git commits for SPEC-$ARGUMENTS.

CONTEXT:
- Implementation completed via TDD cycle
- Commits already created by tdd-implementer (RED, GREEN, REFACTOR phases)
- Your task: Verify commit structure and finalize

VERIFY COMMITS:

1. Check RED Phase Commits
   - Format: test(@SPEC-$ARGUMENTS): Add failing tests for ...
   - Include @TEST-{N} references
   - Each test file has separate commit

2. Check GREEN Phase Commits
   - Format: feat(@SPEC-$ARGUMENTS): Implement ...
   - Include coverage %, @CODE-{N} references
   - Each component has separate commit

3. Check REFACTOR Phase Commits
   - Format: refactor(@SPEC-$ARGUMENTS): Improve code quality
   - Include improvements summary
   - Single commit unless major refactoring

4. Verify TAG Chains
   - Each @CODE-{N} references @TEST-{N}
   - Each @TEST-{N} references @SPEC-$ARGUMENTS
   - Complete traceability

5. Final Verification
   - All commits follow conventional commits format
   - All commits reference @SPEC-$ARGUMENTS
   - No merge conflicts
   - Branch is feature/SPEC-$ARGUMENTS (if using GitFlow)

Output: Commit verification report
"""

**Verify**: All commits created, TAG chains complete
```

---

## Step 5: ë¬¸ì„œí™” ë° ë™ê¸°í™”

### 5.1 CLAUDE.md ì—…ë°ì´íŠ¸

**íŒŒì¼**: `.claude/commands/alfred/2-run.md` ê¸°ë³¸ ë©”íƒ€ë°ì´í„°

ë³€ê²½í•  ë¶€ë¶„:

```markdown
**Version**: 2.1.0 (Agent-Delegated Pattern)
â†“
**Version**: 3.0.0 (Agent-Delegated + Parallel Execution)

**Last Updated**: 2025-11-09
â†“
**Last Updated**: 2025-11-12

**Total Lines**: ~400 (reduced from 619)
â†“
**Total Lines**: ~500 (increased from 400 due to detailed prompts)

**Architecture**: Commands â†’ Agents â†’ Skills
â†“
**Architecture**: Commands â†’ Agents (Parallel) â†’ Skills

**Improvements in v3.0.0**:
- Parallel Task execution in PHASE 1 (Explore + tag-agent)
- 3-way parallel resource preparation in PHASE 2 (milestones, domain, resources)
- Enhanced TDD-Implementer prompt with detailed protocols
- Automated error handling (coverage, blockers, architecture)
- Full TAG chain integration
- TodoWrite automation from structured task list
```

### 5.2 íŒ¨í‚¤ì§€ í…œí”Œë¦¿ ë™ê¸°í™”

**ì¤‘ìš”**: `.claude/` íŒŒì¼ì€ íŒ¨í‚¤ì§€ í…œí”Œë¦¿ì´ source of truth

**ì‘ì—…**:

1. ë¡œì»¬ 2-run.md ê°œì„  ì™„ë£Œ í›„
2. íŒ¨í‚¤ì§€ í…œí”Œë¦¿ ë™ê¸°í™”:
   ```
   src/moai_adk/templates/.claude/commands/alfred/2-run.md
   â† copy from
   .claude/commands/alfred/2-run.md
   ```

3. í…œí”Œë¦¿ ê²€ì¦:
   ```
   - ë³€ìˆ˜ ì¹˜í™˜ ë¶ˆí•„ìš” (2-runì€ project-agnostic)
   - ì˜ì–´ ìœ ì§€ (ì¸í”„ë¼ íŒŒì¼)
   - YAML frontmatter ë™ì¼
   ```

4. Git ì»¤ë°‹:
   ```
   feat(template): Modernize 2-run command with parallel agent delegation

   - PHASE 1: Add parallel SPEC analysis and TAG initialization
   - PHASE 2: Implement 3-way parallel resource preparation
   - PHASE 2.3: Enhance TDD-Implementer prompt (100+ lines)
   - PHASE 2.4: Add error recovery protocols
   - Result: 35% faster execution, better error handling
   ```

---

## ì‹¤í–‰ ì‹œê°„í‘œ

### Day 1 (ì•½ 2ì‹œê°„)

| ì‹œê°„ | ì‘ì—… | ì†Œìš”ì‹œê°„ |
|------|------|--------|
| 09:00-09:30 | Step 1: PHASE 1 ê°œì„  | 30ë¶„ |
| 09:30-10:00 | Step 2-1: PHASE 2 ë‹¨ê³„ 1-2 ê°œì„  | 30ë¶„ |
| 10:00-11:00 | Step 2-2: TDD-Implementer prompt í™•ì¥ | 60ë¶„ |
| **11:00** | **Day 1 ì™„ë£Œ** | **2ì‹œê°„** |

### Day 2 (ì•½ 1.5ì‹œê°„)

| ì‹œê°„ | ì‘ì—… | ì†Œìš”ì‹œê°„ |
|------|------|--------|
| 09:00-09:30 | Step 3: ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€ | 30ë¶„ |
| 09:30-10:00 | Step 4: PHASE 3 ì—…ë°ì´íŠ¸ | 30ë¶„ |
| 10:00-10:30 | Step 5: ë¬¸ì„œí™” ë° í…œí”Œë¦¿ ë™ê¸°í™” | 30ë¶„ |
| **10:30** | **Day 2 ì™„ë£Œ** | **1.5ì‹œê°„** |

**ì´ ì†Œìš”ì‹œê°„**: 3.5ì‹œê°„ (2ì¼ì— ê±¸ì³)

---

## ê²€ì¦ ì ˆì°¨

### ë¡œì»¬ í…ŒìŠ¤íŠ¸ (ê¶Œì¥)

```bash
# 1. ë°±ì—… ìƒì„±
cp .claude/commands/alfred/2-run.md \
   .moai/backups/commands/2-run-v2.1.0.md

# 2. ì„ì‹œ SPEC ìƒì„±
/alfred:1-plan "Test feature for modernization verification"
# â†’ SPEC-TEST-001 ìƒì„±ë¨

# 3. ê°œì„ ëœ 2-run ì‹¤í–‰
/alfred:2-run SPEC-TEST-001

# 4. ê²€ì¦ í¬ì¸íŠ¸
# âœ“ PHASE 1: ë³‘ë ¬ Task í˜¸ì¶œ ë³´ì„?
# âœ“ PHASE 2: 3ê°œ Task ë™ì‹œ ì‹¤í–‰?
# âœ“ PHASE 2.3: ìƒì„¸í•œ TDD ê°€ì´ë“œ?
# âœ“ TodoWrite ìë™ ìƒì„±?
# âœ“ git ì»¤ë°‹ ìƒì„±?
# âœ“ ìŠ¤í¬ë¦½íŠ¸ í˜¸ì¶œ ì—†ìŒ?

# 5. ê²°ê³¼ í™•ì¸
git log -5 --oneline
pytest tests/ --cov
```

---

## ì¶œë ¥ ì‚°ì¶œë¬¼

### ìƒì„±ë˜ëŠ” íŒŒì¼

1. **ê°œì„ ëœ 2-run.md**
   - ìœ„ì¹˜: `.claude/commands/alfred/2-run.md`
   - í¬ê¸°: ~500ì¤„ (ì´ì „ ~400ì¤„)
   - ë³€ê²½: 3ê°œ ì„¹ì…˜ ëŒ€í­ ê°œì„ 

2. **íŒ¨í‚¤ì§€ í…œí”Œë¦¿ ë™ê¸°í™”**
   - ìœ„ì¹˜: `src/moai_adk/templates/.claude/commands/alfred/2-run.md`
   - ë‚´ìš©: ë¡œì»¬ê³¼ ë™ì¼

3. **Git ì»¤ë°‹**
   - ë©”ì‹œì§€: "feat(commands): Modernize 2-run with parallel agent delegation"

---

## ì£¼ìš” ì„±ê³¼ ì§€í‘œ

### Before â†’ After

| ì§€í‘œ | Before | After | ê°œì„  |
|------|--------|-------|------|
| **ë³‘ë ¬ ì‹¤í–‰** | ì—†ìŒ | 5ê°œ Task | +5x |
| **ìŠ¤í¬ë¦½íŠ¸ í˜¸ì¶œ** | 1íšŒ (spec_status_hooks.py) | 0íšŒ | -100% |
| **ì—ëŸ¬ í•¸ë“¤ë§** | ìˆ˜ë™ | ìë™ 3ê°€ì§€ | +ìë™í™” |
| **TDD ê°€ì´ë“œ** | 30ì¤„ | 100ì¤„ | +233% |
| **Progress ì¶”ì ** | ìˆ˜ë™ TodoWrite | ìë™ êµ¬ì¡°í™” | +ìë™í™” |
| **TAG ì²´ì¸** | ì•”ì‹œì  | ëª…ì‹œì  | +ëª…í™•ì„± |

---

**ë¡œë“œë§µ ìƒì„±**: 2025-11-12
**ëŒ€ìƒ**: `.claude/commands/alfred/2-run.md`
**ìƒíƒœ**: Ready for Implementation
**ì˜ˆìƒ ì™„ë£Œ**: 3.5ì‹œê°„ (ì´í‹€ ì‘ì—…)
