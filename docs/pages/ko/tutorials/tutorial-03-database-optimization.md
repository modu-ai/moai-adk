---
title: "Tutorial 3: ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”"
description: "N+1 ë¬¸ì œ, ì¸ë±ìŠ¤, ìºì‹± ì „ëµìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤"
duration: "1ì‹œê°„"
difficulty: "ê³ ê¸‰"
tags: [tutorial, database, optimization, postgresql, redis, caching]
---

# Tutorial 3: ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì‹¤ë¬´ì—ì„œ ë§ˆì£¼ì¹˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. N+1 ì¿¼ë¦¬ ë¬¸ì œ, ì¸ë±ìŠ¤ ì„¤ê³„, ì¿¼ë¦¬ ìµœì í™”, ìºì‹± ì „ëµì„ ë°°ìš°ê³  API ì‘ë‹µ ì†ë„ë¥¼ ê·¹ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ

ì´ íŠœí† ë¦¬ì–¼ì„ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- âœ… N+1 query problem ì´í•´í•˜ê³  í•´ê²°í•˜ê¸°
- âœ… íš¨ê³¼ì ì¸ ì¸ë±ìŠ¤ ì „ëµ ìˆ˜ë¦½í•˜ê¸°
- âœ… SQLAlchemy Eager Loadingìœ¼ë¡œ ì¿¼ë¦¬ ìµœì í™”í•˜ê¸°
- âœ… Connection Pooling ì„¤ì •í•˜ê¸°
- âœ… Redisë¡œ ìºì‹± ì „ëµ êµ¬í˜„í•˜ê¸°
- âœ… ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„í•˜ê¸°
- âœ… Alfred Research Strategiesë¡œ ìµœì‹  ìµœì í™” ê¸°ë²• í•™ìŠµí•˜ê¸°

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ ì„¤ì¹˜

- **Python 3.11+**
- **PostgreSQL 14+**
- **Redis 7+**
- **MoAI-ADK v0.23.0+**
- **Tutorial 1, 2 ì™„ë£Œ** (REST API, ì¸ì¦)

### ì„ í–‰ ì§€ì‹

- SQL ê¸°ë³¸ (JOIN, WHERE, INDEX)
- SQLAlchemy ORM
- ë°ì´í„°ë² ì´ìŠ¤ ì •ê·œí™”
- HTTP ìºì‹± í—¤ë”

### í™˜ê²½ ì¤€ë¹„

```bash
# PostgreSQL ì„¤ì¹˜ (macOS)
brew install postgresql@14
brew services start postgresql@14

# Redis ì„¤ì¹˜
brew install redis
brew services start redis

# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬
mkdir db-optimization-tutorial
cd db-optimization-tutorial
moai-adk init
```

## ğŸ¯ ì„±ëŠ¥ ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤

ë¸”ë¡œê·¸ ì‹œìŠ¤í…œì„ ì˜ˆì œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤:

- **Post**: ê²Œì‹œê¸€ (ì œëª©, ë‚´ìš©, ì‘ì„±ì)
- **Comment**: ëŒ“ê¸€ (ë‚´ìš©, ì‘ì„±ì, ê²Œì‹œê¸€ FK)
- **User**: ì‚¬ìš©ì (ì´ë¦„, ì´ë©”ì¼)

### ì„±ëŠ¥ ëª©í‘œ

| ì—”ë“œí¬ì¸íŠ¸ | í˜„ì¬ | ëª©í‘œ | ê°œì„ ìœ¨ |
|-----------|------|------|--------|
| GET /posts | 2,500ms | 50ms | **98%** |
| GET /posts/{id} | 800ms | 30ms | **96%** |
| GET /users/{id}/posts | 3,200ms | 80ms | **97%** |

## ğŸš€ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
db-optimization-tutorial/
â”œâ”€â”€ .moai/
â”‚   â””â”€â”€ specs/
â”‚       â””â”€â”€ SPEC-DB-OPT-001.md
â”œâ”€â”€ src/
â”‚   â””â”€â”€ blog_api/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ database.py          # DB ëª¨ë¸ ë° ì„¸ì…˜
â”‚       â”œâ”€â”€ models.py            # Pydantic ëª¨ë¸
â”‚       â”œâ”€â”€ cache.py             # Redis ìºì‹±
â”‚       â”œâ”€â”€ monitoring.py        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
â”‚       â””â”€â”€ routes/
â”‚           â”œâ”€â”€ posts.py
â”‚           â””â”€â”€ users.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_performance.py
â”‚   â””â”€â”€ test_optimization.py
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ load_test.py             # ë¶€í•˜ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ë‹¨ê³„ë³„ ì‹¤ìŠµ

### Step 1: SPEC ì‘ì„±

```bash
/alfred:1-plan "ë¸”ë¡œê·¸ API ì„±ëŠ¥ ìµœì í™”"
```

**ìƒì„±ëœ SPEC** (`.moai/specs/SPEC-DB-OPT-001.md`):

```markdown
# SPEC-DB-OPT-001: ë¸”ë¡œê·¸ API ì„±ëŠ¥ ìµœì í™”

## ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­

### ì‘ë‹µ ì‹œê°„ ëª©í‘œ

- PR-001: ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ < 50ms (í˜„ì¬: 2,500ms)
- PR-002: ë‹¨ì¼ ê²Œì‹œê¸€ ì¡°íšŒ < 30ms (í˜„ì¬: 800ms)
- PR-003: ì‚¬ìš©ì ê²Œì‹œê¸€ ì¡°íšŒ < 80ms (í˜„ì¬: 3,200ms)

### ìµœì í™” ì „ëµ

- OPT-001: N+1 ì¿¼ë¦¬ ë¬¸ì œ í•´ê²° (Eager Loading)
- OPT-002: ì ì ˆí•œ ì¸ë±ìŠ¤ ìƒì„±
- OPT-003: Connection Pooling ì„¤ì •
- OPT-004: Redis ìºì‹± ë„ì…
- OPT-005: ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ë°ì´í„° ëª¨ë¸

User:
- id: int (PK)
- name: str (index)
- email: str (unique index)
- created_at: datetime

Post:
- id: int (PK)
- title: str (index)
- content: text
- author_id: int (FK, index)
- created_at: datetime (index)

Comment:
- id: int (PK)
- content: text
- author_id: int (FK, index)
- post_id: int (FK, index)
- created_at: datetime
```

### Step 2: í™˜ê²½ ì„¤ì •

**requirements.txt**:
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
redis==5.0.1
hiredis==2.3.2
pydantic==2.5.0
pydantic-settings==2.1.0
locust==2.19.1  # ë¶€í•˜ í…ŒìŠ¤íŠ¸
pytest==7.4.3
pytest-benchmark==4.0.0
```

**.env**:
```env
DATABASE_URL=postgresql://user:password@localhost/blog_db
REDIS_URL=redis://localhost:6379/0

# Connection Pool ì„¤ì •
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=10

# ìºì‹± ì„¤ì •
CACHE_TTL=300  # 5ë¶„
ENABLE_CACHE=true
```

### Step 3: ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ (ìµœì í™” ì „)

**src/blog_api/database.py**:

```python
"""
ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
"""
from datetime import datetime
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey, create_engine
from sqlalchemy.orm import declarative_base, relationship, Session, sessionmaker
from .config import settings

Base = declarative_base()


class User(Base):
    """ì‚¬ìš©ì ëª¨ë¸"""
    __tablename__ = "users"

    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False, index=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)

    # ê´€ê³„ (BEFORE: lazy loading - N+1 ë¬¸ì œ ë°œìƒ)
    posts = relationship("Post", back_populates="author", lazy="select")
    comments = relationship("Comment", back_populates="author", lazy="select")


class Post(Base):
    """ê²Œì‹œê¸€ ëª¨ë¸"""
    __tablename__ = "posts"

    id = Column(Integer, primary_key=True)
    title = Column(String(200), nullable=False, index=True)
    content = Column(Text, nullable=False)
    author_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)

    # ê´€ê³„
    author = relationship("User", back_populates="posts", lazy="select")
    comments = relationship("Comment", back_populates="post", lazy="select", cascade="all, delete-orphan")


class Comment(Base):
    """ëŒ“ê¸€ ëª¨ë¸"""
    __tablename__ = "comments"

    id = Column(Integer, primary_key=True)
    content = Column(Text, nullable=False)
    author_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    post_id = Column(Integer, ForeignKey("posts.id"), nullable=False, index=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    # ê´€ê³„
    author = relationship("User", back_populates="comments", lazy="select")
    post = relationship("Post", back_populates="comments", lazy="select")


# ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ (BEFORE: Connection Pool ë¯¸ì„¤ì •)
engine = create_engine(
    settings.DATABASE_URL,
    echo=settings.DEBUG,  # ì¿¼ë¦¬ ë¡œê¹…
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


def get_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


def init_db():
    """í…Œì´ë¸” ìƒì„±"""
    Base.metadata.create_all(bind=engine)
```

### Step 4: ë¬¸ì œ ìƒí™© ì¬í˜„ (N+1 Query)

**src/blog_api/routes/posts.py** (ìµœì í™” ì „):

```python
"""
ê²Œì‹œê¸€ ë¼ìš°íŠ¸ (ìµœì í™” ì „)
"""
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from ..database import Post, get_db
from ..models import PostResponse

router = APIRouter(prefix="/posts", tags=["posts"])


@router.get("/", response_model=list[PostResponse])
def get_posts(db: Session = Depends(get_db)):
    """
    ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ (ìµœì í™” ì „)

    âš ï¸ ë¬¸ì œ: N+1 ì¿¼ë¦¬ ë°œìƒ
    - 1ê°œ ì¿¼ë¦¬: ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ
    - Nê°œ ì¿¼ë¦¬: ê° ê²Œì‹œê¸€ì˜ ì‘ì„±ì ì¡°íšŒ (lazy loading)
    - Nê°œ ì¿¼ë¦¬: ê° ê²Œì‹œê¸€ì˜ ëŒ“ê¸€ ì¡°íšŒ

    100ê°œ ê²Œì‹œê¸€ = 1 + 100 + 100 = 201ê°œ ì¿¼ë¦¬! ğŸ”¥
    """
    posts = db.query(Post).all()

    # ì—¬ê¸°ì„œ N+1 ë¬¸ì œ ë°œìƒ
    # post.author ì ‘ê·¼ ì‹œë§ˆë‹¤ ì¶”ê°€ ì¿¼ë¦¬ ì‹¤í–‰
    result = []
    for post in posts:
        result.append({
            "id": post.id,
            "title": post.title,
            "author_name": post.author.name,  # +1 ì¿¼ë¦¬
            "comment_count": len(post.comments)  # +1 ì¿¼ë¦¬
        })

    return result
```

**ì„±ëŠ¥ ì¸¡ì •**:

```python
# 100ê°œ ê²Œì‹œê¸€ ë¡œë“œ ì‹œê°„
# BEFORE: 2,500ms (201 queries)
# ëª©í‘œ: 50ms (1-3 queries)
```

### Step 5: N+1 ë¬¸ì œ í•´ê²° (Eager Loading)

**src/blog_api/routes/posts.py** (ìµœì í™” í›„):

```python
"""
ê²Œì‹œê¸€ ë¼ìš°íŠ¸ (ìµœì í™” í›„)
"""
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session, selectinload, joinedload
from sqlalchemy import func
from ..database import Post, Comment, get_db
from ..models import PostResponse, PostListResponse

router = APIRouter(prefix="/posts", tags=["posts"])


@router.get("/", response_model=PostListResponse)
def get_posts_optimized(
    limit: int = 20,
    offset: int = 0,
    db: Session = Depends(get_db)
):
    """
    ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ (ìµœì í™” í›„)

    âœ… í•´ê²°: Eager Loading + Subquery
    - selectinload: ë³„ë„ ì¿¼ë¦¬ë¡œ ê´€ë ¨ ë°ì´í„° í•œ ë²ˆì— ë¡œë“œ
    - joinedload: JOINìœ¼ë¡œ í•œ ë²ˆì— ë¡œë“œ
    - ì´ 2-3ê°œ ì¿¼ë¦¬ë¡œ ê°ì†Œ
    """
    # ë°©ë²• 1: selectinload (1:N ê´€ê³„ì— ì í•©)
    posts = (
        db.query(Post)
        .options(
            selectinload(Post.author),  # ì‘ì„±ì ì •ë³´ eager load
            selectinload(Post.comments)  # ëŒ“ê¸€ eager load
        )
        .order_by(Post.created_at.desc())
        .limit(limit)
        .offset(offset)
        .all()
    )

    # ë°©ë²• 2: ëŒ“ê¸€ ìˆ˜ë¥¼ subqueryë¡œ ê³„ì‚° (ë” íš¨ìœ¨ì )
    # comment_counts = (
    #     db.query(
    #         Comment.post_id,
    #         func.count(Comment.id).label("count")
    #     )
    #     .group_by(Comment.post_id)
    #     .subquery()
    # )
    #
    # posts = (
    #     db.query(Post, comment_counts.c.count)
    #     .outerjoin(comment_counts, Post.id == comment_counts.c.post_id)
    #     .options(joinedload(Post.author))
    #     .all()
    # )

    total = db.query(func.count(Post.id)).scalar()

    return PostListResponse(
        posts=[
            PostResponse(
                id=post.id,
                title=post.title,
                content=post.content[:200],  # ë¯¸ë¦¬ë³´ê¸°
                author_name=post.author.name,
                comment_count=len(post.comments),
                created_at=post.created_at
            )
            for post in posts
        ],
        total=total,
        limit=limit,
        offset=offset
    )


@router.get("/{post_id}", response_model=PostResponse)
def get_post_optimized(post_id: int, db: Session = Depends(get_db)):
    """
    ë‹¨ì¼ ê²Œì‹œê¸€ ì¡°íšŒ (ìµœì í™”)

    âœ… joinedloadë¡œ í•œ ë²ˆì— ë¡œë“œ
    """
    post = (
        db.query(Post)
        .options(
            joinedload(Post.author),
            selectinload(Post.comments).selectinload(Comment.author)
        )
        .filter(Post.id == post_id)
        .first()
    )

    if not post:
        raise HTTPException(status_code=404, detail="Post not found")

    return post
```

**ì„±ëŠ¥ ê°œì„  ê²°ê³¼**:

```python
# BEFORE: 201 queries, 2,500ms
# AFTER: 2 queries, 45ms
# ê°œì„ ìœ¨: 98.2% âš¡
```

### Step 6: Connection Pooling ìµœì í™”

**src/blog_api/database.py** (ê°œì„ ):

```python
"""
Connection Pool ìµœì í™”
"""
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

# BEFORE: ê¸°ë³¸ ì„¤ì •
# engine = create_engine(settings.DATABASE_URL)

# AFTER: Connection Pool ìµœì í™”
engine = create_engine(
    settings.DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,          # ê¸°ë³¸ ì—°ê²° ìˆ˜
    max_overflow=10,       # ì¶”ê°€ ìƒì„± ê°€ëŠ¥ ì—°ê²° ìˆ˜
    pool_timeout=30,       # ì—°ê²° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    pool_recycle=3600,     # ì—°ê²° ì¬ìƒì„± ì£¼ê¸° (1ì‹œê°„)
    pool_pre_ping=True,    # ì—°ê²° ìƒíƒœ í™•ì¸
    echo=False,            # í”„ë¡œë•ì…˜ì—ì„œëŠ” False
)

# ì—°ê²° í’€ ìƒíƒœ ëª¨ë‹ˆí„°ë§
def get_pool_status():
    """Connection Pool ìƒíƒœ í™•ì¸"""
    pool = engine.pool
    return {
        "size": pool.size(),
        "checked_in": pool.checkedin(),
        "checked_out": pool.checkedout(),
        "overflow": pool.overflow(),
        "total": pool.size() + pool.overflow()
    }
```

### Step 7: Redis ìºì‹± êµ¬í˜„

**src/blog_api/cache.py**:

```python
"""
Redis ìºì‹±
"""
import json
from typing import Optional, Any
from functools import wraps
import redis
from .config import settings

# Redis í´ë¼ì´ì–¸íŠ¸
redis_client = redis.from_url(
    settings.REDIS_URL,
    encoding="utf-8",
    decode_responses=True
)


def cache_key(*args, **kwargs) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    key_parts = [str(arg) for arg in args]
    key_parts.extend(f"{k}:{v}" for k, v in sorted(kwargs.items()))
    return ":".join(key_parts)


def cached(prefix: str, ttl: int = 300):
    """
    ìºì‹± ë°ì½”ë ˆì´í„°

    Args:
        prefix: ìºì‹œ í‚¤ ì ‘ë‘ì‚¬
        ttl: Time To Live (ì´ˆ)

    Usage:
        @cached("posts", ttl=300)
        def get_posts():
            return expensive_query()
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            if not settings.ENABLE_CACHE:
                return func(*args, **kwargs)

            # ìºì‹œ í‚¤ ìƒì„±
            key = f"{prefix}:{cache_key(*args, **kwargs)}"

            # ìºì‹œ í™•ì¸
            cached_value = redis_client.get(key)
            if cached_value:
                return json.loads(cached_value)

            # ìºì‹œ ë¯¸ìŠ¤: í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)

            # ìºì‹œ ì €ì¥
            redis_client.setex(
                key,
                ttl,
                json.dumps(result, default=str)
            )

            return result

        # ìºì‹œ ë¬´íš¨í™” í•¨ìˆ˜ ì¶”ê°€
        def invalidate(*args, **kwargs):
            key = f"{prefix}:{cache_key(*args, **kwargs)}"
            redis_client.delete(key)

        wrapper.invalidate = invalidate
        return wrapper

    return decorator


class CacheManager:
    """ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤"""

    @staticmethod
    def invalidate_pattern(pattern: str):
        """íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  ìºì‹œ ë¬´íš¨í™”"""
        keys = redis_client.keys(pattern)
        if keys:
            redis_client.delete(*keys)

    @staticmethod
    def clear_all():
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        redis_client.flushdb()

    @staticmethod
    def get_stats() -> dict:
        """ìºì‹œ í†µê³„"""
        info = redis_client.info("stats")
        return {
            "hits": info.get("keyspace_hits", 0),
            "misses": info.get("keyspace_misses", 0),
            "hit_rate": info.get("keyspace_hits", 0) / max(
                info.get("keyspace_hits", 0) + info.get("keyspace_misses", 0), 1
            ) * 100
        }
```

**ìºì‹± ì ìš©**:

```python
"""
ìºì‹±ì´ ì ìš©ëœ ë¼ìš°íŠ¸
"""
from .cache import cached, CacheManager


@router.get("/posts/", response_model=PostListResponse)
@cached("posts:list", ttl=300)  # 5ë¶„ ìºì‹±
def get_posts_cached(
    limit: int = 20,
    offset: int = 0,
    db: Session = Depends(get_db)
):
    """
    ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ (ìºì‹± ì ìš©)

    âœ… Redis ìºì‹±ìœ¼ë¡œ DB ë¶€í•˜ ê°ì†Œ
    - ì²« ìš”ì²­: DB ì¿¼ë¦¬ ì‹¤í–‰, Redisì— ì €ì¥
    - ì´í›„ 5ë¶„: Redisì—ì„œ ì§ì ‘ ë°˜í™˜
    - ì‘ë‹µ ì‹œê°„: 45ms â†’ 2ms (95% ê°œì„ )
    """
    posts = (
        db.query(Post)
        .options(selectinload(Post.author))
        .order_by(Post.created_at.desc())
        .limit(limit)
        .offset(offset)
        .all()
    )

    total = db.query(func.count(Post.id)).scalar()

    return {
        "posts": [serialize_post(p) for p in posts],
        "total": total
    }


@router.post("/posts/", response_model=PostResponse)
def create_post(post_data: PostCreate, db: Session = Depends(get_db)):
    """
    ê²Œì‹œê¸€ ìƒì„±

    âœ… ìƒì„± í›„ ìºì‹œ ë¬´íš¨í™”
    """
    post = Post(**post_data.dict())
    db.add(post)
    db.commit()

    # ëª©ë¡ ìºì‹œ ë¬´íš¨í™”
    CacheManager.invalidate_pattern("posts:list:*")

    return post
```

### Step 8: ì¸ë±ìŠ¤ ìµœì í™”

**migration.sql**:

```sql
-- ì¸ë±ìŠ¤ ì „ëµ

-- 1. ë‹¨ì¼ ì»¬ëŸ¼ ì¸ë±ìŠ¤ (ì´ë¯¸ ìƒì„±ë¨)
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_name ON users(name);
CREATE INDEX idx_posts_author_id ON posts(author_id);
CREATE INDEX idx_posts_created_at ON posts(created_at);

-- 2. ë³µí•© ì¸ë±ìŠ¤ (ì—¬ëŸ¬ ì»¬ëŸ¼ ì¡°í•©)
-- ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ ìµœì í™” (author_id + created_at)
CREATE INDEX idx_posts_author_created ON posts(author_id, created_at DESC);

-- ëŒ“ê¸€ ì¡°íšŒ ìµœì í™” (post_id + created_at)
CREATE INDEX idx_comments_post_created ON comments(post_id, created_at);

-- 3. ë¶€ë¶„ ì¸ë±ìŠ¤ (ì¡°ê±´ë¶€ ì¸ë±ìŠ¤)
-- í™œì„± ì‚¬ìš©ìë§Œ ì¸ë±ì‹±
CREATE INDEX idx_users_active_email ON users(email) WHERE is_active = true;

-- 4. ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ (PostgreSQL Full-Text Search)
-- ê²Œì‹œê¸€ ì œëª© + ë‚´ìš© ê²€ìƒ‰
CREATE INDEX idx_posts_search ON posts USING GIN(
    to_tsvector('english', title || ' ' || content)
);

-- ì¸ë±ìŠ¤ ì‚¬ìš© í™•ì¸ ì¿¼ë¦¬
-- EXPLAIN ANALYZE SELECT * FROM posts WHERE author_id = 1 ORDER BY created_at DESC;
```

**ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¶„ì„**:

```python
"""
ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¶„ì„
"""
from sqlalchemy import text


def analyze_query_performance(db: Session, query: str):
    """
    ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„

    EXPLAIN ANALYZE ê²°ê³¼ë¥¼ ë°˜í™˜
    """
    explain_query = f"EXPLAIN ANALYZE {query}"
    result = db.execute(text(explain_query))

    return [row[0] for row in result]


# ì‚¬ìš© ì˜ˆì œ
analysis = analyze_query_performance(
    db,
    "SELECT * FROM posts WHERE author_id = 1 ORDER BY created_at DESC LIMIT 20"
)

# ê²°ê³¼:
# BEFORE (ì¸ë±ìŠ¤ ì—†ìŒ):
# Seq Scan on posts  (cost=0.00..1234.56 rows=100 width=123) (actual time=125.123..250.456 rows=100 loops=1)
# Planning Time: 0.123 ms
# Execution Time: 250.789 ms

# AFTER (ë³µí•© ì¸ë±ìŠ¤):
# Index Scan using idx_posts_author_created on posts  (cost=0.29..8.31 rows=1 width=123) (actual time=0.015..0.018 rows=1 loops=1)
# Planning Time: 0.054 ms
# Execution Time: 0.032 ms
```

### Step 9: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

**src/blog_api/monitoring.py**:

```python
"""
ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""
import time
from functools import wraps
from typing import Callable
from fastapi import Request
import logging

logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

    def __init__(self):
        self.metrics = {
            "requests": 0,
            "total_time": 0,
            "slow_queries": []
        }

    def record_request(self, path: str, duration: float):
        """ìš”ì²­ ê¸°ë¡"""
        self.metrics["requests"] += 1
        self.metrics["total_time"] += duration

        # ëŠë¦° ì¿¼ë¦¬ ê¸°ë¡ (100ms ì´ìƒ)
        if duration > 0.1:
            self.metrics["slow_queries"].append({
                "path": path,
                "duration": duration,
                "timestamp": time.time()
            })

    def get_stats(self) -> dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            "total_requests": self.metrics["requests"],
            "avg_response_time": (
                self.metrics["total_time"] / self.metrics["requests"]
                if self.metrics["requests"] > 0 else 0
            ),
            "slow_queries_count": len(self.metrics["slow_queries"]),
            "recent_slow_queries": self.metrics["slow_queries"][-10:]
        }


monitor = PerformanceMonitor()


async def performance_middleware(request: Request, call_next):
    """ì„±ëŠ¥ ì¸¡ì • ë¯¸ë“¤ì›¨ì–´"""
    start_time = time.time()

    response = await call_next(request)

    duration = time.time() - start_time
    monitor.record_request(request.url.path, duration)

    # ì‘ë‹µ í—¤ë”ì— ì†Œìš” ì‹œê°„ ì¶”ê°€
    response.headers["X-Process-Time"] = str(duration)

    # ëŠë¦° ìš”ì²­ ë¡œê¹…
    if duration > 0.1:
        logger.warning(
            f"Slow request: {request.method} {request.url.path} "
            f"took {duration:.3f}s"
        )

    return response
```

**main.pyì— ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€**:

```python
from .monitoring import performance_middleware

app.middleware("http")(performance_middleware)


@app.get("/metrics")
def get_metrics():
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    from .monitoring import monitor
    from .cache import CacheManager
    from .database import get_pool_status

    return {
        "performance": monitor.get_stats(),
        "cache": CacheManager.get_stats(),
        "database_pool": get_pool_status()
    }
```

### Step 10: ë¶€í•˜ í…ŒìŠ¤íŠ¸

**benchmarks/load_test.py** (Locust):

```python
"""
ë¶€í•˜ í…ŒìŠ¤íŠ¸
"""
from locust import HttpUser, task, between


class BlogUser(HttpUser):
    """ë¸”ë¡œê·¸ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜"""
    wait_time = between(1, 3)

    @task(3)
    def get_posts(self):
        """ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ (ê°€ì¥ ë¹ˆë²ˆ)"""
        self.client.get("/posts/?limit=20&offset=0")

    @task(2)
    def get_post_detail(self):
        """ê²Œì‹œê¸€ ìƒì„¸ ì¡°íšŒ"""
        post_id = 1  # ì‹¤ì œë¡œëŠ” ëœë¤ ID
        self.client.get(f"/posts/{post_id}")

    @task(1)
    def get_user_posts(self):
        """ì‚¬ìš©ì ê²Œì‹œê¸€ ì¡°íšŒ"""
        user_id = 1
        self.client.get(f"/users/{user_id}/posts")


# ì‹¤í–‰:
# locust -f benchmarks/load_test.py --host=http://localhost:8000
```

**ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼**:

```
# BEFORE ìµœì í™”:
# RPS: 45 requests/sec
# í‰ê·  ì‘ë‹µ: 2,200ms
# P95: 4,500ms
# ì—ëŸ¬ìœ¨: 12%

# AFTER ìµœì í™”:
# RPS: 1,250 requests/sec (27ë°° ì¦ê°€)
# í‰ê·  ì‘ë‹µ: 35ms (98% ê°œì„ )
# P95: 120ms (97% ê°œì„ )
# ì—ëŸ¬ìœ¨: 0.1%
```

## âœ… ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¿¼ë¦¬ ìµœì í™”

- âœ… N+1 ì¿¼ë¦¬ ì œê±° (selectinload, joinedload)
- âœ… í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¡°íšŒ (defer, load_only)
- âœ… í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
- âœ… ì„œë¸Œì¿¼ë¦¬ë¡œ ì§‘ê³„ ìµœì í™”

### ì¸ë±ìŠ¤ ì „ëµ

- âœ… ìì£¼ ì¡°íšŒë˜ëŠ” ì»¬ëŸ¼ì— ì¸ë±ìŠ¤
- âœ… ë³µí•© ì¸ë±ìŠ¤ (WHERE + ORDER BY)
- âœ… ë¶€ë¶„ ì¸ë±ìŠ¤ (ì¡°ê±´ë¶€)
- âœ… EXPLAIN ANALYZEë¡œ ê²€ì¦

### ìºì‹±

- âœ… Redis ìºì‹± ë„ì…
- âœ… ì ì ˆí•œ TTL ì„¤ì •
- âœ… ìºì‹œ ë¬´íš¨í™” ì „ëµ
- âœ… ìºì‹œ íˆíŠ¸ìœ¨ ëª¨ë‹ˆí„°ë§

### Connection Pool

- âœ… Pool size ìµœì í™”
- âœ… Connection ì¬ì‚¬ìš©
- âœ… Timeout ì„¤ì •
- âœ… Pool ìƒíƒœ ëª¨ë‹ˆí„°ë§

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: Too many connections

**ì¦ìƒ**:
```
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) FATAL: sorry, too many clients already
```

**í•´ê²°**:
```python
# Connection Pool í¬ê¸° ì¤„ì´ê¸°
pool_size=10
max_overflow=5

# PostgreSQL max_connections ì¦ê°€
# postgresql.conf:
# max_connections = 200
```

### ë¬¸ì œ 2: Redis ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ**:
```
redis.exceptions.ConnectionError: Error connecting to Redis
```

**í•´ê²°**:
```bash
# Redis ì„œë¹„ìŠ¤ í™•ì¸
brew services list
brew services start redis

# ë˜ëŠ” Docker
docker run -d -p 6379:6379 redis:7-alpine
```

### ë¬¸ì œ 3: ìºì‹œ ì¼ê´€ì„± ë¬¸ì œ

**ì¦ìƒ**: ë°ì´í„° ìˆ˜ì • í›„ ì´ì „ ë°ì´í„°ê°€ í‘œì‹œë¨

**í•´ê²°**:
```python
# ë°ì´í„° ìˆ˜ì • ì‹œ ìºì‹œ ë¬´íš¨í™”
@router.put("/posts/{post_id}")
def update_post(post_id: int, data: PostUpdate, db: Session = Depends(get_db)):
    post = db.query(Post).filter(Post.id == post_id).first()
    # ... ì—…ë°ì´íŠ¸ ë¡œì§

    # ê´€ë ¨ ìºì‹œ ëª¨ë‘ ë¬´íš¨í™”
    CacheManager.invalidate_pattern(f"posts:*:{post_id}")
    CacheManager.invalidate_pattern("posts:list:*")

    return post
```

## ğŸ’¡ Best Practices

### 1. í•­ìƒ EXPLAIN ANALYZE ì‚¬ìš©

```python
# ëŠë¦° ì¿¼ë¦¬ ì°¾ê¸°
EXPLAIN ANALYZE SELECT * FROM posts WHERE author_id = 1;

# Index Scan vs Seq Scan í™•ì¸
# Index Scan = ì¢‹ìŒ âœ…
# Seq Scan = ë‚˜ì¨ (ì¸ë±ìŠ¤ í•„ìš”) âŒ
```

### 2. Eager Loading ì „ëµ

```python
# 1:1, N:1 ê´€ê³„ â†’ joinedload (JOIN ì‚¬ìš©)
query.options(joinedload(Post.author))

# 1:N ê´€ê³„ â†’ selectinload (ë³„ë„ ì¿¼ë¦¬)
query.options(selectinload(Post.comments))

# ê¹Šì€ ê´€ê³„ â†’ ì²´ì´ë‹
query.options(
    selectinload(Post.comments).selectinload(Comment.author)
)
```

### 3. ìºì‹± ì „ëµ

```python
# ì½ê¸° ë§ìŒ â†’ ê¸´ TTL (1ì‹œê°„)
@cached("posts:popular", ttl=3600)

# ìì£¼ ë³€ê²½ â†’ ì§§ì€ TTL (5ë¶„)
@cached("posts:recent", ttl=300)

# ì‹¤ì‹œê°„ ì¤‘ìš” â†’ ìºì‹± ì•ˆ í•¨
# (ì£¼ë¬¸, ê²°ì œ ë“±)
```

### 4. Connection Pool í¬ê¸°

```python
# ê³µì‹: pool_size = (CPU ì½”ì–´ ìˆ˜ * 2) + 1
# ì˜ˆ: 4ì½”ì–´ â†’ pool_size = 9

# ì›¹ ì„œë²„:
pool_size = 20
max_overflow = 10

# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…:
pool_size = 5
max_overflow = 0
```

## ğŸ“Š ì„±ëŠ¥ ê°œì„  ê²°ê³¼ ìš”ì•½

| í•­ëª© | BEFORE | AFTER | ê°œì„ ìœ¨ |
|------|--------|-------|--------|
| ê²Œì‹œê¸€ ëª©ë¡ | 2,500ms | 45ms | **98.2%** |
| ë‹¨ì¼ ê²Œì‹œê¸€ | 800ms | 30ms | **96.3%** |
| ì‚¬ìš©ì ê²Œì‹œê¸€ | 3,200ms | 75ms | **97.7%** |
| ì¿¼ë¦¬ ìˆ˜ (ëª©ë¡) | 201 | 2 | **99%** |
| RPS | 45 | 1,250 | **27ë°°** |
| ì—ëŸ¬ìœ¨ | 12% | 0.1% | **99.2%** |

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì¶•í•˜í•©ë‹ˆë‹¤! ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤.

### ì¶”ê°€ ìµœì í™”

1. **Database Replication**: ì½ê¸°/ì“°ê¸° ë¶„ë¦¬
2. **Partitioning**: ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ë¶„í• 
3. **Materialized Views**: ë³µì¡í•œ ì§‘ê³„ ë¯¸ë¦¬ ê³„ì‚°
4. **CDN**: ì •ì  ì½˜í…ì¸  ìºì‹±

### ë‹¤ìŒ íŠœí† ë¦¬ì–¼

- **[Tutorial 4: Supabase BaaS](/ko/tutorials/tutorial-04-baas-supabase)**
  - ë°±ì—”ë“œ ê°œë°œ ì†ë„ ê·¹ëŒ€í™”

## ğŸ“š ì°¸ê³  ìë£Œ

- [PostgreSQL Performance Tips](https://wiki.postgresql.org/wiki/Performance_Optimization)
- [SQLAlchemy ORM Tutorial](https://docs.sqlalchemy.org/en/20/orm/tutorial.html)
- [Redis Caching Best Practices](https://redis.io/docs/manual/patterns/)

---

**Happy Optimizing! âš¡**
