# ðŸ“š Skill Factory Parallel Analysis - Step-by-Step Detailed Guide

**Goal**: Use skill-factory agents to analyze existing skill folders in parallel and derive improvement items

---

## ðŸŽ¯ Overall Flow (3 Steps)

```
Step 1ï¸âƒ£  â†’ Select analysis targets (Tier representative skills)
         â†“
Step 2ï¸âƒ£  â†’ Execute parallel agent analysis (simultaneous execution)
         â†“
Step 3ï¸âƒ£  â†’ Integrate results and create report
```

---

## Step 1ï¸âƒ£: Select Analysis Targets

### Objectives
âœ… Select representative skills for each tier
âœ… Finalize 4 skills to analyze
âœ… Define analysis criteria

### Execution Method

#### 1-1) Understand Skill Folder Structure
```bash
# Command
find .claude/skills -type d -name "moai-*" | sort

# Output example
.claude/skills/moai-foundation-trust
.claude/skills/moai-foundation-tags
.claude/skills/moai-alfred-tag-scanning
.claude/skills/moai-domain-backend
.claude/skills/moai-lang-python
... (50+ skills)
```

#### 1-2) Categorize by Tier
```
ðŸ“¦ Foundation Tier (6 skills)
  â”œâ”€ moai-foundation-trust       â† Selected â­
  â”œâ”€ moai-foundation-tags
  â”œâ”€ moai-foundation-specs
  â”œâ”€ moai-foundation-ears
  â”œâ”€ moai-foundation-git
  â””â”€ moai-foundation-langs

ðŸ“¦ Alfred Tier (11 skills)
  â”œâ”€ moai-alfred-tag-scanning    â† Selected â­
  â”œâ”€ moai-alfred-code-reviewer
  â”œâ”€ moai-alfred-debugger-pro
  â”œâ”€ moai-alfred-language-detection
  â””â”€ ... (7 more)

ðŸ“¦ Domain Tier (10 skills)
  â”œâ”€ moai-domain-backend         â† Selected â­
  â”œâ”€ moai-domain-frontend
  â”œâ”€ moai-domain-web-api
  â”œâ”€ moai-domain-security
  â””â”€ ... (6 more)

ðŸ“¦ Language Tier (23 skills)
  â”œâ”€ moai-lang-python            â† Selected â­
  â”œâ”€ moai-lang-typescript
  â”œâ”€ moai-lang-go
  â”œâ”€ moai-lang-rust
  â””â”€ ... (19 more)
```

#### 1-3) Selection Criteria
| Criteria | Description | Application |
|----------|-------------|-------------|
| **Importance** | Impact on overall project | Foundation > Alfred > Domain > Language |
| **Complexity** | Document size and concept scope | Higher increases analysis value |
| **Connectivity** | Relationship with other skills | Clear integration points |

### Result: Selected Skills (4 skills)
```
âœ… moai-foundation-trust      (Foundation, core quality principles)
âœ… moai-alfred-tag-scanning   (Alfred, tracking system)
âœ… moai-domain-backend        (Domain, architecture)
âœ… moai-lang-python           (Language, latest standards)
```

---

## Step 2ï¸âƒ£: Execute Parallel Agent Analysis

### Objectives
âœ… Analyze 4 skills **simultaneously**
âœ… Generate completion scores for each skill
âœ… Specify improvement items

### Execution Method

#### 2-1) Execute Parallel Analysis Agents

**Important**: Call the following 4 Tasks in **the same message** at once
(This causes Claude Code to execute them in parallel)

```python
# Pseudocode: Parallel execution of 4 agents

Agent 1: Task(
  subagent_type="general-purpose",
  description="Foundation: Trust skill analysis",
  prompt="""
  File: /Users/goos/MoAI/MoAI-ADK/.claude/skills/moai-foundation-trust/SKILL.md

  Analysis items:
  1. Metadata (name, version, description)
  2. Document structure (sections, headings)
  3. Core content (TRUST 5 principles)
  4. Code examples presence
  5. Completion score (0-100)

  Return in JSON format
  """
)

Agent 2: Task(
  subagent_type="general-purpose",
  description="Alfred: Tag-scanning skill analysis",
  prompt="..." # Same structure, different file
)

Agent 3: Task(
  subagent_type="general-purpose",
  description="Domain: Backend skill analysis",
  prompt="..." # Same structure, different file
)

Agent 4: Task(
  subagent_type="general-purpose",
  description="Language: Python skill analysis",
  prompt="..." # Same structure, different file
)
```

#### 2-2) Analysis Items for Each Agent

Each agent analyzes the following items:

```json
{
  "skill_name": "Skill name",
  "category": "Tier name",
  "metadata": {
    "name": "...",
    "description": "...",
    "allowed_tools": ["..."],
    "auto_load": "...",
    "trigger_cues": "..."
  },
  "structure": {
    "sections": ["Metadata", "What it does", ...],
    "total_lines": 100,
    "has_yaml_frontmatter": true,
    "has_code_examples": true
  },
  "content_score": 75,
  "findings": [
    "âœ… Strength 1",
    "âœ… Strength 2",
    "âš ï¸ Weakness 1",
    "âŒ Critical issue"
  ],
  "recommendations": [
    "Improvement item 1",
    "Improvement item 2"
  ]
}
```

#### 2-3) Benefits of Parallel Execution

| Aspect | Sequential | Parallel | Improvement |
|--------|------------|----------|-------------|
| **Time Required** | 60 minutes | 15 minutes | â¬‡ï¸ 75% reduction |
| **System Efficiency** | 1 agent | 4 simultaneous | â¬†ï¸ 4x |
| **Analysis Consistency** | Time bias | Simultaneity guaranteed | â¬†ï¸ High |
| **Cross-Tier Patterns** | Limited | Comprehensive | â¬†ï¸ Strong |

#### 2-4) Agent Execution Confirmation

Each agent signals completion with the following:

```bash
âœ… Agent 1 Complete: Foundation Trust analysis (JSON returned)
âœ… Agent 2 Complete: Alfred Tag-scanning analysis (JSON returned)
âœ… Agent 3 Complete: Domain Backend analysis (JSON returned)
âœ… Agent 4 Complete: Language Python analysis (JSON returned)

# Wait until all agents complete simultaneously
```

### Execution Result Examples

#### Agent 1 Output (Foundation Trust)
```json
{
  "skill_name": "moai-foundation-trust",
  "category": "Foundation",
  "content_score": 75,
  "findings": [
    "âœ… Perfect YAML metadata",
    "âœ… Clear TRUST 5 principles",
    "âš ï¸ Lack of specific verification commands"
  ],
  "recommendations": [
    "Add language-specific TRUST verification command matrix",
    "Provide automated verification script templates"
  ]
}
```

#### Agent 2 Output (Alfred Tag-scanning)
```json
{
  "skill_name": "moai-alfred-tag-scanning",
  "category": "Alfred",
  "content_score": 68,
  "findings": [
    "âœ… Clear CODE-FIRST principle",
    "âŒ Missing template file (templates/tag-inventory-template.md)",
    "âš ï¸ Examples are boilerplate"
  ],
  "recommendations": [
    "[CRITICAL] Create missing template file",
    "[HIGH] Detail how-it-works algorithm"
  ]
}
```

#### Agents 3 & 4 (Omitted, similar structure)

---

## Step 3ï¸âƒ£: Integrate Results and Create Report

### Objectives
âœ… Integrate 4 analysis results
âœ… Discover tier-by-tier patterns
âœ… Derive actionable recommendations

### Execution Method

#### 3-1) Score Aggregation
```
Foundation (Trust):     75/100
Alfred (Tag-scanning):  68/100
Domain (Backend):       75/100
Language (Python):      85/100
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Average:                75.75/100  (B+ grade)
```

#### 3-2) Cross-Tier Pattern Analysis

**Common Strengths** (All skills):
```
âœ… Standardized metadata structure (YAML frontmatter)
âœ… Consistent documentation system (13 standard sections)
âœ… Explicit connections to other skills
âœ… Foundation principle compliance
```

**Common Weaknesses** (All skills):
```
âŒ Lack of code examples (theory-focused)
âŒ Insufficient workflow guides (tool listings)
âŒ Missing error handling guides
âŒ No templates/scripts provided
âŒ Low practicality of examples
```

#### 3-3) Improvement Priority Matrix

```
        High Impact
            â–²
            â”‚
    TAG-    â”‚  TRUST
    scanningâ”‚  â˜…    Python
    â˜…â˜…â˜…    â”‚  â˜…â˜…   (Complete)
            â”‚
    Backend â”‚
    â˜…â˜…     â”‚
            â”‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Low Effort
          Low   High

Legend:
â˜…â˜…â˜… = High impact, low effort (Immediate action)
â˜…â˜…  = Medium impact, effort needed (Planned)
â˜…   = Low impact but optional (When available)
```

#### 3-4) Action Plan Development

##### Phase 1: Urgent (1 week)
```
Priority 1: TAG-scanning missing template creation
  File: templates/tag-inventory-template.md
  Content: TAG inventory samples + normal/broken TAG examples
  Effort: 2-3 hours
  ROI: Very high (completion 68â†’85)

Priority 2: Trust verification commands addition
  Items: Language-specific TRUST verification command matrix
  Examples: Python pytest, Go go test, Rust cargo test
  Effort: 2-3 hours
  ROI: High (applies to all projects)

Priority 3: Backend code examples addition
  Count: 5 examples (1 per language)
  Examples: Python FastAPI, Go Gin, Node.js Express
  Effort: 3-4 hours
  ROI: Medium (applies to backend projects only)
```

##### Phase 2: In Progress (2 weeks)
```
- Add 5 real TAG-scanning use cases
- Trust automated verification script templates
- Backend security section creation (JWT, RBAC, Secrets)
```

##### Phase 3: Ongoing (1 month)
```
- Add error handling guides to all skills
- Integrate CI/CD pipeline examples
- Write verification tests for each skill
```

#### 3-5) Report Creation

Organize results into a Markdown document:

```markdown
# Parallel Analysis Report

## Executive Summary
- Analysis targets: 4 skills (Foundation, Alfred, Domain, Language)
- Average score: 75.75/100 (B+)
- Overall assessment: Structurally solid but needs practical guide enhancement

## Detailed Analysis Results
### 1. Foundation - Trust (75/100)
- Strengths: Perfect YAML structure, clear TRUST 5 principles
- Weaknesses: Lack of verification commands, abstract examples

### 2. Alfred - Tag-scanning (68/100)
- Strengths: Clear CODE-FIRST principle
- Weaknesses: Missing template file, undocumented algorithm

... (continued)

## Cross-Tier Patterns
- Common strengths: Metadata standardization, documentation system
- Common weaknesses: Lack of code examples, insufficient workflows

## Action Plan
Phase 1 (1 week): Urgent - Templates, commands, code examples
Phase 2 (2 weeks): In progress - Use cases, security, scripts
Phase 3 (1 month): Ongoing - Error handling, CI/CD, tests
```

### File Creation
```bash
File: .claude/skills/moai-skill-factory/PARALLEL-ANALYSIS-REPORT.md
Size: About 600-800 lines
Content: Comprehensive analysis, tier-by-tier evaluation, improvements, action plan
```

---

## ðŸ”„ Actual Execution Example

### Command Line Execution (Pseudocode)
```bash
# Step 1: Check skill folders
$ ls -la .claude/skills/ | grep moai-

# Step 2: Execute 4 agents in parallel
# (Call simultaneously in Claude Code)

# Step 3: Collect results
Agent 1: âœ… Foundation Trust complete (JSON)
Agent 2: âœ… Alfred Tag-scanning complete (JSON)
Agent 3: âœ… Domain Backend complete (JSON)
Agent 4: âœ… Language Python complete (JSON)

# Step 4: Create report
$ echo "Integrating parallel analysis results..."
$ cat > PARALLEL-ANALYSIS-REPORT.md << EOF
# Analysis Report
(Integrate 4 analysis results)
EOF
```

---

## ðŸ“Š Analysis Metrics (Understanding)

### Completion Score Interpretation
```
90-100: â­â­â­ Model case (production-ready)
80-89:  â­â­   Excellent (minor improvements needed)
70-79:  â­    Needs improvement (has key content but insufficient)
60-69:  âš ï¸    Incomplete (structure only, lacks content)
<60:    ðŸ”´    Incomplete (needs rework)
```

### Score Components
```
Metadata:        20% (structural foundation)
Document structure: 15% (standard compliance)
Content depth:    25% (concept explanation)
Code examples:    20% (practicality)
Workflow:        10% (usage guide)
Error handling:   10% (recovery procedures)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:           100%
```

---

## ðŸŽ“ Key Learning Points

### Strengths of Parallel Analysis
1. **Time Efficiency**: Sequential 60 minutes â†’ Parallel 15 minutes (4x improvement)
2. **Consistency**: Simultaneous evaluation with same criteria
3. **Pattern Discovery**: Cross-Tier comparison reveals common weaknesses
4. **Prioritization**: Impact analysis for ROI optimization

### Role of Skill-factory Agents
- YAML structure validation
- Document standard compliance evaluation
- Objective completion score calculation
- Specific improvement item identification and prioritization

### Execution of Improvement Recommendations
1. **Code Examples**: Minimum 3 per language
2. **Templates**: Provide pyproject.toml, pytest.ini references
3. **Workflows**: REDâ†’GREENâ†’REFACTOR TDD cycle
4. **Error Handling**: Common error pattern cataloging

---

## âœ… Completion Checklist

- [ ] Step 1: Selection of 4 analysis target skills complete
- [ ] Step 2: Parallel execution of 4 agents complete
- [ ] Step 3: Result integration and report creation complete
- [ ] Action Plan Phase 1 review complete
- [ ] Improvement item priority confirmation complete

---

**Next Steps**: Review PARALLEL-ANALYSIS-REPORT.md â†’ Execute Phase 1 Action Plan

