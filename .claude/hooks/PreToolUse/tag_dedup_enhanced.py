#!/usr/bin/env python3
"""
Enhanced TAG De-duplication PreToolUse Hook
GPT-5 Pro ë¶„ì„ ê¸°ë°˜ ì‹¤ì‹œê°„ TAG ì¤‘ë³µ ê²€ì¦ ì‹œìŠ¤í…œ
"""

import json
import os
import re
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

# Configuration paths
POLICY_PATH = Path(".moai/tag-policy-updated.json")
DEDUP_POLICY_PATH = Path(".moai/tag-dedup-policy.json")
LEDGER_PATH = Path(".moai/tags/ledger.jsonl")
INDEX_PATH = Path(".moai/tags/index.json")
DUPLICATE_CACHE_PATH = Path(".moai/cache/duplicate_cache.json")

class TagDedupValidator:
    """TAG ì¤‘ë³µ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.policy = self._load_policy()
        self.dedup_policy = self._load_dedup_policy()
        self.duplicate_cache = self._load_duplicate_cache()
        
    def _load_policy(self) -> Dict[str, Any]:
        """TAG ì •ì±… ë¡œë“œ"""
        if POLICY_PATH.exists():
            with open(POLICY_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _load_dedup_policy(self) -> Dict[str, Any]:
        """ì¤‘ë³µ ì œê±° ì •ì±… ë¡œë“œ"""
        if DEDUP_POLICY_PATH.exists():
            with open(DEDUP_POLICY_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _load_duplicate_cache(self) -> Dict[str, Any]:
        """ì¤‘ë³µ ìºì‹œ ë¡œë“œ"""
        if DUPLICATE_CACHE_PATH.exists():
            with open(DUPLICATE_CACHE_PATH, 'r', encoding='utf-8') as f:
                cache = json.load(f)
                # 1ì‹œê°„ ì´ìƒ ëœ ìºì‹œëŠ” ì´ˆê¸°í™”
                cache_time = datetime.fromisoformat(cache.get("timestamp", "1970-01-01"))
                if datetime.now() - cache_time > timedelta(hours=1):
                    return {"timestamp": datetime.now().isoformat(), "duplicates": {}}
                return cache
        return {"timestamp": datetime.now().isoformat(), "duplicates": {}}
    
    def _save_duplicate_cache(self) -> None:
        """ì¤‘ë³µ ìºì‹œ ì €ì¥"""
        DUPLICATE_CACHE_PATH.parent.mkdir(exist_ok=True)
        with open(DUPLICATE_CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump(self.duplicate_cache, f, indent=2)
    
    def extract_tags_from_content(self, content: str, file_path: str) -> List[Dict[str, Any]]:
        """ë‚´ìš©ì—ì„œ TAG ì¶”ì¶œ"""
        tag_pattern = re.compile(r'@([A-Z]+):([A-Z_]+)-([0-9]{3,})')
        lines = content.split('\n')
        tags = []
        
        for line_num, line in enumerate(lines, 1):
            matches = tag_pattern.finditer(line)
            for match in matches:
                tag_type, domain, id_num = match.groups()
                full_tag = match.group(0)
                
                # Topline ì—¬ë¶€ í™•ì¸ (ì²˜ìŒ 20ì¤„)
                is_topline = line_num <= 20
                
                # ê¶Œí•œ ì ìˆ˜ ê³„ì‚°
                authority_score = self._calculate_authority_score(file_path, line_num)
                
                tag_info = {
                    "tag": full_tag,
                    "tag_type": tag_type,
                    "domain": domain,
                    "id_number": id_num,
                    "line_number": line_num,
                    "is_topline": is_topline,
                    "authority_score": authority_score,
                    "pattern": f"{tag_type}:{domain}-{id_num}"
                }
                tags.append(tag_info)
        
        return tags
    
    def _calculate_authority_score(self, file_path: str, line_number: int) -> int:
        """íŒŒì¼ ê¶Œí•œ ì ìˆ˜ ê³„ì‚°"""
        path = Path(file_path)
        authority_hierarchy = self.policy.get("topline_rules", {}).get("authority_hierarchy", {})
        
        # ìµœê³  ê¶Œí•œ
        for pattern in authority_hierarchy.get("highest", []):
            if path.match(pattern):
                return 100
                
        # ë†’ì€ ê¶Œí•œ
        for pattern in authority_hierarchy.get("high", []):
            if path.match(pattern):
                return 80
                
        # ì¤‘ê°„ ê¶Œí•œ
        for pattern in authority_hierarchy.get("medium", []):
            if path.match(pattern):
                return 60
                
        # ë‚®ì€ ê¶Œí•œ
        for pattern in authority_hierarchy.get("low", []):
            if path.match(pattern):
                return 40
                
        # ê¸°ë³¸ ê¶Œí•œ
        return 20
    
    def _is_whitelisted_path(self, file_path: str) -> bool:
        """í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê²½ë¡œ í™•ì¸"""
        path = Path(file_path)
        whitelist = self.dedup_policy.get("gpt5_pro_analysis", {}).get("exception_whitelist", {}).get("paths", [])
        
        for pattern in whitelist:
            if path.match(pattern):
                return True
        
        return False
    
    def validate_topline_uniqueness(self, new_tags: List[Dict[str, Any]], file_path: str) -> List[Dict[str, Any]]:
        """Topline ìœ ì¼ì„± ê²€ì¦"""
        violations = []
        
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê²½ë¡œëŠ” ê±´ë„ˆëœ€
        if self._is_whitelisted_path(file_path):
            return violations
        
        for new_tag in new_tags:
            if not new_tag["is_topline"]:
                continue
            
            pattern = new_tag["pattern"]
            
            # ìºì‹œì—ì„œ ì¤‘ë³µ í™•ì¸
            if pattern in self.duplicate_cache["duplicates"]:
                existing = self.duplicate_cache["duplicates"][pattern]
                
                # ë” ë†’ì€ ê¶Œí•œì„ ê°€ì§„ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                if existing["authority_score"] >= new_tag["authority_score"]:
                    violation = {
                        "type": "topline_duplicate",
                        "severity": "critical",
                        "tag": new_tag["tag"],
                        "pattern": pattern,
                        "file_path": file_path,
                        "line_number": new_tag["line_number"],
                        "existing_file": existing["file_path"],
                        "existing_authority": existing["authority_score"],
                        "new_authority": new_tag["authority_score"],
                        "message": f"Topline duplicate detected: {new_tag['tag']} already exists in {existing['file_path']} with higher authority",
                        "suggestion": "Use /alfred:tag-dedup to resolve duplicates"
                    }
                    violations.append(violation)
        
        return violations
    
    def update_duplicate_cache(self, new_tags: List[Dict[str, Any]], file_path: str) -> None:
        """ì¤‘ë³µ ìºì‹œ ì—…ë°ì´íŠ¸"""
        for tag in new_tags:
            if tag["is_topline"]:
                pattern = tag["pattern"]
                
                # ìºì‹œì— ì—†ê±°ë‚˜ ë” ë†’ì€ ê¶Œí•œì„ ê°€ì§„ TAGë©´ ì—…ë°ì´íŠ¸
                if (pattern not in self.duplicate_cache["duplicates"] or 
                    tag["authority_score"] > self.duplicate_cache["duplicates"][pattern]["authority_score"]):
                    
                    self.duplicate_cache["duplicates"][pattern] = {
                        "tag": tag["tag"],
                        "file_path": file_path,
                        "line_number": tag["line_number"],
                        "authority_score": tag["authority_score"],
                        "timestamp": datetime.now().isoformat()
                    }
        
        self._save_duplicate_cache()
    
    def validate_chain_integrity(self, new_tags: List[Dict[str, Any]], file_path: str) -> List[Dict[str, Any]]:
        """TAG ì²´ì¸ ë¬´ê²°ì„± ê²€ì¦"""
        violations = []
        
        # TODO: ì²´ì¸ ë¬´ê²°ì„± ê²€ì¦ ë¡œì§ êµ¬í˜„
        # SPEC â†’ TEST â†’ CODE â†’ DOC ì—°ê²° í™•ì¸
        
        return violations
    
    def validate_scope_compliance(self, new_tags: List[Dict[str, Any]], file_path: str) -> List[Dict[str, Any]]:
        """ìŠ¤ì½”í”„ ê·œì • ì¤€ìˆ˜ ê²€ì¦"""
        violations = []
        
        monorepo_scoping = self.policy.get("monorepo_scoping", {})
        if not monorepo_scoping.get("enabled", False):
            return violations
        
        for tag in new_tags:
            # íŒ¨í‚¤ì§€ ìŠ¤ì½”í”„í•‘ ê·œì¹™ í™•ì¸
            if "#" not in tag["tag"] and tag["is_topline"]:
                # ìŠ¤ì½”í”„ê°€ ì—†ëŠ” topline TAG ê²½ê³ 
                violation = {
                    "type": "scope_violation",
                    "severity": "warning",
                    "tag": tag["tag"],
                    "file_path": file_path,
                    "line_number": tag["line_number"],
                    "message": f"Package-scoped tag format recommended: {tag['tag']} should use domain scoping",
                    "suggestion": f"Consider using format like @TYPE:DOMAIN#PACKAGE-{tag['id_number']}"
                }
                violations.append(violation)
        
        return violations
    
    def validate_tags(self, content: str, file_path: str) -> Dict[str, Any]:
        """TAG ê²€ì¦ ì‹¤í–‰"""
        try:
            # TAG ì¶”ì¶œ
            new_tags = self.extract_tags_from_content(content, file_path)
            
            if not new_tags:
                return {"valid": True, "violations": []}
            
            # ê²€ì¦ ì‹¤í–‰
            violations = []
            
            # 1. Topline ìœ ì¼ì„± ê²€ì¦
            topline_violations = self.validate_topline_uniqueness(new_tags, file_path)
            violations.extend(topline_violations)
            
            # 2. ì²´ì¸ ë¬´ê²°ì„± ê²€ì¦
            chain_violations = self.validate_chain_integrity(new_tags, file_path)
            violations.extend(chain_violations)
            
            # 3. ìŠ¤ì½”í”„ ê·œì • ì¤€ìˆ˜ ê²€ì¦
            scope_violations = self.validate_scope_compliance(new_tags, file_path)
            violations.extend(scope_violations)
            
            # 4. ìºì‹œ ì—…ë°ì´íŠ¸ (ìœ„ë°˜ ì—†ëŠ” ê²½ìš°ë§Œ)
            if not any(v["severity"] == "critical" for v in violations):
                self.update_duplicate_cache(new_tags, file_path)
            
            return {
                "valid": len(violations) == 0,
                "violations": violations,
                "tags_found": len(new_tags),
                "topline_tags": sum(1 for t in new_tags if t["is_topline"])
            }
            
        except Exception as e:
            return {
                "valid": False,
                "violations": [{
                    "type": "validation_error",
                    "severity": "error",
                    "message": f"TAG validation failed: {str(e)}",
                    "file_path": file_path
                }]
            }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì¸ì íŒŒì‹±
    if len(sys.argv) < 4:
        print("Usage: PreToolUse.py <tool_name> <file_path> <content>")
        sys.exit(1)
    
    tool_name = sys.argv[1]
    file_path = sys.argv[2]
    content = sys.argv[3]
    
    # íŒŒì¼ íƒ€ì… í™•ì¸
    file_extension = Path(file_path).suffix.lower()
    supported_extensions = ['.py', '.md', '.js', '.ts', '.jsx', '.tsx', '.java', '.go', '.rs']
    
    if file_extension not in supported_extensions:
        # ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ íƒ€ì…ì€ í†µê³¼
        print(json.dumps({"valid": True, "violations": []}))
        sys.exit(0)
    
    # ê²€ì¦ê¸° ìƒì„± ë° ì‹¤í–‰
    validator = TagDedupValidator()
    result = validator.validate_tags(content, file_path)
    
    # ê²°ê³¼ ì¶œë ¥
    print(json.dumps(result, indent=2))
    
    # ì¹˜ëª…ì  ìœ„ë°˜ì´ ìˆìœ¼ë©´ ë¹„ì œë¡œ ì¢…ë£Œ
    critical_violations = [v for v in result["violations"] if v["severity"] == "critical"]
    if critical_violations:
        print(f"\nğŸš¨ Critical TAG violations detected:", file=sys.stderr)
        for violation in critical_violations:
            print(f"  â€¢ {violation['message']}", file=sys.stderr)
            print(f"    Suggestion: {violation.get('suggestion', 'Contact administrator')}", file=sys.stderr)
        sys.exit(1)
    
    sys.exit(0)

if __name__ == "__main__":
    main()
